{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09483ac1-bfbc-4b74-9688-5c67b283db17",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"IFQ720 Worksheet 08 - Diagnostics\"\n",
    "output:\n",
    "  html_document\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082bac1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57607625",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import plotnine\n",
    "import numpy\n",
    "import scipy\n",
    "import math\n",
    "from plotnine import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c96861",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3112d6e7",
   "metadata": {},
   "source": [
    "# Diagnostics\n",
    "\n",
    "All models, including those discussed so far, ANOVA, linear regression, and logisitic regression, rely on some assumptions.  Model Diagnositics are a set of tools to checks the underlying model assumptions, including if there are any other exogenous effects not accounted for in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fb379a",
   "metadata": {},
   "source": [
    "## Diagnostic Tools for Testing the Regression Results\n",
    "\n",
    "Let's consider the model from Module 6 where we fit a simple linear regression model for the city and highway mileages from the `epa_data` dataset. \n",
    "\n",
    "Recall that the relationship between highway fuel economy ($x$) and\n",
    "city fuel economy ($y$) is\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1x_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57a6357",
   "metadata": {},
   "source": [
    "The underlying assumptions about the distribution of the residuals and\n",
    "hence the sampling distributions of the parameter estimates have\n",
    "important implications for both inference and, as we will see, using the\n",
    "regression model for prediction. Luckily there are several useful tools\n",
    "that rely on using the residuals\n",
    "$$\n",
    "e_i =y_i-\\hat{\\beta}_0-\\hat{\\beta}_1\n",
    "$$ \n",
    "available for exploring and testing the validity of these assumptions.\n",
    "\n",
    "###  Normality  {#normality .unnumbered}\n",
    "\n",
    "The first assumption to check is that the residuals from the model are\n",
    "from a Gaussian distribution. We can do this using a normal probability\n",
    "plot. A normal probability plot is a plot of the residuals against their\n",
    "expected value if they had come from a Gaussian distribution. If the\n",
    "residuals are from a Gaussian distribution, they should fall more\n",
    "or less along a straight, diagonal line. We plot this is by first\n",
    "sorting the residuals in ascending order, then standardising them\n",
    "$$e_i^* = \\frac{e_i}{\\sqrt{s^2}}$$ and plotting them against their\n",
    "corresponding quantiles from a standard Gaussian distribution.\n",
    "\n",
    "The use of this plot is not a rigorous test of normality versus\n",
    "non-normality, but instead can indicate potential deviance from the\n",
    "Gaussian assumption, particularly in the behaviour of the tails of the\n",
    "distribution. The results are not conclusive but should inform\n",
    "judgement when interpreting the model results.\n",
    "\n",
    "### Heteroskedasticity {#heteroskedasticity .unnumbered}\n",
    "\n",
    "The second residual plot we can use is to plot the fitted values of the\n",
    "model \n",
    "$$\n",
    "\\hat{y}_i = \\hat{\\beta}_0+\\hat{\\beta}_1x_i\n",
    "$$ \n",
    "against the residuals. The residuals should fall evenly along either side of a\n",
    "horizontal line centred at $0$, with no evidence of trend or increase in\n",
    "magnitude as $\\hat{y}$ increases. We use this plot to check the assumption that\n",
    "the variance of the residuals is equal for all observations, and\n",
    "specifically that the variance is not a function of the expected\n",
    "value of the observation.\n",
    "\n",
    "The plots for the residuals versus fitted values and the $q-q$ plots show that (despite the small sample size) the residuals\n",
    "appear to adhere to the model's assumptions. The variance (i.e. the\n",
    "magnitude of the residuals) doesn't appear to be a function of the\n",
    "fitted value, indicating that the variance of the residuals appears\n",
    "constant. The $q-q$ plot looks approximately Gaussian (supported by the\n",
    "histogram as well). We can disregard the index plot for now, but it also\n",
    "supports the notion of homoskedasticity in the residuals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da5d7ad",
   "metadata": {},
   "source": [
    "### Other concerns {#other-concerns .unnumbered}\n",
    "\n",
    "The two techniques of using normal probability plots and plots of the\n",
    "residuals versus the fitted values address two of the three assumptions\n",
    "about the residuals: the unbiasedness of the regression model and\n",
    "the constant variance of the residuals. Tests of independence are\n",
    "technically possible and, while beyond the scope of this material, should\n",
    "be used in cases where the independent variables are temporally or\n",
    "spatially referenced. The specific tools for modelling time-series or\n",
    "spatial data exist, and we should use them where appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e4ca8c",
   "metadata": {},
   "source": [
    "###   Example\n",
    "Analyse the results for the model\n",
    "$$\n",
    "city = \\beta_0+\\beta_1hwy\n",
    "$$\n",
    "Do the assumptions for linear regression appear to be met?\n",
    "\n",
    "####    Solution\n",
    "\n",
    "The methods we want to check are graphical, meaning that they rely on our interpretation of visual plots.  Hence, they are qualitative and not statistically rigorous, and somewhat subjective.  Still, they are useful and relatively straightforward to understand.  If needed, there are more rigorous methods for formally testing the regression assumptions. \n",
    "\\\n",
    "\\\n",
    "In general, we rely on computational tools for creating the images needed.\n",
    "\n",
    "####    Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import statsmodels\n",
    "\n",
    "df = pandas.read_csv(\"../DATA/epa_data.csv\")\n",
    "\n",
    "df = df[df[\"year\"]==1999]\n",
    "\n",
    "model = statsmodels.formula.api.ols('city~hwy',df)\n",
    "\n",
    "result = model.fit()\n",
    "\n",
    "residuals = result.resid\n",
    "\n",
    "fitted_values = result.fittedvalues\n",
    "\n",
    "df_fit = pandas.DataFrame({'residuals':residuals,'fitted_values':fitted_values})\n",
    "\n",
    "plot1 = (ggplot(df,aes(x = 'hwy', y = 'city'))+\n",
    "  geom_point()+\n",
    "    geom_smooth(method = \"lm\")).draw();\n",
    "\n",
    "plot2 = (ggplot(df_fit,aes(x = 'residuals'))+\n",
    "  geom_histogram(bins = 21)).draw();\n",
    "\n",
    "plot3 = (ggplot(df_fit,aes(x = 'fitted_values',y = 'residuals'))+\n",
    "  geom_point()).draw();\n",
    "\n",
    "plot4 = (ggplot(df_fit,aes(sample='residuals'))+\n",
    "  geom_qq()+\n",
    "    geom_qq_line()).draw();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c174729",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "####    Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e97c6f9-46ce-4033-9659-68f36861f9ab",
   "metadata": {
    "lines_to_next_cell": 0,
    "message": false,
    "tags": [
     "remove_input"
    ],
    "warning": false
   },
   "outputs": [],
   "source": [
    "plot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ce521-f23a-4e91-a515-a49a61df43d7",
   "metadata": {
    "lines_to_next_cell": 0,
    "message": false,
    "tags": [
     "remove_input"
    ],
    "warning": false
   },
   "outputs": [],
   "source": [
    "plot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e88dac3-be40-44f0-8f17-84bf93acdd53",
   "metadata": {
    "lines_to_next_cell": 0,
    "message": false,
    "tags": [
     "remove_input"
    ],
    "warning": false
   },
   "outputs": [],
   "source": [
    "plot3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a52ed8-3e7c-4e29-a532-3bf54102cd14",
   "metadata": {
    "lines_to_next_cell": 0,
    "message": false,
    "tags": [
     "remove_input"
    ],
    "warning": false
   },
   "outputs": [],
   "source": [
    "plot4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28fc9ad",
   "metadata": {},
   "source": [
    "The first plot shows that the linear relationship between city and highway full economy is reasonable.  The histogram of the residuals and the $q$-$q$ plot show that the residuals are approximately Gaussian.  The plot of the residuals versus the fitted values shows some evidence of heteroskedasticity, which doesn't look extreme but is likely as the full economy measures are based on counts (i.e. Poisson-like), indicating that the variance is proportional to the expected value.\n",
    "\n",
    "####    Improvement\n",
    "\n",
    "Note that there are some areas of concern, and in particular the residuals versus fitted plot shows some clear outliers. One way we can address this is by looking at adding additional variable to the model.\n",
    "\n",
    "In practice you might try different variable and decide on which ones to include in your model by trial and error (though the study of systematic and rigourous ways to do this are quite extensive).  But in our case we are going to select the number of cylinders in the engine (`cyl`) as our additional variable just to see what if that can reduce the outliers and \"clean up\" the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb48e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_2 = statsmodels.formula.api.ols('city~hwy+cyl',df).fit()\n",
    "\n",
    "df_fit = pandas.DataFrame({'residuals_2':result_2.resid,'fitted_values_2':result_2.fittedvalues})\n",
    "\n",
    "plot1 = (ggplot(df,aes(x = 'hwy', y = 'city'))+\n",
    "  geom_point()+\n",
    "    geom_smooth(method = \"lm\")).draw();\n",
    "\n",
    "plot2 = (ggplot(df_fit,aes(x = 'residuals_2'))+\n",
    "  geom_histogram(bins = 21)).draw();\n",
    "\n",
    "plot3 = (ggplot(df_fit,aes(x = 'fitted_values_2',y = 'residuals_2'))+\n",
    "  geom_point()).draw();\n",
    "\n",
    "plot4 = (ggplot(df_fit,aes(sample='residuals_2'))+\n",
    "  geom_qq()+\n",
    "    geom_qq_line()).draw();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f3fff7-25d8-4819-bc86-43fd8b31ff1d",
   "metadata": {
    "message": false,
    "tags": [
     "remove_input"
    ],
    "warning": false
   },
   "outputs": [],
   "source": [
    "plot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67dac4f-f2fc-4a4d-82a1-4aebbbad4577",
   "metadata": {
    "message": false,
    "tags": [
     "remove_input"
    ],
    "warning": false
   },
   "outputs": [],
   "source": [
    "plot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb2c786-5849-4ce6-ae76-f787c33ce388",
   "metadata": {
    "message": false,
    "tags": [
     "remove_input"
    ],
    "warning": false
   },
   "outputs": [],
   "source": [
    "plot3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37c355b-a78b-4f90-919a-5c883b80dd66",
   "metadata": {
    "message": false,
    "tags": [
     "remove_input"
    ],
    "warning": false
   },
   "outputs": [],
   "source": [
    "plot4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae294c9",
   "metadata": {},
   "source": [
    "We can note that the second model residuals appear to be much better behaved.  While there are still some outliers, they aren't as extreme as in the first model and the residual versus fitted values plot looks more like what we would like to see.  \n",
    "\n",
    "Note that finally we can compare the $R^2_{adj}$ values to compare model and pick the \"better\" one. We note that the second model (including `cyl`) is the preferred model under this criteria. \n",
    "\n",
    "##    Diagnostic Tools for ANOVA Models\n",
    "\n",
    "ANOVA and ANCOVA models rely on a similar set of assumptions as linear regerssion models, but they also have some additional areas of concern that need to be addressed. We will look at some data for the salaries for IT professionals to understand how to diagnose ANOVA models and address some issues that might arise. \n",
    "\n",
    "Our data is an example Salary Table for IT professionals with varying educational backgrounds (Bachelors, Masters, and PhD), experience (Years), and level (Management or Lower).  First let's explore the data, noting that Experience in years is a continuous variable which we are going to set aside for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f6f06c-26c9-4069-9f11-d5603624fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#S = Salary \n",
    "#X = Experience\n",
    "#E = Education\n",
    "#P = Managment or Lower Level\n",
    "\n",
    "df = pandas.read_csv(\"../DATA/salary.csv\")\n",
    "\n",
    "df = df.rename(columns = {'S': 'Salary', 'X': 'Experience','E': 'Education','P': 'Level'})\n",
    "\n",
    "ed = (\n",
    "  ggplot(df)+\n",
    "  geom_boxplot(aes(y = 'Salary', x = 'Education'))\n",
    ").draw();\n",
    "\n",
    "\n",
    "level = (\n",
    "  ggplot(df)+\n",
    "  geom_boxplot(aes(y = 'Salary', x = 'Level'))\n",
    ").draw();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b706fb4-e8ab-4177-ab6a-79456e86dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af268863-5a7e-4637-90fa-ae2aa70f3115",
   "metadata": {},
   "outputs": [],
   "source": [
    "level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff47689",
   "metadata": {},
   "source": [
    "Note that we are interested in the relationship between Salary and Education, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d04e56f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "import statsmodels\n",
    "\n",
    "model = statsmodels.formula.api.ols('Salary ~ C(Education)+C(Level)',df).fit()\n",
    "\n",
    "aov_model = statsmodels.stats.anova.anova_lm(model)\n",
    "\n",
    "print(aov_model)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede907d3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "It appears that both education and your level are statistically significant. Look at the summary of the OLS model and see that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f72224c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204de2fd",
   "metadata": {},
   "source": [
    "This looks a little odd, there is not a statistically significant difference between Bachelors and PhD, this seems counter intuitive. \n",
    "\n",
    "Let's check the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fa1c0c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "df[\"res\"] = model.resid\n",
    "df['ind'] = df.index\n",
    "\n",
    "(ggplot(df)+\n",
    "geom_point(aes(y = 'res', x = 'ind'))\n",
    "\n",
    ")\n",
    "\n",
    "res = (ggplot(df)+geom_qq(aes(sample = 'res'))+\n",
    "geom_qq_line(aes(sample = 'res'))\n",
    ").draw();\n",
    "\n",
    "res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b10b6f",
   "metadata": {},
   "source": [
    "Uh-Oh this looks very odd, the residuals aren't centered at 0 and instead are linearly related to the order of observations?  Seems like our model might be missing something, perhaps we ought to look at experience. This is an example of an ANCOVA model and we need to check to see if there is an interaction between the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8bdc5c-8158-4547-9b80-3e320bbcc79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_ed = (\n",
    "  ggplot(df)+\n",
    "  geom_point(aes(y = 'Salary', x = 'Experience', color = 'Education'))\n",
    ").draw();\n",
    "\n",
    "ex_lev = (\n",
    "  ggplot(df)+\n",
    "  geom_point(aes(y = 'Salary', x = 'Experience', color = 'Level'))\n",
    ").draw();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5918a3d6-d8b5-40c3-adbe-d65d8c938509",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9784ed26-0a72-466c-b0a3-f9d798a4ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_lev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99c61c9",
   "metadata": {},
   "source": [
    "The data for experience versus salary appear to be parallel for Education and Level, so it appears that the assumption for ANCOVA is valid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ad0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_2 = statsmodels.formula.api.ols('Salary ~ C(Education)+C(Level)+Experience',df).fit()\n",
    "\n",
    "aov_model_2 = statsmodels.stats.anova.anova_lm(model_2)\n",
    "\n",
    "print(aov_model_2)\n",
    "\n",
    "model_2.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c5a145-8303-4841-9e1f-d8ba13c715e2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df[\"resid_2\"] = model_2.resid\n",
    "\n",
    "df['ind'] = df.index\n",
    "\n",
    "res2_1 = (ggplot(df)+\n",
    "geom_point(aes(y = 'resid_2', x = 'ind'))+ylim(-9000,9000)\n",
    ").draw();\n",
    "\n",
    "res2_2 = (ggplot(df)+geom_qq(aes(sample = 'resid_2'))+\n",
    "geom_qq_line(aes(sample = 'resid_2'))+ylim(-9000,9000)\n",
    ").draw();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4e2975-de11-472a-a238-399cddb4eb9a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "res2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7255373-31bf-424b-88d4-5aaf0ca943b0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "res2_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df178e72",
   "metadata": {},
   "source": [
    "This looks a better.  The residuals now appear unbiased  and the range of variables is much smaller.  The `q-q` plot looks a bit better too.  As for linear models we can alos compare the $R^2_{adj}$ values as well to make some judgement about model performance and preference. \n",
    "\n",
    "##    For Further Thought\n",
    "\n",
    "In this module we have barely scratched the surface of tools and techniques for evaluating models.  There are numerous tools including information criteria for model selection, stepwise procedures for variable selections, and cross validation for testing the predictive capabilities of a model.   \n",
    "\n",
    "We should also note that for the logistic regression model the diagnostics become much more complicated and their derivation and explanation are beyond the scoe of this course.  It is our advice that you should consult other resources or seek out expert advice if you have any other questions or concerns when fitting a logistic regression model.  \n",
    "\n",
    "#   Further Examples\n",
    "\n",
    "## Additional Questions for Practice\n",
    "\n",
    "1.    The concentration of asbestos fibres is important to environmental health. There are two methods for measuring the concentration of small asbestos fibres: scanning electron microscope (SEM) and phase-contrast microscope (PCM).  Are the two methods equivalent?  Assume that the SEM results are a \"gold standard\".  What do your results tell you about how the PCM compares to SEM?  What aspects of the model should you look at to determine this? The data are in the dataset `asbestos`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a875296",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df = pandas.read_csv(\"../DATA/asbestos.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b86861",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pandas.read_csv(\"../DATA/asbestos.csv\")\n",
    "\n",
    "model = statsmodels.formula.api.ols(\"PCM~SEM\",df)\n",
    "\n",
    "results = model.fit()\n",
    "\n",
    "results.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5ffe9",
   "metadata": {},
   "source": [
    "Now, perform diagnostics on the model residuals and comment on the results.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9661a1c-dae1-43ba-9c7e-e10007d5c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv(\"../DATA/asbestos.csv\")\n",
    "\n",
    "model = statsmodels.formula.api.ols(\"PCM~SEM\",df)\n",
    "\n",
    "results = model.fit()\n",
    "\n",
    "results.summary()\n",
    "\n",
    "df[\"fittedvalues\"] = results.fittedvalues\n",
    "df[\"residuals\"] = results.resid\n",
    "\n",
    "plot1 = (ggplot(df,aes(x = 'SEM', y = 'PCM'))+\n",
    "  geom_point()+\n",
    "    geom_smooth(method = \"lm\")).draw();\n",
    "\n",
    "plot2 = (ggplot(df,aes(x = 'residuals'))+\n",
    "  geom_histogram(bins = 21)).draw();\n",
    "\n",
    "plot3 = (ggplot(df,aes(x = 'fittedvalues',y = 'residuals'))+\n",
    "  geom_point()).draw();\n",
    "\n",
    "plot4 = (ggplot(df,aes(sample='residuals'))+\n",
    "  geom_qq()+\n",
    "    geom_qq_line()).draw();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795cef2f-8224-4bb5-97b9-622db49f0b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a5d01-d3ae-4c36-84f6-09b5de736e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4421a9-2d3c-47a1-9be1-83bafe407120",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2350a2-36d7-4a99-b950-cd17274f1dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f68974",
   "metadata": {},
   "source": [
    "1.    The scatter plot looks odd with the \"bulge\" in the middle.\n",
    "2.    Histogram of residuals doesn't look symmetric (i.e. not Gaussian)\n",
    "3.    Residuals vs Fitted Values looks off again \"bulge\" in middle. \n",
    "4.    Q-Q Plot looks off too, errors are probably not Gaussian, appear herteroskedastic.  Need to consider more looking into things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f3818",
   "metadata": {},
   "source": [
    "2.    A proving ring is a device for measuring load based on deflection.  The data set `provingring` results include the deflection for several know loads and three separate repetitions (runs).  Fit a linear model relating the deflection to the load.  Is there any evidence of hysteresis?  Examine the model results and the residuals, is there any way to improve the results, e.g.\\ a way to more accurately predict the deflection from the load?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c5ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv(\"../DATA/provingring.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ac088",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df = pandas.read_csv(\"../DATA/provingring.csv\")\n",
    "  \n",
    "model = statsmodels.formula.api.ols(\"Deflection~Load\",df)\n",
    "\n",
    "results = model.fit()\n",
    "\n",
    "results.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fdd3f0",
   "metadata": {},
   "source": [
    "Now, perform diagnostics on the model residuals and comment on the results.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e500d7-c52c-4320-9525-1ad706266ce4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df = pandas.read_csv(\"../DATA/provingring.csv\")\n",
    "\n",
    "model = statsmodels.formula.api.ols(\"Deflection~Load\",df)\n",
    "\n",
    "results = model.fit()\n",
    "\n",
    "results.summary()\n",
    "\n",
    "df[\"fittedvalues\"] = results.fittedvalues\n",
    "df[\"residuals\"] = results.resid\n",
    "\n",
    "plot1 = (ggplot(df,aes(x = 'Load', y = 'Deflection'))+\n",
    "  geom_point()+\n",
    "    geom_smooth(method = \"lm\")).draw();\n",
    "\n",
    "plot2 = (ggplot(df,aes(x = 'residuals'))+\n",
    "  geom_histogram(bins = 21)).draw();\n",
    "\n",
    "plot3 = (ggplot(df,aes(x = 'fittedvalues',y = 'residuals'))+\n",
    "  geom_point()).draw();\n",
    "\n",
    "plot4 = (ggplot(df,aes(sample='residuals'))+\n",
    "  geom_qq()+\n",
    "    geom_qq_line()).draw();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2067cf83-2426-44d3-9436-668a537bd1a3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb7673d-91eb-4ac9-b102-f086abc2c0d2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e2c88-533b-4810-b932-984e65dd057e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plot3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c849e6d-0f2d-452b-b760-b4c519b8aa84",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plot4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa5aa34",
   "metadata": {},
   "source": [
    "1.    Scatterplot looks good, a little \"too\" good.\n",
    "2.    Historgram does not look Gaussian\n",
    "3.    Residuals versus Fitted Values has a parabolic shape, we should add a quadratic term.\n",
    "4.    Q-Q Plot looks a bit off, there appears to be censoring in the measurements, definitely not Gaussian.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1615ca6d-34ce-4df6-924c-c43b88a2f809",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv(\"../DATA/provingring.csv\")\n",
    "df[\"Load2\"] = df.Load**2\n",
    "\n",
    "model = statsmodels.formula.api.ols(\"Deflection~Load+Load2\",df)\n",
    "\n",
    "results = model.fit()\n",
    "\n",
    "results.summary()\n",
    "\n",
    "df[\"fittedvalues\"] = results.fittedvalues\n",
    "df[\"residuals\"] = results.resid\n",
    "\n",
    "plot1 = (ggplot(df,aes(x = 'Load', y = 'Deflection'))+\n",
    "  geom_point()+\n",
    "    geom_smooth(method = \"lm\")).draw();\n",
    "\n",
    "plot2 = (ggplot(df,aes(x = 'residuals'))+\n",
    "  geom_histogram(bins = 21)).draw();\n",
    "\n",
    "plot3 = (ggplot(df,aes(x = 'fittedvalues',y = 'residuals'))+\n",
    "  geom_point()).draw();\n",
    "\n",
    "plot4 = (ggplot(df,aes(sample='residuals'))+\n",
    "  geom_qq()+\n",
    "    geom_qq_line()).draw();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb900c-5cd1-41bd-9c2f-75c3f30ad6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd0ee54-1128-4878-9b62-6b3367e0abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96496362-3284-44f8-8be4-27f0923daca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5845cef-b885-4178-aa85-b44fb48d3542",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af903b74",
   "metadata": {},
   "source": [
    "1.    Histogram now appears bi-modal\n",
    "2.    Resdiuals versus Fitted Values looks better, but still \"bulgy\"\n",
    "3.    Q-Q Plot looks better but still looks like censored measurements.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4835fe1a-8206-403a-868f-7eca9f3b4a18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "warning,tags,message,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
