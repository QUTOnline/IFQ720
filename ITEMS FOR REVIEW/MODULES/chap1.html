<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Module 1</title>

<script src="site_libs/header-attrs-2.14/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<head>
<link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

</head>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="QUTReadings.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">IFQ720</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="contact.html">Contacts</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Readings
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="chap1.html">Module 1</a>
    </li>
    <li>
      <a href="chap2.html">Module 2</a>
    </li>
    <li>
      <a href="chap3.html">Module 3</a>
    </li>
    <li>
      <a href="chap4.html">Module 4</a>
    </li>
    <li>
      <a href="chap5.html">Module 5</a>
    </li>
    <li>
      <a href="chap6.html">Module 6</a>
    </li>
    <li>
      <a href="chap7.html">Module 7</a>
    </li>
    <li>
      <a href="chap8.html">Module 8</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Workshops
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-01/">Workshop 1</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-02/">Workshop 2</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-03/">Workshop 3</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-04/">Workshop 4</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-05/">Workshop 5</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-06/">Workshop 6</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-07/">Workshop 7</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-08/">Workshop 8</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-09/">Workshop 9</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-10/">Workshop 10</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-11/">Workshop 11</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-12/">Workshop 12</a>
    </li>
  </ul>
</li>
<li>
  <a href="assessment.html">Assessments</a>
</li>
<li>
  <a href="videos.html">Videos</a>
</li>
<li>
  <a href="https://blackboard.qut.edu.au/webapps/blackboard/execute/announcement?method=search&amp;context=course&amp;course_id=_164898_1&amp;handle=cp_announcements&amp;mode=reset">IFQ720 Canvas</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="mailto:gentry.white@qut.edu.au">
    <span class="fa fa-envelope fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://mxb1072022.slack.com">
    <span class="fab fa-slack fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://https://qutvirtual4.qut.edu.au/web/qut/hiq">
    <span class="fa fa-users"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<div><a href="https://www.qut.edu.au">
     <img alt="QUT" src="logo.png" width=50px" align="left" style="vertical-align:middle;margin:10px 10px 10px 0px"></a></div>

<div id="header">



<h1 class="title toc-ignore">Module 1</h1>
<h3 class="subtitle">Data and Sources of Uncertainty</h3>

</div>

<div id="TOC">
<ul>
<li><a href="#introduction" id="toc-introduction">Introduction</a>
<ul>
<li><a href="#data" id="toc-data">Data</a></li>
<li><a href="#summarising-and-describing-data"
id="toc-summarising-and-describing-data">Summarising and Describing
Data</a></li>
<li><a href="#charts-vs-plots-vs-graphs"
id="toc-charts-vs-plots-vs-graphs">Charts vs Plots vs Graphs</a></li>
<li><a href="#numerical-summaries-of-data"
id="toc-numerical-summaries-of-data">Numerical Summaries of
Data</a></li>
</ul></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>Introduction</h1>
<div class="sidenote">
<p><img src="Hollerith.jpg" /> <em>Herman Hollerith and Big
Data</em></p>
<p>We often hear the phrase “big data”, describing the incredible rate
at which data are produced and the problem of drawing understanding and
meaning from it. However, history teaches us that this problem is
nothing new.</p>
<p>The story of Herman Hollerith and the U.S. Census of 1890 gives us a
great insight into the problem of big data and its connection to the
history of computers and computational methods. Read more <a
href="https://en.wikipedia.org/wiki/Herman_Hollerith">here</a>.</p>
</div>
<p>Introducing IFQ720 etc.</p>
<!--
## What is Statistics

Every first-year test on Statistics starts with a section titled "What is Statistics?".  In keeping with this tradition, we will try to briefly discuss the scope of the field of Statistics as it applies to this unit.  Statistics is a field of Mathematics that deals with data, summarising data, constructing probabilistic models, estimating parameters, and making statistical inference. In short, Statistics is the science of extracting meaning from data.   

Statistics is an approach to using mathematical tools to help us understand data and find a basis for making informed decisions.  Statistics represents a set of fundamental techniques for understanding data based on the idea that probabilistic models accurately describe real-world phenomena. We can use this idea to develop a broad range for understanding all varieties of data. More than any other science studying statistics gives us a powerful understanding of all the data that confronts us and insight into the world we live in.

Statistics is a mathematical approach to understanding and finding meaning in these data.  It is in this understanding that we can find a basis for making informed decisions.  Statistics represents a set of fundamental approaches for understanding data based on the idea that probabilistic models accurately describe real-world phenomena, rules for interpreting the uncertainty in observed data, and estimating parameters and performing statistical tests. Thus, more than any other science studying statistics gives us a powerful understanding of all the data that confronts us and insight into the world we live in.
-->
<!--


### Making decisions based on information

Ultimately we want to collect data to inform our decisions; a rough description of how we interact with the world begins with being confronted by choices about things we don't know.  Where should I go for school holidays, what car should I buy, what course should I study at Uni?  The results of our decisions depend on reality, which is typically not known.  If I go to place A for school holidays will I have fun, will there be things I like to do, will it rain, can I afford it?  If I buy car B versus car C, which one will cost more?  which one is safer, which one will be the least expensive to own? Which course of study will I like the most, which will give me a better future, which one will make me the most money? All of these are questions about a reality that we may know nothing about, and to make our decisions, we need to ask more questions.  Which place for school holiday has more of the activities that I enjoy, how much does each one cost?  What is the purchase price for car B and car C, what is the mileage for car B and car C, how much will the insurance be for car B versus car C? Which degree has the highest salary, how much will a course of study cost me, which course of study has the most subjects I enjoy?  In all these cases, we translate our initial questions into questions about things we can count, rank, or otherwise measure as a number.  In translating these initial questions, we must be careful to collect the right information to make our informed decision.  Statistical modelling is all about finding the link between our initial questions and the things we can count, rank, or measure, collecting data, and using probability and other mathematical tools to answer these questions. 
Ultimately we want to collect data to inform our decisions; a rough description of how we interact with the world begins with being confronted by choices about things we don't know.  Where should I go for school holidays, what car should I buy, what course should I study at Uni?  The results of our decisions depend on reality, which is typically not known.  If I go to place A for school holidays will I have fun, will there be things I like to do, will it rain, can I afford it?  If I buy car B versus car C, which one will cost more?  which one is safer, which one will be the least expensive to own? Which course of study will I like the most, which will give me a better future, which one will make me the most money? All of these are questions about a reality that we may know nothing about, and to make our decisions, we need to ask more questions.  Which place for school holiday has more of the activities that I enjoy, how much does each one cost?  What is the purchase price for car B and car C, what is the mileage for car B and car C, how much will the insurance be for car B versus car C? Which degree has the highest salary, how much will a course of study cost me, which course of study has the most subjects I enjoy?  In all these cases, we translate our initial questions into questions about things we can count, rank, or otherwise measure as a number.  In translating these initial questions, we must be careful to collect the right information to make our informed decision.  Statistical modelling is all about finding the link between our initial questions and the things we can count, rank, or measure, collecting data, and using probability and other mathematical tools to answer these questions. 


:::{.sidenote}
A quote to remember when thinking about collecting data:

"The plural of anecdote is not data."  ---Professor Gary LaFree. 

Where anecdotes are casual observations, data are carefully collected to record facts accurately. This care ensures that the conclusions we draw from data are reliable.  
:::


## The Elements of Statistical Modelling

Statistical modelling is in some ways art rather than a science because it requires that we see the world both in terms of our very real questions and cast those questions in mathematical terms. We answer these questions using mathematical tools, and finally, bring our mathematical answer back into the real world to answer our question. Statistical modelling involves many choices along the way; some paths lead to the same answers, and some lead us astray.  Understanding these choices and their impact can (with the help of some guiding principles) lead us to more accurate and reliable answers.  The basic components used in statistical modelling include our questions, data, and a mathematical model for connecting all three of these in a coherent, accurate, and useful manner. 
-->
<div id="data" class="section level2">
<h2>Data</h2>
<p>Data are a collection of numbers that describe some characteristic(s)
that can be ranked, counted, or measured. This definition is pretty
abstract, but it bears consideration. Think about questions like “how
blue is the ocean?” or “who is the best footballer?”. Concepts like
colour or “best” can be subjective, meaning that their interpretation
depends on the individual. To answer these kinds of questions, we have
to convert them into questions about things that can be ranked, counted,
or measured. For example, we can measure the frequency of waves for
colour; for footballers, we can count goals or classify them according
to other measures. So, this combination of data and question(s) makes up
part of statistical modelling.</p>
<div id="collecting-information" class="section level3">
<h3>Collecting information</h3>
<p>Statistical modelling relies on data, but collecting data can be
fraught with issues or reliability and accuracy. How do we collect data
to ensure that the answers it leads us to are good? Often the data,
their source, what was collected, and how much was collected are
overlooked. We should ask ourselves what questions we are trying to
answer, what information we need to answer these questions and the best
source for that information? In reality, data are the most important
part of statistical modelling, but collecting a good set of data can
seem deceptively easy and trivial. It is important to remember that the
answers you get from any statistical modelling technique always depends
on the data. We should always consider this when we start our process of
statistical modelling.</p>
</div>
<div id="randomness" class="section level3">
<h3>Randomness</h3>
<p>Everything in the world is different. Two flowers of the same breed
will differ in petal length or other measures; even manufactured objects
and machines can vary among supposedly identical artefacts. We refer to
this as randomness, which is in turn, becomes uncertainty in our data.
Random events are events whose exact outcome cannot be predicted ahead
of time. We assume that all the variation in the world we observe is due
to randomness; we can think of this randomness as manifesting itself as
uncertainty in our world knowledge. This randomness is not a good or bad
thing, and in most cases, it isn’t something we have any control over.
Instead, we have to be aware that all our data will contain randomness.
As a result, our data will contain uncertainty, as will our resulting
decisions will all be made under the cloud uncertainty. Thus our
decision-making process will have to account for this uncertainty. So
the next component we need in statistical modelling is a tool for
dealing with uncertainty and randomness.</p>
</div>
<div id="probability" class="section level3">
<h3>Probability</h3>
<p>Probability is a mathematical construct for dealing with randomness
and uncertainty. It turns out to be a very useful and powerful tool.
Probability gives us a whole suite of rules for describing, quantifying,
and calculating the uncertainty associated with our data and the answers
we get from it. It is important to note that the concept of probability
is philosophically challenging for statistical modelling. This unit will
focus on the mathematical construct of probability and its associated
rules and properties.</p>
<div id="experimental-units-and-measurements" class="section level4">
<h4>Experimental Units and Measurements</h4>
<p>The concept of an experiment will be formally defined later in this
unit when we perform a comprehensive review of probability. But for now,
we can define what we mean by ``experimental’’ unit and
measurements.</p>
<div class="boxed">
<p><strong>Definition:</strong> An <em>Experimental Unit</em> is an
individual that generates information for the data collection process.
Careful consideration of what constitutes an experimental unit must be
made to ensure that it aligns with the questions of interest.</p>
</div>
<p>If we wanted to know the most popular ice cream flavour, we would ask
individual people their favourite flavours. In this case, the individual
people would be the experimental units. In another case, we might want
to compare different high schools to see which school was most
successful at placing students at university. In this case, we would
look at what proportion of students from each school went onto Uni.
Because our question is about the schools, they would be the
experimental unit, not the individual students.</p>
<p>In these examples, the measurement, i.e. the information collected
from the experimental units, is directly related to the question of
interest. Thus when designing a data collection plan, we must carefully
consider selecting the right measurement and the proper experimental
unit.</p>
<!-- ### Levels of Measurement -->
<!-- Data can be a nebulous concept, and our choice of mathematical methods for statistical modelling can depend on the specifics of the data itself. To help guide our selection of methods, we can classify data into levels of measurement.   -->
<!-- * Nominal -->
<!-- * Ordinal -->
<!-- * Interval  -->
<!-- * Ratio -->
<!-- #### Nominal -->
<!-- Nominal data are sometimes called qualitative data; they consist of classifications or group membership.   -->
<!-- #### Ordinal  -->
<!-- #### Interval  -->
<!-- #### Ratio -->
</div>
</div>
<div id="sample-versus-population" class="section level3">
<h3>Sample versus Population</h3>
<div class="sidenote">
<p><strong>Example:</strong><br />
Political polling is a very challenging example of trying to answer
questions about a population based on a sample. Pollsters take great
pains to ensure that the sample they are using has the same demographic
profile as the population of interest so that their polling results will
be accurate. They can use various methods to do this but often rely on
reweighting answers to account for the differences between their sample
and the population.</p>
</div>
<p>When performing statistical modelling, we might have questions about
a very large collection of things called a population. In some cases, we
can collect data from an entire population to answer our question, but
this is not feasible in many cases. In these cases, we rely on using a
sample of the population.</p>
<div class="boxed">
<p><strong>Definition:</strong> A <em>sample</em> is a subset of a
population. If we want to answer questions about a population, often a
sample is measured as part of the data collection process. In this case,
we want to make sure we construct our sample so that the results we
obtain are valid.</p>
</div>
<p><br />
We would like our sample to be representative of the population and
should take care to ensure this. Doing this makes us more confident that
the results of our statistical modelling will apply to the population
and not just the sample.</p>
<p>The problem of collecting a representative sample can become quite
challenging depending on the questions of interest. In some cases, a
<em>random sample</em> is sufficient.</p>
<div class="boxed">
<p><strong>Definition:</strong> A <em>random sample</em> is one where
the sample members are selected from the population by chance.</p>
</div>
<p>If we wanted to know the average height of students at QUT, we might
randomly select a group of students to measure. Because males tend to be
taller than females, we assume that the proportion of males to females
in our random sample will be the same as the population. In other cases,
random sampling isn’t enough.</p>
</div>
<div id="univariate-bivariate-and-multivariate" class="section level3">
<h3>Univariate, Bivariate, and Multivariate</h3>
<div class="sidenote">
<center>
<img src="fisher.JPG" />
</center>
<ul>
<li>R.A. Fisher and smoking*</li>
</ul>
<p>The mathematician R. A. Fisher was one of the brilliant minds of the
twentieth century, responsible for developing much of the mathematical
underpinning of modern statistical methods and many of the tools we will
learn in this unit. However, he stubbornly resisted claims of a link
between smoking and cancer. Why?</p>
<p>Fisher noted that the initial evidence to support claims of a link
between smoking and cancer was based on a survey of U.K. physicians to
determine if they smoked and if they contracted any form of cancer.
Fisher claimed that because the scientist conducting the study didn’t
control the treatment (smoking or not smoking), there could be biases in
the data because of some other factor not considered.</p>
<p>Fisher’s solution to this problem was randomisation. By randomising
the treatment assignment to subjects, the theory is that any other
effects would cancel each other out by being distributed randomly
through all the subjects and not systematically associated with the
treatment. Randomisation or data collected from what are called
<strong>Randomised Controlled Trials (RCTs)</strong> are today
considered the “gold standard” of data for pursuing scientific
research.</p>
<p>Fisher’s protestations faded overtime under the onslaught of
additional evidence, both observational and experimental, supporting a
connection between smoking and cancer. And it is possible that the fact
that Fisher himself was a lifelong smoker might have let personal bias
affect his opinions, proof that even the most brilliant minds are
susceptible to personal biases clouding their judgement.</p>
</div>
<p>We can also describe data in terms of dimension, that is, how many
measurements were collected from each experimental unit. By collecting
multiple measurements from each experimental unit, we can answer
questions about the relationship between the measurements.</p>
<p>Assume that we draw a random sample of students from QUT and record
their gender, height, and weight. We could then answer questions, like
“are males taller than females?” or “what is the nature of the
relationship between height and weight?”.</p>
<p><br />
</p>
<p>Suppose we only collect a single measurement from each experimental
unit. In that case, the resulting dataset is
<strong>univariate</strong>. If two measurements are collected, then the
dataset is <strong>bivariate</strong>. If more than two measurements are
taken, then the resulting dataset is <strong>multivariate</strong>. It
is important to distinguish and recognise that differences between data
types can affect the possible analyses and results from statistical
modelling.</p>
<div id="experimental-versus-observational-data" class="section level4">
<h4>Experimental versus Observational Data</h4>
<p>So far, we have discussed data under the assumption that you, as the
researcher, will be collecting the data in a controlled manner. In
reality, many datasets have been collected and curated without any
specific analyses or modelling in mind. These data sets are called
<strong>observational</strong> data. By contrast, when a collection
procedure is specifically designed to obtain data with a specific
intent, e.g. a laboratory experiment, these data are called
<strong>experimental</strong> data. Experimental data is collected under
controlled circumstances, seeking to account for any extraneous effects
not of interest to the research question. Observational data may contain
biases that can limit its usefulness and bias any modelling or analyses
results. While experimental data is preferable or the “gold standard” in
research, in many cases, it is just not possible to obtain experimental
data, whether the limitations be logistical, ethical, or legal.</p>
<p>Quantitative data are things that are naturally represented
numerically. Dimensions like length, area, time, and scalar measures of
intensity are examples of quantitative data. So also are counts of
things or rankings. While there are other classification schemes for
quantitative data, we will restrict ourselves to distinguishing between
discrete, continuous, and ordinal measures. These distinctions again are
important as they guide our choices in modelling and analyses.</p>
</div>
<div id="discrete" class="section level4">
<h4>Discrete</h4>
<p>Discrete data are defined as observations that occur as natural or
whole numbers. In the broadest sense, this includes anything counted,
i.e. the number of things that happen. Note that this applies only to
items that occur as whole entities.<br />
The score in many sporting events like football or cricket are discrete
values; there are no half or fractional points awarded for scoring.
Hence the number of points scored by an individual or a team is
considered a discrete variable.</p>
</div>
<div id="continuous" class="section level4">
<h4>Continuous</h4>
<p>By contrast, continuous data are measurements on a continuum or
measures that may be subdivided infinitely. Things like individuals’
heights and weights are continuous data, as are things like time between
events. Often continuous data can appear to be discrete due to the way
they are measured and recorded.<br />
We may record time in hours, minutes and seconds, but it is not
discrete. Instead, it is by the limits of our measuring devices or how
it is recorded. Similarly, length or weight measurements can appear as
discrete due to limitations in the accuracy of the measuring
instruments.</p>
<p>It is important to distinguish between discrete and continuous based
on the natural occurrence of the observations, rather than the
limitations of measurement schemes or recording methods.</p>
</div>
<div id="ordinal" class="section level4">
<h4>Ordinal</h4>
<p>Ordinal data is data where the order or ranking of values (either
discrete or continuous) is important. Temperature is one example of
ordinal data that is easy to conceptualise, we understand intuitively
that “hotter” and “colder” are important (and natural) concepts.</p>
<p>Naturally occurring ranking, i.e. customer rankings on surveys, are
ordinal data. When customers are asked to rank customer service or other
traits on a scale from 1 to 10, the data are ordinal (and discrete)
since it is explicit that five is ``better’’ than four. Academic
achievement scores are also ordinal (and continuous) as they are ranked
on a scale from 1 to 100 as a percentage or measure of achievement for
learning outcomes.</p>
<p>Order in data can affect how they are modelled and how the results
are interpreted; ordinal data is typically used to answer questions
about relative rankings or extrema for phenomena, e.g. questions about
maximum rainfall or relationships between other variables and rank.</p>
</div>
<div id="qualitative-or-categorical-data" class="section level4">
<h4>Qualitative or Categorical Data</h4>
<div class="sidenote">
<p><strong>Example:</strong></p>
<p>Consider the choice of attending university or not attending
university. In this instance, the variable of interest is attending or
not (i.e. membership in the group ``university students’’). We would be
interested in developing models and analyses to examine which other
variables or factors influenced students to attend university.</p>
</div>
<p>Qualitative data are data we collect where the variable of interest
is membership in a group or category. The simplest version of this is
cases where the question of interest is the occurrence or non-occurrence
of an event.<br />
Qualitative or categorical data are converted to numerical measures for
modelling and analyses by counting the number of members in each group
or category. This way, the data become discrete and can be modelled and
analysed similarly to other discrete data. What differs is that
typically, we are interested in other variables or factors that may
influence the occurrence of an event or membership in a group. Thus our
research questions will be about membership versus non-membership rather
than the actual counts.</p>
<div class="boxed">
<div style="max-width: 590px">
<div
style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe width="590" height="332" src="https://web.microsoftstream.com/embed/video/c0f2dbfd-fb5e-4cb6-8099-fa983ed69e09?autoplay=false&amp;showinfo=true" allowfullscreen style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
</iframe>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="summarising-and-describing-data" class="section level2">
<h2>Summarising and Describing Data</h2>
<p>Once we have collected or obtained data, it is often just a
collection of numbers that may or may not be particularly illuminating.
Initially, we are interested in getting a ``feel’’ for our data or
summarising the information in our data numerically and graphically.
This approach falls under the topic of descriptive statistics or
techniques for describing our data, in a sense letting the data tell its
own story.</p>
<div id="tables" class="section level3">
<h3>Tables</h3>
<p>Tables are the most immediate way of summarising a data set;
typically, we might organise data in a table with one row for each
subject and a column for each measurement.</p>
<div class="boxed">
<p><strong>Example:</strong></p>
<p><em>EPA Fuel Economy Data</em></p>
<p>The U.S. Department of Energy Office of Energy Efficiency and
Renewable Energy maintains the website <a
href="https://www.fueleconomy.gov"
class="uri">https://www.fueleconomy.gov</a> where you can download data
on fuel economy and emissions data for every make and model of car sold
in the U.S. for the model years 1984 through 2021 <a
href="https://www.fueleconomy.gov/feg/download.shtml"
class="uri">https://www.fueleconomy.gov/feg/download.shtml</a>. This
data set consists of entries for 42,426 cars with 83 characteristics for
each car recorded. These characteristics are quantitative and
qualitative; a subset of the data can be arranged in a table for easy
reference. Here we have compiled a reduced set of data
<code>epa_data</code> including nine variables.</p>
<pre><code>#&gt;     city  hwy   cyl  disp  ...        make                model      trans  year
#&gt; 1     19   25   4.0   2.0  ...  Alfa Romeo   Spider Veloce 2000     Manual  1985
#&gt; 2      9   14  12.0   4.9  ...     Ferrari           Testarossa     Manual  1985
#&gt; 3     23   33   4.0   2.2  ...       Dodge              Charger     Manual  1985
#&gt; 4     10   12   8.0   5.2  ...       Dodge  B150/B250 Wagon 2WD  Automatic  1985
#&gt; 5     17   23   4.0   2.2  ...      Subaru     Legacy AWD Turbo     Manual  1993
#&gt; 6     21   24   4.0   1.8  ...      Subaru               Loyale  Automatic  1993
#&gt; 7     22   29   4.0   1.8  ...      Subaru               Loyale     Manual  1993
#&gt; 8     23   26   4.0   1.6  ...      Toyota              Corolla  Automatic  1993
#&gt; 9     23   31   4.0   1.6  ...      Toyota              Corolla     Manual  1993
#&gt; 10    23   30   4.0   1.8  ...      Toyota              Corolla  Automatic  1993
#&gt; 
#&gt; [10 rows x 9 columns]</code></pre>
</div>
</div>
<div id="bar-charts" class="section level3">
<h3>Bar Charts</h3>
<div class="sidenote">
<p><strong>Example:</strong><br />
Create a barplot of the number of models produces by each manufacturer
in the data set.</p>
<p>Now create a Pareto plot of the same data; which plot tells a clearer
“story”?</p>
</div>
<p>Graphical depictions of the data can also be useful but are limited
in the number of variables displayed in one picture. The bar chart is
one of the simplest and easiest to understand. It is most useful for
categorical data; categories are listed on the <span
class="math inline">\(x\)</span>-axis of the plot, and bars for each
category are drawn with their heights corresponding to the counts for
that category. The simple bar chart can be very useful if the categories
are ordered from left to right in descending order counts. This plot is
sometimes called a <strong>Pareto Plot</strong> after the Italian
economist Vilfredo Pareto.</p>
</div>
<div id="example" class="section level3 tabset tabset-pills boxed">
<h3 class="tabset tabset-pills">Example</h3>
<div id="solution" class="section level4">
<h4>Solution</h4>
<p>We can create a barplot simply using the built-in functions of the
<code>ggplot2</code> package. The resulting plot is highly unreadable
and doesn’t provide any insight into the data. This confusion is because
there are many (138) manufacturers, and attempting to plot results for
all of them at once is confusing and possibly misleading as many
manufactures make very few models.</p>
<p>Creating the Pareto plot requires that we manipulate the data using
the <code>tidyverse</code> package to order the bars by count rather
than alphabetically by manufacturer.</p>
<pre class="python"><code>import plotnine
from plotnine import *
import pandas
epa_data = pandas.read_csv(&quot;epa_data.csv&quot;, index_col = 0)
df = pandas.DataFrame(epa_data)
ggplot(epa_data,aes(x = &#39;make&#39;))+geom_bar()+xlab(&quot;Manufacturer&quot;)+ylab(&quot;Number of models&quot;) + ggtitle(&quot;Number of Models by Make for the Top 10 Manufacturers&quot;)
#&gt; &lt;ggplot: (305923997)&gt;</code></pre>
<p><img src="chap1_files/figure-html/unnamed-chunk-2-1.png" width="614" style="display: block; margin: auto;" /></p>
</div>
<div id="code" class="section level4">
<h4>Code</h4>
<pre class="python"><code>
##  Simple Barplot

##  The ggplot command takes a data frame as an argument, 
##  you can also specify the aesthetics &quot;aes()&quot; to 
##  associate variables with specific axes of your plot

##  geom_bar() specifies that we want a bar plot, ggtitle adds the title and theme centres it
##  over the plot. 

import plotnine
from plotnine import *
import pandas
epa_data = pandas.read_csv(&quot;epa_data.csv&quot;, index_col = 0)
df = pandas.DataFrame(epa_data)

make_bar = (ggplot(epa_data,aes(x = &#39;make&#39;))
+geom_bar()+
xlab(&quot;Manufacturer&quot;)+
ylab(&quot;Number of models&quot;) + 
ggtitle(&quot;Number of Models by Manufacturers&quot;)
)</code></pre>
<pre class="python"><code>
##  Pareto Plot

##  We create a new data frame called &quot;df&quot; to count the number of models 
##  per manufacturer and aggregate by manufacturer.
##  We then use ggplot calling the first ten rows only and 
##  plot the results from largest to smallest from left to right.  
##  Finally, we add a title and centre it over the plot. 

manufacturer_list = epa_data[&#39;make&#39;].value_counts().index.tolist()

manufacturer_cat = pandas.Categorical(epa_data[&#39;make&#39;], categories=manufacturer_list)

manufacturer_count = epa_data[&#39;make&#39;].value_counts().nlargest(5)

# assign to a new column in the DataFrame

df = epa_data.assign(manufacturer_cat = manufacturer_cat)
df[&#39;count&#39;] = df.groupby(&#39;make&#39;)[&#39;make&#39;].transform(&#39;count&#39;)
df = df[df.make.isin(manufacturer_count.index)]

make_par = (ggplot(df)
 + aes(x=&#39;manufacturer_cat&#39;)
 + geom_bar(size=20)
 + labs(y=&#39;Count&#39;, x=&#39;Manufacturer&#39;, title=&#39;Number of Cars by Make&#39;) + 
ggtitle(&quot;Number of Models by Make for the Top 5 Manufacturers&quot;)
)</code></pre>
</div>
<div id="plot" class="section level4">
<h4>Plot</h4>
<pre><code>#&gt; &lt;ggplot: (305311498)&gt;</code></pre>
<p><img src="chap1_files/figure-html/unnamed-chunk-5-3.png" width="614" style="display: block; margin: auto;" /></p>
<pre><code>#&gt; &lt;ggplot: (308697107)&gt;</code></pre>
<p><img src="chap1_files/figure-html/unnamed-chunk-5-4.png" width="614" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="line-charts" class="section level3">
<h3>Line Charts</h3>
<div class="sidenote">
<p><strong>Example:</strong><br />
Fuel economy in vehicles is a major concern for manufacturers and
environmental policymakers, as well as consumers. Plot the average fuel
economy (EPA city rating for MPG) for each model year from 1984 to 2021
using a line chart.</p>
</div>
<p>Line charts illustrate a trend of change based on two quantitative
variables. Line Charts are especially useful for displaying trends over
time (or other ordinal variables). It is important to be clear on what
is being plotted. Often trends over time need to be aggregated,
e.g. plotting the average or median per year. Otherwise, the plots can
look too “busy” and be difficult to read.</p>
<p>While the resulting chart can explain overall trends, they can
obscure how much variability or “noise” is in the data and may be
misleading if the overall trend is obscured by variability.</p>
</div>
<div id="example-line-charts"
class="section level3 tabset tabset-pills boxed">
<h3 class="tabset tabset-pills">Example: Line Charts</h3>
<div id="solution-1" class="section level4">
<h4>Solution</h4>
<p>We can use the function <code>ggplot()</code> from the package
<code>ggplot2</code>, which is part of the <code>tidyverse</code>. The
<code>tidyverse</code> is a collection of packages performing many data
science tasks for manipulating data, performing exploratory data
analysis, and creating high-quality graphics.</p>
</div>
<div id="code-1" class="section level4">
<h4>Code</h4>
<pre class="python"><code>
##  We can use the data set `epa_data` directly in `ggplot`
##  to plot yearly averages for fuel economy 

yearly_city = epa_data.groupby([&#39;year&#39;])[&#39;city&#39;].mean().reset_index()
mean_mpg_year = (ggplot(yearly_city,aes(x = &#39;year&#39;, y = &#39;city&#39;))+
  geom_line()+
  xlab(&quot;Year&quot;)+
  ylab(&quot;City MPG&quot;)+
  ggtitle(&quot;EPA City MPG by Year&quot;)
  )</code></pre>
</div>
<div id="plot-1" class="section level4">
<h4>Plot</h4>
<pre><code>#&gt; &lt;ggplot: (308723220)&gt;</code></pre>
<p><img src="chap1_files/figure-html/unnamed-chunk-7-7.png" width="614" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="histograms" class="section level3">
<h3>Histograms</h3>
<div class="sidenote">
<p><strong>Example:</strong><br />
Use <code>ggplot</code> and the <code>isin()</code> function to create a
histogram of EPA city fuel economy measurements for 1990.</p>
</div>
<p>Histograms give a visual description of our data by “binning” or
grouping data into data ranges, then plotting bars with heights equal to
the count of the bins’ contents or the relative proportion of the bins’
contents. Histograms give us a picture of the shape of the data and help
identify patterns in the distribution of values.</p>
</div>
<div id="example-historgrams"
class="section level3 tabset tabset-pills boxed">
<h3 class="tabset tabset-pills">Example: Historgrams</h3>
<div id="solution-2" class="section level4">
<h4>Solution</h4>
<p>We can use the filter function along with the pipe operator
(<code>%&gt;%</code>) to extract only the data we want (the year 1990)
and then pass that directly to <code>ggplot()</code>.</p>
</div>
<div id="plot-2" class="section level4">
<h4>Plot</h4>
<pre class="python"><code>mpg_1990 = (ggplot(epa_data[epa_data[&#39;year&#39;].isin([1990])],aes(&#39;city&#39;))+
  geom_histogram(aes(y=after_stat(&#39;density&#39;)), binwidth = 2)+
  xlab(&quot;City MPG&quot;)+
  ylab(&quot;Relative Frequency&quot;)+
  ggtitle(&quot;EPA City MPG for 1990&quot;)
  )</code></pre>
<p>Note that the “binning” process is performed automatically by the
software used to create the plot; however, in most cases, we override
the automatic settings and select either the number of bins or the width
of each bin (<code>binwidth</code>).</p>
</div>
<div id="plot-3" class="section level4">
<h4>Plot</h4>
<p>Note that because we “saved” the plot as an object
(<code>mpg_1990</code>), we can display it just by entering its name in
the console:</p>
<pre class="python"><code>mpg_1990
#&gt; &lt;ggplot: (301270749)&gt;</code></pre>
<p><img src="chap1_files/figure-html/unnamed-chunk-9-9.png" width="614" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="charts-vs-plots-vs-graphs" class="section level2">
<h2>Charts vs Plots vs Graphs</h2>
<p>The terms “chart”, “plot”, and “graph” often get interchange in
normal usage. However, there are some more exact definitions of these
terms that we should use:</p>
<div class="boxed">
<ul>
<li>A <strong>chart</strong> is a visual display of data; this could be
a table, a graph, or a diagram</li>
<li>A <strong>graph</strong> is a diagram showing the relationship
between (typically two) variables, each measured along orthogonal
axes.<br />
</li>
<li>A <strong>plot</strong> is used as a synonym for graph but is less
precise in its definition; it also sometimes refers specifically to a
graph produced by a computer.</li>
</ul>
</div>
<div id="interpreting-graphical-descriptions" class="section level3">
<h3>Interpreting Graphical Descriptions</h3>
<p>Graphical descriptions of data should be viewed with a critical eye
looking to ensure that you get all the information you can about the
data from the graphical description. You need to look for different
things in different graphical depictions, but in general, you
should:</p>
<div class="boxed">
<ul>
<li>Look at the <span class="math inline">\(x\)</span> and <span
class="math inline">\(y\)</span> axes and make sure you understand what
they are measuring; units are important to note as well.</li>
<li>Think about how the graph or chart was made. What choices were made
in creating the graph, and how might different options change how the
graph looked. Would these changes alter your impression or conclusions
from the plot?</li>
<li>Are there any extreme points or , i.e. points that don’t seem to
“fit” with the rest of the data? These points or outliers typically
merit investigation to determine if they are accurate measurements or if
they result from either measurement or recording error.</li>
<li>For Pareto charts and histograms, the <span
class="math inline">\(y\)</span>-axis should be proportion or density,
not frequency; this makes comparing Pareto charts or histograms of
different distributions easier.</li>
</ul>
</div>
</div>
<div id="centrality" class="section level3">
<h3>Centrality</h3>
<div class="sidenote">
<p><strong>Example:</strong><br />
Create histograms for the 1990 EPA Fuel Economy Ratings for both City
and Highway driving. Try letting <code>ggplot</code> pick the <span
class="math inline">\(x\)</span>-axes and <span
class="math inline">\(y\)</span>-axes values automatically for each one
and then force them to match. Looking at the plots side-by-side, do you
see the difference?</p>
</div>
<p>Histograms are a graphical representation of the distribution or
density of observations. The first thing we should note about the
histogram is where on the <span class="math inline">\(x\)</span>-axis is
the data “centred”, this is relatively easy to approximate by eye, but
we will see more precise measures for this later. Additionally, we need
to note if that data appear “multi-modal”, are there more than one
“peak” or “centre” to the histogram. Sometimes this can be an artefact
of how we chose the number of bins or bin width, and experimenting with
different numbers of bins or bin widths can reveal this.</p>
</div>
<div id="example-1" class="section level3 tabset tabset-pills boxed">
<h3 class="tabset tabset-pills">Example</h3>
<div id="solution-3" class="section level4">
<h4>Solution</h4>
<p>We use the <code>geom_histogram</code> to make our histograms and</p>
<p>Notice that the City MPG ratings are centred between 10 and 20, and
the Highway ratings are centred just over 20. Also, note that because we
created both histograms with the same limits on the <span
class="math inline">\(y\)</span>-axis, we can easily see that the values
for Highway ratings are more varied or spread out.</p>
</div>
<div id="code-2" class="section level4">
<h4>Code</h4>
<pre class="python"><code>
##  Create a new dataframe df by filtering the data for 1990
##  pivoting the table to a &quot;long&quot; format creating a new variable &quot;type&quot; 
##  as either &quot;city&quot; or &quot;hwy&quot; with the mpg values labelled &quot;mpg&quot;.

df = epa_data[epa_data.year.isin([1990])]
df = df[[&quot;city&quot;,&quot;hwy&quot;]].reset_index()
df = pandas.melt(df,id_vars = [&quot;index&quot;],value_vars= [&quot;city&quot;,&quot;hwy&quot;], var_name = &quot;type&quot;, value_name = &quot;mpg&quot;)

##  We then use ggplot to create histograms specifying that we want the 
##  density, not the frequency as the heights of the bars.  Finally, 
##  We use facet_wrap to create the side-by-side plots with axes different f
##  or each type (scales = &quot;free&quot;)

mpg_hist_free = (
  ggplot(df,aes(&#39;mpg&#39;))+
  geom_histogram(aes(y = after_stat(&#39;density&#39;)),binwidth = 2)+
  facet_wrap(&#39;type&#39;,scales=&quot;free&quot;)+
  xlab(&quot;MPG&quot;)+
  ggtitle(&quot;MPG 1990&quot;)
  )

##  Now create the same plot but force the axes to match (scales = &quot;fixed&quot;)

mpg_hist_fixed = (
  ggplot(df,aes(&#39;mpg&#39;))+
  geom_histogram(aes(y = after_stat(&#39;density&#39;)),binwidth = 2)+
  facet_wrap(&#39;type&#39;,scales=&quot;fixed&quot;)+
  xlab(&quot;MPG&quot;)+
  ggtitle(&quot;MPG 1990&quot;)
  )</code></pre>
</div>
<div id="plot-4" class="section level4">
<h4>Plot</h4>
<pre class="python"><code>
mpg_hist_free
#&gt; &lt;ggplot: (308899459)&gt;</code></pre>
<p><img src="chap1_files/figure-html/unnamed-chunk-11-11.png" width="614" style="display: block; margin: auto;" /></p>
<pre class="python"><code>mpg_hist_fixed
#&gt; &lt;ggplot: (308868274)&gt;</code></pre>
<p><img src="chap1_files/figure-html/unnamed-chunk-11-12.png" width="614" style="display: block; margin: auto;" /></p>
<p>Why do you think these two sets of data look different? What do these
difference mean?</p>
</div>
</div>
<div id="skew" class="section level3">
<h3>Skew</h3>
<p>Another characteristic we need to examine for histograms is their
general shape as well as their skew. Skew is a deviation from symmetry
about the centre of the data. Skew is defined as either “right” skew
where the tail of the density or histogram is heavier on the right-hand
side, and “left” skew where the tail of the density is heavier on the
left-hand side.</p>
<div class="boxed">
<p><strong>Example:</strong></p>
<p><em>Skewed Distributions</em></p>
<pre><code>#&gt; &lt;ggplot: (308693767)&gt;</code></pre>
<p><img src="chap1_files/figure-html/unnamed-chunk-12-15.png" width="614" style="display: block; margin: auto;" /></p>
<pre><code>#&gt; &lt;ggplot: (308898804)&gt;</code></pre>
<p><img src="chap1_files/figure-html/unnamed-chunk-12-16.png" width="614" style="display: block; margin: auto;" /></p>
</div>
<div class="boxed">
<p><strong>Example:</strong></p>
<p><em>EPA City Fuel Economy Ratings for 1990</em></p>
<p>Returning to plot for EPA City Fuel economy ratings for 1990</p>
<pre><code>#&gt; &lt;ggplot: (301270749)&gt;</code></pre>
<p><img src="chap1_files/figure-html/unnamed-chunk-13-19.png" width="614" style="display: block; margin: auto;" /></p>
<p>Notice that the data are right-skewed; the right tail stretches
further on the <span class="math inline">\(x\)</span>-axis than on the
left. This phenomenon is, to some extent, a “natural”, as there is a
definite limit on the left, i.e. fuel economy has a lowest possible
value of <span class="math inline">\(0\)</span>. What are other
explanations for this skew possible?</p>
</div>
</div>
<div id="trends" class="section level3">
<h3>Trends</h3>
<p>The term trend refers to a pattern in one variable plotted against
another; typically, it applies to data plotted in a line chart with time
on the <span class="math inline">\(x\)</span>-axis. Trends are described
as a constant (first-derivative) pattern of either increasing or
decreasing values.</p>
<div class="boxed">
<p><strong>Example:</strong></p>
<p><em>Trends in the EPA City Fuel Economy Ratings</em></p>
<p>The line chart of the EPA City fuel economy ratings shows two trends
over 1984 to 2021. First from 1984 until the mid-90s, fuel economy
ratings fell slightly and then remained roughly flat until approximately
2007 when they began a significant continuing increase.</p>
<pre><code>#&gt; &lt;ggplot: (308723220)&gt;</code></pre>
<p><img src="chap1_files/figure-html/unnamed-chunk-14-21.png" width="614" style="display: block; margin: auto;" /></p>
<p>Recognising trends is a first step in analysing data; the next (and
more difficult) questions are are the trends significant, and what (if
anything) causes them?</p>
</div>
<div class="boxed">
<div style="max-width: 590px">
<div
style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe width="590" height="332" src="https://web.microsoftstream.com/embed/video/7fbe0c4a-1228-443a-8292-6f469736c36d?autoplay=false&amp;showinfo=true" allowfullscreen style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
</iframe>
</div>
</div>
</div>
</div>
</div>
<div id="numerical-summaries-of-data" class="section level2">
<h2>Numerical Summaries of Data</h2>
<div id="measures-of-centrality" class="section level3">
<h3>Measures of Centrality</h3>
<div class="sidenote">
<p><img src="Sir_Francis_Galton.jpg" alt="Sir Francis Galton" /><br />
<a href="https://en.wikipedia.org/wiki/Francis_Galton">Sir Francis
Galton</a> was a British <a
href="https://en.wikipedia.org/wiki/Polymath">polymath</a> who developed
many foundational statistical concepts including measures of centrality
and dispersion.</p>
<p><br> <strong>Example: Computing the Mean, Median, and
Mode</strong><br />
For the following data, compute the mean median and mode by hand and
using the built-in functions <code>mean</code> and <code>median</code>
in <code>R</code> and writing a function to find the mode.</p>
<p>Given the observations: <span class="math display">\[
6.8, 3.4, 4.2, 3.7, 1.5, 6.0, 4.1, 1.8, 5.2
\]</span> compute the mean and median.</p>
<p>Given the observations: <span class="math display">\[
8,  9, 14, 13, 13, 11,  9, 13, 15, 10
\]</span> compute the mean, median, and mode.</p>
<p> </p>
<p>Given the observations: <span class="math display">\[
17, 14, 12, 15, 24, 19, 23,  9, 22, 18, 17, 19, 14, 22, 13
\]</span> compute the mean, median, and mode.</p>
<p>Given the observations: <span class="math display">\[
2, 5, 6, 1, 6, 2, 6, 1, 2, 6, 4, 4, 4, 6, 5
\]</span> compute the mean, median, and mode.</p>
</div>
<p>In Week 1, histograms are presented to summarise graphically two
aspects of data: centrality and dispersion. Envisioning a histogram, you
can imagine the <span class="math inline">\(x\)</span>-axis representing
the set of possible values for observations in the data set. The shape
or pattern to the distribution of these observations gives rise to a
desire to measure or identify some central point of measure of
centrality to summarise the data.</p>
<div id="mean" class="section level4">
<h4>Mean</h4>
<p>Given a set of observations or measurements <span
class="math inline">\(x_1,x_2,\ldots,x_n\)</span> in a data set. The
arithmetic <strong>mean</strong> or average is one of the most common
measures of centrality or central tendency, it is defined as <span
class="math display">\[\frac{\sum_{i=1}^nx_i}{n}\equiv
\frac{x_1+x_2+\cdots+x_n}{n}.\]</span> The notation <span
class="math inline">\(\sum_{i=1}^nx_i\)</span> is summation notation,
meaning add up all the <span class="math inline">\(x_i\)</span> for
<span class="math inline">\(i=1,2,3,\ldots,n\)</span>. Sometimes this
is”abbreviated” as <span class="math inline">\(\sum x_i\)</span> or
<span class="math inline">\(\sum x\)</span> meaning add up all the <span
class="math inline">\(x\)</span>’s. By dividing the sum of the <span
class="math inline">\(x_i\)</span>’s we can find the average value of
the observations, or the “balance” point for the distribution of
values.</p>
<div class="boxed">
<p>If our data are a <em>sample</em> from a population, the sample mean
is <span class="math display">\[
\bar{x}=\frac{\sum_{i=1}^nx_i}{n}\qquad\mbox{(&quot;x-bar&quot;)}
\]</span> if we are using another random variable, e.g. <span
class="math inline">\(y\)</span>, we would denote the sample mean as
<span class="math inline">\(\bar{y}\)</span>.</p>
</div>
<p>The mean has several nice properties that we will explore and discuss
in more detail later, one nice feature of it is that it uses all the
data we observe, so we know that it is based on all the data and should
be our most “informed” summary measure.</p>
</div>
<div id="median" class="section level4">
<h4>Median</h4>
<p>The drawback to the mean is that it can be very sensitive or change a
lot based on extreme values or values that far away from the “centre” of
the data. This sensitivity is especially true for small sample sizes.
That is why it is always a good idea to compute the
<strong>median</strong> as well.</p>
<div class="boxed">
<p>The median is defined by taking all the observations <span
class="math inline">\(x_i\)</span> and sorting them from largest to
smallest. The median is the observation that occupies the middle
position.</p>
<p>If <span class="math inline">\(n\)</span> is odd, then the median is
<span class="math display">\[
\text{median}=x^{\left(\frac{n+1}{2}\right)}
\]</span> or the <span class="math inline">\((n+1)/2\)</span>th observed
value of <span class="math inline">\(x\)</span> ordered from smallest
(<span class="math inline">\(x^{(1)\)</span>) to largest (<span
class="math inline">\(x^{(n)}\)</span>). If <span
class="math inline">\(n\)</span> is even then the median is average of
the two observations in positions <span
class="math inline">\(n/2\)</span> and <span
class="math inline">\(n/2+1\)</span> <span class="math display">\[
\mbox{median}=\frac{x^{\left(\frac{n}{2}\right)}+x^{\left(\frac{n}{2}+1\right)}}{2}.
\]</span></p>
</div>
</div>
<div id="mode" class="section level4">
<h4>Mode</h4>
<p>The mode is most easily defined for discrete data as the most
frequently occurring observation, and in the context of discrete data,
where observations take on a finite number of values the mode can be a
useful and informative summary. We will see in Week 5 that the mode is
also defined for continuous random variables based on their probability
density function, and can be a useful summary measure in that context as
well.</p>
</div>
<div id="example-2" class="section level4 tabset tabset-pills boxed">
<h4 class="tabset tabset-pills">Example</h4>
<p>For the following data, compute the mean median and mode by hand and
using the built-in functions <code>mean</code> and <code>median</code>
in <code>R</code>, and writing a function to find the mode.</p>
<p>Given the observations: <span class="math display">\[
6.8, 3.4, 4.2, 3.7, 1.5, 6.0, 4.1, 1.8, 5.2
\]</span> compute the mean and median.</p>
<p>Given the observations: <span class="math display">\[
8,  9, 14, 13, 13, 11,  9, 13, 15, 10
\]</span> compute the mean, median, and mode.</p>
<p> </p>
<p>Given the observations: <span class="math display">\[
17, 14, 12, 15, 24, 19, 23,  9, 22, 18, 17, 19, 14, 22, 13
\]</span> compute the mean and median.</p>
<p>Given the observations: <span class="math display">\[
2, 5, 6, 1, 6, 2, 6, 1, 2, 6, 4, 4, 4, 6, 5
\]</span> compute the mean, median and mode.</p>
<div id="solution-4" class="section level5">
<h5>Solution</h5>
<p>For the first example where <span class="math inline">\(x=6.8, 3.4,
4.2, 3.7, 1.5, 6.0, 4.1, 1.8, 5.2\)</span> the sample mean is <span
class="math display">\[
\begin{align}
\bar{x}&amp;=\frac{\sum_{i=1}^nx_i}{n}\\
&amp;=\frac{6.8+ 3.4+ 4.2+ 3.7+ 1.5+ 6.0+ 4.1+ 1.8+ 5.2}{9}\\
&amp;=4.08
\end{align}
\]</span> To find the median, we sort the values of <span
class="math inline">\(x\)</span> from smallest to largest <span
class="math display">\[
x=1.5, 1.8, 3.4, 3.7, 4.1, 4.2, 5.2, 6.0, 6.8
\]</span> and because <span class="math inline">\(n=9\)</span> the
median is the <span class="math inline">\((n+1)/2=5\)</span>th value,
<span class="math display">\[
\text{median }x=4.1.
\]</span> For the second example where <span class="math inline">\(x=8,
9, 14, 13, 13, 11, 9, 13, 15, 10\)</span> the sample mean is <span
class="math display">\[
\begin{align}
\bar{x}&amp;=\frac{\sum_{i=1}^nx_i}{n}\\
&amp;=\frac{8+  9+14+13+13+11+9+13+15+10}{10}\\
&amp;=11.5
\end{align}
\]</span></p>
<p>To find the median, we sort the values of <span
class="math inline">\(x\)</span> from smallest to largest <span
class="math display">\[
x=8,  9,  9, 10, 11, 13, 13, 13, 14, 15
\]</span> and because <span class="math inline">\(n=10\)</span>, the
median is <span class="math display">\[
\begin{align}
\text{median }x&amp;=\frac{x^{(n/2)}+x^{(n/2+1)}}{2}\\
&amp;=\frac{11+13}{2}\\
&amp;=12.
\end{align}
\]</span></p>
<p>We can also see by inspection that the mode or the most frequently
occurring value is <span class="math display">\[
\text{mode }x=13.
\]</span> For the third example where <span class="math display">\[
x = 17, 14, 12, 15, 24, 19, 23,  9, 22, 18, 17, 19, 14, 22, 13
\]</span> the sample mean is <span class="math display">\[
\begin{align}
\bar{x}&amp;=\frac{\sum_{i=1}^nx_i}{n}\\
&amp;=\frac{17+ 14+ 12+ 15+ 24+ 19+ 23+  9+ 22+ 18+ 17+ 19+ 14+ 22+
13}{15}\\
&amp;=17.2
\end{align}
\]</span> To find the median, we sort the values of <span
class="math inline">\(x\)</span> from smallest to largest <span
class="math display">\[
x= 9, 12, 13, 14, 14, 15, 17, 17, 18, 19, 19, 22, 22, 23, 24
\]</span> and because <span class="math inline">\(n=15\)</span> the
median is the <span class="math inline">\((n+1)/2=5\)</span>th value,
<span class="math display">\[
\text{median }x=17.
\]</span> For the fourth example where <span class="math display">\[
x = 2, 5, 6, 1, 6, 2, 6, 1, 2, 6, 4, 4, 4, 6, 5
\]</span> the sample mean is <span class="math display">\[
\begin{align}
\bar{x}&amp;=\frac{\sum_{i=1}^nx_i}{n}\\
&amp;=\frac{2+ 5+ 6+ 1+ 6+ 2+ 6+ 1+ 2+ 6+ 4+ 4+ 4+ 6+ 5}{15}\\
&amp;=4
\end{align}
\]</span> To find the median, we sort the values of <span
class="math inline">\(x\)</span> from smallest to largest <span
class="math display">\[
x=  1, 1, 2, 2, 2, 4, 4, 4, 5, 5, 6, 6, 6, 6, 6
\]</span> and because <span class="math inline">\(n=15\)</span> the
median is the <span class="math inline">\((n+1)/2=5\)</span>th value,
<span class="math display">\[
\text{median }x=4.
\]</span> We can also see by inspection that the mode or the most
frequently occurring value is <span class="math display">\[
\text{mode }x=6.
\]</span></p>
</div>
<div id="code-3" class="section level5">
<h5>Code</h5>
<pre class="python"><code>import numpy

x = numpy.array([6.8, 3.4, 4.2, 3.7, 1.5, 6.0, 4.1, 1.8, 5.2])
x.mean()
#&gt; 4.077777777777778
numpy.mean(x)
#&gt; 4.077777777777778
numpy.median(x)
#&gt; 4.1
numpy.sort(x)
#&gt; array([1.5, 1.8, 3.4, 3.7, 4.1, 4.2, 5.2, 6. , 6.8])
x = numpy.array([8,  9, 14, 13, 13, 11,  9, 13, 15, 10])
x.mean()
#&gt; 11.5
numpy.mean(x)
#&gt; 11.5
numpy.median(x)
#&gt; 12.0
numpy.sort(x)
#&gt; array([ 8,  9,  9, 10, 11, 13, 13, 13, 14, 15])
x = numpy.array([17, 14, 12, 15, 24, 19, 23,  9, 22, 18, 17, 19, 14, 22, 13])
x.mean()
#&gt; 17.2
numpy.mean(x)
#&gt; 17.2
numpy.median(x)
#&gt; 17.0
numpy.sort(x)
#&gt; array([ 9, 12, 13, 14, 14, 15, 17, 17, 18, 19, 19, 22, 22, 23, 24])
x = numpy.array([2, 5, 6, 1, 6, 2, 6, 1, 2, 6, 4, 4, 4, 6, 5])
x.mean()
#&gt; 4.0
numpy.mean(x)
#&gt; 4.0
numpy.median(x)
#&gt; 4.0
numpy.sort(x)

#&gt; array([1, 1, 2, 2, 2, 4, 4, 4, 5, 5, 6, 6, 6, 6, 6])</code></pre>
</div>
<div id="videos" class="section level5">
<h5>Videos</h5>
<div style="max-width: 590px">
<div
style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe width="590" height="332" src="https://web.microsoftstream.com/embed/video/72655bc2-4b0f-46ce-b766-46221a8284c1?autoplay=false&amp;showinfo=true" allowfullscreen style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
</iframe>
</div>
</div>
<div style="max-width: 590px">
<div
style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe width="590" height="332" src="https://web.microsoftstream.com/embed/video/d31eaa00-64fe-4600-8ad3-dc1d1d7c804f?autoplay=false&amp;showinfo=true" allowfullscreen style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
</iframe>
</div>
</div>
<div style="max-width: 590px">
<div
style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe width="590" height="332" src="https://web.microsoftstream.com/embed/video/95af55c4-3d99-42a7-9f52-f2e937ccf3f5?autoplay=false&amp;showinfo=true" allowfullscreen style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
</iframe>
</div>
</div>
</div>
</div>
<div id="population-versus-sample-measures-of-centrality"
class="section level4">
<h4>Population versus Sample Measures of Centrality</h4>
<p>When we talk about developing numerical summaries, it also makes
sense that we discuss them in terms of samples versus populations.
Suppose a numerical summary is calculated from a sample. In that case,
it stands to reason that depending on the sample; we might get different
quantities even though we calculated the summary the same way in each
instance. This phenomenon leads to the notion of uncertainty in
summaries calculated from samples. If a numerical summary is calculated
from a population, then the numerical summary is constant, and no
uncertainty is associated with its value.</p>
<div class="boxed">
<p>The mean of a finite <em>population</em> is computed the same way as
the mean for a sample. The only difference is in the notation that we
use; if the observations or measurements are from a population, then we
use the symbol <span class="math inline">\(\mu\)</span> <span
class="math display">\[
\mu=\frac{\sum_{i=1}^nx_i}{n}\qquad\mbox{(&quot;mew&quot;)}
\]</span> to represent the population mean.</p>
</div>
<p>Numerical summaries of sample data are called
<strong>statistics</strong> and numerical summaries of populations data
are called <strong>parameters</strong>.</p>
<p>It is important to recognise the distinction between parameters and
statistics. How we interpret their meaning depends greatly on this
distinction.</p>
</div>
</div>
<div id="measures-of-dispersion" class="section level3">
<h3>Measures of Dispersion</h3>
<div class="sidenote">
<p>For the following data, compute the variance, standard deviation, and
range both by hand and using the built-in functions <code>var</code>,
<code>sd</code> and <code>range</code>.</p>
<p><span class="math display">\[
x = 6.8, 3.4, 4.2, 3.7, 1.5, 6.0, 4.1, 1.8, 5.2
\]</span></p>
<p><span class="math display">\[
x = 8,  9, 14, 13, 13, 11,  9, 13, 15, 10
\]</span></p>
<p><span class="math display">\[
x = 17, 14, 12, 15, 24, 19, 23,  9, 22, 18, 17, 19, 14, 22, 13
\]</span></p>
<p><span class="math display">\[
x = 2, 5, 6, 1, 6, 2, 6, 1, 2, 6, 4, 4, 4, 6, 5
\]</span> Discuss the range and standard deviation comparisons.</p>
</div>
<p>The other feature of the data that we explored in the histogram was
dispersion, or how “spread out” the data were. Graphically we could use
histograms to make comparisons, but as with centrality, these
comparisons are subjective and not rigorously defined. Instead, we need
some numerical measures of dispersion that can be used to make some more
formal statements of comparison, and ultimately inference.</p>
<p>The easiest value to calculate for summarising dispersion is the
<strong>range</strong>, or the difference between the largest and
smallest observations,</p>
<p><strong>Range</strong><br />
Given a set of observations ordered such that <span
class="math display">\[x^{(1)}\leq x^{(2)},\ldots,\leq x^{(n)}\]</span>
the range is: <span class="math display">\[x^{(n)}-x^{(1)}.\]</span></p>
<p><strong>Computing the Range</strong><br />
Given the observations: <span class="math display">\[17, 14, 12, 15, 24,
19, 23,  9, 22, 18, 17, 19, 14, 22, 13\]</span> compute the range.</p>
<p>While the range is relatively straightforward to calculate, one
drawback that we can see is that it only tells us the distance between
the largest and smallest observations, it doesn’t offer any insight into
how the observations between the largest and smallest are dispersed.</p>
<div id="variance" class="section level4">
<h4>Variance</h4>
<p>Another method of measuring dispersion or variability is the
variance, which is defined in terms of the distance between each
observation and either the population mean or the sample mean.</p>
<p><strong>Population Variance</strong><br />
Given a set of observations: <span
class="math display">\[x_1,x_2,\ldots,x_N\]</span> from a population of
size <span class="math inline">\(N\)</span> with mean <span
class="math inline">\(\mu\)</span>, the population variance is <span
class="math display">\[\sigma^2 =
\frac{\sum_{i=1}^N(x_i-\mu)^2}{N}.\]</span></p>
<p><strong>Sample Variance</strong><br />
Given a sample of observations of size <span
class="math inline">\(n\)</span> with sample mean <span
class="math inline">\(\bar{x}\)</span> <span
class="math display">\[x_1,x_2,\ldots,x_n\]</span> the sample variance
is <span class="math display">\[s^2 =
\frac{\sum_{i=1}^n(x_i-\bar{x})^2}{n-1}\]</span></p>
<p>The variance is basically the average of the squared distance between
observations and the mean, and as such, it has the advantage of being
based on all the observations. The drawback in interpreting the variance
is that its units are the square of the units for the observations. For
example, if <span class="math inline">\(x_i\)</span> is a measurement of
time in seconds, then the sample variance <span
class="math inline">\(s^2\)</span> is in the units (seconds)<span
class="math inline">\(^2\)</span>, and conceptually it is difficult to
understand a squared second in terms of the data. Instead, the
<strong>standard deviation</strong> is often used.</p>
<p><strong>Standard Deviation</strong><br />
The standard deviation is the square root of the variance, <span
class="math display">\[\mbox{population standard deviation: }\sigma =
\sqrt{\sigma^2}\qquad\qquad\mbox{sample standard deviation: } s =
\sqrt{s^2}\]</span> hence the units of the standard deviation are the
same as the observations or the mean.</p>
</div>
</div>
<div id="interpreting-standard-deviation-and-range"
class="section level3">
<h3>Interpreting Standard Deviation and Range</h3>
<div class="sidenote">
<p><img src="Pafnuty_Lvovich_Chebyshev.jpg" alt="Chebyshev" /><br />
<a href="https://en.wikipedia.org/wiki/Pafnuty_Chebyshev">Pafnuty
Lvovich Chebyshev</a> is considered the founding father of Russian
mathematics and was the first mathematician to rigorously examine
moments and expectations of random variables. <strong>Chebyshev’s
Theorem</strong>, also known as Chebyshev’s Inequality, is an important
tool for proving the Weak Law of Large Numbers, as well as for
interpreting measures of dispersion.</p>
</div>
<p>Interpreting the standard deviation involves making statements about
what proportion of the data are bounded by an interval defined by the
location of the mean and upper and lower bounds equidistant from the
mean. This proportion depends on some assumptions (in some cases) about
the “shape” of the histogram. Or, more clearly, the standard deviation
is interpreted by considering the proportion of the data contained in an
interval centred at the mean with some upper and lower limits defined in
terms of the standard deviation. We will look at two commonly used rules
for interpreting the standard deviation and a rule using the range to
estimate the standard deviation.</p>
<p><strong>Chebeyshev’s Theorem</strong><br />
For <span class="math inline">\(k\geq 1\)</span> given <span
class="math inline">\(n\)</span> observations at least <span
class="math display">\[1-\frac{1}{k^2}\]</span> of the observations will
lie within <span class="math inline">\(k\)</span> standard deviations of
the mean. Or more formally: For a sample of size <span
class="math inline">\(n\)</span> and <span class="math inline">\(k\geq
1\)</span> <span
class="math display">\[\frac{\#\{x|\bar{x}-ks&lt;x&lt;\bar{x}+ks\}}{n}\geq
1-\frac{1}{k^2}.\]</span></p>
<p>Chebyshev’s theorem is very conservative and is not dependent on the
“shape” or actual dispersion pattern of the data. It merely establishes
a lower bound for the proportion of observations in an interval defined
by the mean and <span class="math inline">\(s\)</span>. Given additional
information, such as the shape of the histogram, more precise statements
are possible.</p>
<p><strong>The Empirical Rule</strong><br />
If the histogram of the data is approximately unimodal and symmetric,
then, in general:</p>
<ul>
<li><p>68% of the observations will fall within one standard deviation
of the mean</p></li>
<li><p>95% of the observations will fall within two standard deviations
of the mean</p></li>
<li><p>99% of the observations will fall within three standard
deviations of the mean.</p></li>
</ul>
<p>It turns out that the conditions of the Empirical Rule are relatively
mild and are adequately met in most situations. While the statements of
the Empirical Rule concern narrower intervals; they are heuristic
approximations rather than the mathematically precise bounds provided by
Chebyshev’s Theorem.</p>
<div id="standard-deviation-and-range" class="section level4">
<h4>Standard Deviation and Range</h4>
<p>Chebyshev’s Theorem and the Empirical Rule are useful tools for
understanding the standard deviation, but often we can not calculate the
standard deviation easily, but the range can. In this case, it is useful
to relate the range of observed values to the standard deviation, in
essence working the Empirical Rule backwards. As a rule of thumb, it is
reasonable to assume that <span
class="math display">\[\mbox{range}\approx 4s\]</span> or that the range
is approximately equal to four standard deviations. This rule is easily
“derived” from the Empirical Rule, and while not always accurate, it
provides a conservative estimate of the standard deviation of <span
class="math display">\[s\approx\frac{\mbox{range}}{4}.\]</span></p>
</div>
<div id="example-3" class="section level4 tabset tabset-pills boxed">
<h4 class="tabset tabset-pills">Example</h4>
<p>For the following data, compute the variance, standard deviation, and
range both by hand and using the built-in functions <code>var</code>,
<code>sd</code> and <code>range</code>.</p>
<p><span class="math display">\[
x = 6.8, 3.4, 4.2, 3.7, 1.5, 6.0, 4.1, 1.8, 5.2
\]</span></p>
<p><span class="math display">\[
x = 8,  9, 14, 13, 13, 11,  9, 13, 15, 10
\]</span></p>
<p><span class="math display">\[
x = 17, 14, 12, 15, 24, 19, 23,  9, 22, 18, 17, 19, 14, 22, 13
\]</span></p>
<p><span class="math display">\[
x = 2, 5, 6, 1, 6, 2, 6, 1, 2, 6, 4, 4, 4, 6, 5
\]</span> Discuss the range and standard deviation comparisons.</p>
<div id="solution-5" class="section level5">
<h5>Solution</h5>
<p>For the first example, the variance is</p>
<p><span class="math display">\[
\begin{align}
s^2&amp;=\frac{1}{n-1}\sum_{i=1}^n\left(x_i-\bar{x}\right)^2\\
&amp;=\frac18\sum_{i=1}^9(x_i-4.08)^2\\
&amp;=2.76
\end{align}
\]</span> and the standard deviation is <span class="math display">\[
\begin{align}
s&amp;=\sqrt{s^2}\\
&amp;=\sqrt{2.76}\\
&amp;=1.66.
\end{align}
\]</span> we can compute the range by sorting the values</p>
<p><span class="math display">\[
x = 1.5, 1.8, 3.4, 3.7, 4.1, 4.2, 5.2, 6, 6.8
\]</span> and finding the first and last values</p>
<p><span class="math display">\[
\begin{align}
\text{range of }x&amp;=6.8-1.5\\
&amp;=5.3.
\end{align}
\]</span> Note that <span class="math display">\[
\begin{align}
\frac{\text{range of }x}{4}&amp;=\frac{5.3}{4}\\
&amp;=1.325
\end{align}
\]</span> which tends to over-estimate <span
class="math inline">\(s=1.325\)</span>.</p>
<p>For the second example, the variance is</p>
<p><span class="math display">\[
\begin{align}
s^2&amp;=\frac{1}{n-1}\sum_{i=1}^n\left(x_i-\bar{x}\right)^2\\
&amp;=\frac{1}{10}\sum_{i=1}^{10}\left(x_i-11.5\right)^2\\
&amp;=5.25
\end{align}
\]</span> and the standard deviation is <span class="math display">\[
\begin{align}
s&amp;=\sqrt{s^2}\\
&amp;=\sqrt{5.25}\\
&amp;=2.29.
\end{align}
\]</span> we can compute the range by sorting the values <span
class="math display">\[
x =
\]</span> and finding the first and last values <span
class="math display">\[
\begin{align}
\text{range of }x&amp;=15-8\\
&amp;=7.
\end{align}
\]</span> Note that <span class="math display">\[
\begin{align}
\frac{\text{range of }x}{4}&amp;=\frac{7}{4}\\
&amp;=1.75
\end{align}
\]</span> which tends to under-estimate <span
class="math inline">\(s=2.29\)</span>.</p>
<p>For the third example, the variance is</p>
<p><span class="math display">\[
\begin{align}
s^2&amp;=\frac{1}{n-1}\sum_{i=1}^n\left(x_i-\bar{x}\right)^2\\
&amp;=\frac{1}{15}\sum_{i=1}^{15}\left(x_i-17.2\right)^2\\
&amp;=18.03
\end{align}
\]</span> and the standard deviation is <span class="math display">\[
\begin{align}
s&amp;=\sqrt{s^2}\\
&amp;=\sqrt{18.03}\\
&amp;=4.25.
\end{align}
\]</span> we can compute the range by sorting the values <span
class="math display">\[
x =
\]</span> and finding the first and last values <span
class="math display">\[
\begin{align}
\text{range of }x&amp;=24-9\\
&amp;=15.
\end{align}
\]</span> Note that <span class="math display">\[
\begin{align}
\frac{\text{range of }x}{4}&amp;=\frac{15}{4}\\
&amp;=3.75
\end{align}
\]</span> which tends to under-estimate <span
class="math inline">\(s=4.25\)</span>.</p>
<p>For the fourth example, the variance is</p>
<p><span class="math display">\[
\begin{align}
s^2&amp;=\frac{1}{n-1}\sum_{i=1}^n\left(x_i-\bar{x}\right)^2\\
&amp;=\frac{1}{15}\sum_{i=1}^{15}\left(x_i-4\right)^2\\
&amp;=3.47
\end{align}
\]</span> and the standard deviation is <span class="math display">\[
\begin{align}
s&amp;=\sqrt{s^2}\\
&amp;=\sqrt{3.47}\\
&amp;=1.86.
\end{align}
\]</span> we can compute the range by sorting the values <span
class="math display">\[
x =
\]</span> and finding the first and last values <span
class="math display">\[
\begin{align}
\text{range of }x&amp;=6-1\\
&amp;=5.
\end{align}
\]</span> Note that <span class="math display">\[
\begin{align}
\frac{\text{range of }x}{4}&amp;=\frac{5}{4}\\
&amp;=1.25
\end{align}
\]</span> which tends to under-estimate <span
class="math inline">\(s=1.86\)</span>.</p>
</div>
<div id="code-4" class="section level5">
<h5>Code</h5>
<pre class="python"><code>
x1 = numpy.array([6.8, 3.4, 4.2, 3.7, 1.5, 6.0, 4.1, 1.8, 5.2])

numpy.var(x1)
#&gt; 2.7572839506172837
numpy.std(x1)
#&gt; 1.6605071365752342
numpy.max(x1)-numpy.min(x1)
#&gt; 5.3
x2 = numpy.array([8,  9, 14, 13, 13, 11,  9, 13, 15, 10])

numpy.var(x2)
#&gt; 5.25
numpy.std(x2)
#&gt; 2.29128784747792
numpy.max(x2)-numpy.min(x2)
#&gt; 7
x3 = numpy.array([17, 14, 12, 15, 24, 19, 23,  9, 22, 18, 17, 19, 14, 22, 13])

numpy.var(x3)
#&gt; 18.026666666666667
numpy.std(x3)
#&gt; 4.245782220824175
numpy.max(x3)-numpy.min(x3)
#&gt; 15
x4 = numpy.array([2, 5, 6, 1, 6, 2, 6, 1, 2, 6, 4, 4, 4, 6, 5])

numpy.var(x4)
#&gt; 3.466666666666667
numpy.std(x4)
#&gt; 1.8618986725025255
numpy.max(x4)-numpy.min(x4)
#&gt; 5</code></pre>
</div>
<div id="videos-1" class="section level5">
<h5>Videos</h5>
<div style="max-width: 590px">
<div
style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe width="590" height="332" src="https://web.microsoftstream.com/embed/video/d31eaa00-64fe-4600-8ad3-dc1d1d7c804f?autoplay=false&amp;showinfo=true" allowfullscreen style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
</iframe>
</div>
</div>
<div style="max-width: 590px">
<div
style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe width="590" height="332" src="https://web.microsoftstream.com/embed/video/33a06879-3b3f-4258-b7e0-1769a66d6632?autoplay=false&amp;showinfo=true" allowfullscreen style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
</iframe>
</div>
</div>
<div style="max-width: 590px">
<div
style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe width="590" height="332" src="https://web.microsoftstream.com/embed/video/4cb59225-081e-4901-bd89-6ffc048b43c7?autoplay=false&amp;showinfo=true" allowfullscreen style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
</iframe>
</div>
</div>
<div style="max-width: 590px">
<div
style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe width="590" height="332" src="https://web.microsoftstream.com/embed/video/4cb59225-081e-4901-bd89-6ffc048b43c7?autoplay=false&amp;showinfo=true" allowfullscreen style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
</iframe>
</div>
</div>
</div>
</div>
</div>
<div id="skew-1" class="section level3">
<h3>Skew</h3>
<div class="sidenote">
<p><img src="Karl_Pearson,_1910.jpg" alt="Pearson" /><br />
Karl Pearson was an English mathematicisn credited with creating
mathematical statistics. He developed many of the concept of describing
probability distributions including moments: mean, variance, skew, and
kurtosis.</p>
<p><br></p>
<p><strong>Example: Skewness</strong><br />
</p>
<p>Given the data: <span class="math inline">\(x =\)</span> 0.15, 0.84,
0.31, 0.27, 0.53, 0.1, 0.73, 0.3, 0.57, 0.77, 1.5, 0.07, 1.01,
0.22`<br />
compute the skew. Is the data right or left skewed?</p>
</div>
<p>The other characteristic that we described with graphical summaries
was <strong>skew</strong>, or the asymmetry of the histogram. Not
surprisingly, we can define a numerical measure of skewness and formally
designate the type of skew based on the mean and median.</p>
<div class="boxed">
<p><strong>Skew</strong><br />
For a finite population of size <span class="math inline">\(N\)</span>
skew is defined as: <span
class="math display">\[\frac{1}{N}\sum_{i=1}^N\left(\frac{(x_i-\mu)}{\sigma}\right)^3\]</span>
For a sample of size <span class="math inline">\(n\)</span> the sample
skew is <span
class="math display">\[\frac{\frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^3}{\left(\sqrt{\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})^2}\right)^3}\]</span>
Note that is the mean and median are equal, the non-parametric skew is
0.</p>
</div>
<p>Note that if the skew is positive, the data are said to be
right-skewed, where the “tail” of the histogram is longer on the right;
if the skew is negative the data are left-skewed where the “tail” of the
histogram is longer on the left.</p>
<div id="example-4" class="section level4 tabset tabset-pills boxed">
<h4 class="tabset tabset-pills">Example</h4>
<p>Given the data: <span class="math inline">\(x =\)</span> 0.15, 0.84,
0.31, 0.27, 0.53, 0.1, 0.73, 0.3, 0.57, 0.77, 1.5, 0.07, 1.01,
0.22`<br />
compute the skew. Is the data right or left skewed?</p>
<div id="solution-6" class="section level5">
<h5>Solution</h5>
<p>The skew is <span class="math display">\[
\begin{align}
\tilde{\mu}&amp;=\frac{\frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^3}{\left(\sqrt{\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})^2}\right)^3}\\
&amp;=0.93.
\end{align}
\]</span></p>
<p>Note that the sample mean is <span
class="math inline">\(\bar{x}=\)</span> 0.5259501 is greater than the
sample median is <span class="math inline">\(\text{median}(x)=\)</span>
0.4172729, verifying the result that for a positive skew the data have a
heavier right tail.</p>
</div>
<div id="code-5" class="section level5">
<h5>Code</h5>
<pre class="python"><code>
num = ((x-numpy.mean(x))**3).mean()
den = numpy.std(x)**3
skew = num/den

skew
#&gt; 0.9306082125963702
numpy.mean(x)
#&gt; 0.5264285714285715
numpy.median(x)

#&gt; 0.42000000000000004</code></pre>
</div>
<div id="plot-5" class="section level5">
<h5>Plot</h5>
<p>If we plot a histogram of the data, we verify that the data are
right-skewed, as shown analytically.</p>
<pre><code>#&gt; &lt;ggplot: (309357632)&gt;</code></pre>
<p><img src="chap1_files/figure-html/unnamed-chunk-27-23.png" width="614" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="measures-of-rank-or-relative-standing" class="section level3">
<h3>Measures of Rank or Relative Standing</h3>
<div class="sidenote">
<p><strong>Example: Computing Quantiles, the Inter-Quartile Range, and
<span class="math inline">\(Z\)</span>-scores</strong><br />
Given the observations:<br />
<span class="math inline">\(x =\)</span> 15.91, 16.63, 6.06, 13.62,
15.11, 16.86, 13.47, 12.47, 16.88, 21.81, 23.87, 11.12, 17.59, 27.26,
24.97, 15.90, 17.82, 17.29, 23.99, 17.91<br />
find the median, the 75th quantile, the 5th quantile the 95th quantile,
and the Inter-Quartile Range (IQR). For each observation compute the
<span class="math inline">\(Z\)</span>-score.</p>
</div>
<p>It is often useful to look at data and observations in terms of their
relative standing or rank. This approach is natural for ordinal data
whose ordering has implicit meaning, but it can also be useful for
non-ordinal data as a means of measuring dispersion.</p>
<p>Given two sets of data, we may want to compare the relative ranking
of observations from each data set, to do this, we need a quantity that
can be calculated from the data that is independent of the mean and
standard deviation (this is called a pivotal quantity).</p>
<p><strong><span class="math inline">\(Z\)</span>-score</strong><br />
The <span class="math inline">\(Z\)</span>-score is <span
class="math display">\[Z=\frac{x-\mu}{\sigma}\qquad\mbox{ or
}\qquad\frac{x-\bar{x}}{s}.\]</span> The <span
class="math inline">\(Z\)</span>-score is a unit-less quantity and can
be used to make comparisons of relative rank between members of a
population.</p>
<p>In addition to <span class="math inline">\(Z\)</span>-scores, the
<strong>quantiles</strong> can also be used to make comparisons of
relative ranking between populations as well as construct intervals
bounding a given proportion of the observations.</p>
<p><strong>Quantiles</strong><br />
For a set of <span class="math inline">\(n\)</span> observations <span
class="math inline">\(x_1,,x_2,\dots,x_n\)</span> <span
class="math inline">\(x_q\)</span> is the <span
class="math inline">\(q\)</span>th quantile if <span
class="math inline">\(q\)</span>% of the observations are less than
<span class="math inline">\(x_q\)</span>. For example, the median is
<span class="math inline">\(x_{50}\)</span> or the fiftieth percentile,
or the value where 50% of the values are less than the median.</p>
<p><strong>The Inter-Quartile Range (<span
class="math inline">\(IQR\)</span>)</strong> The interquartile range
(IQR) is the distance between the 25th and the 75th quantile, or the
range covering the “middle” 50% of the data. The IQR is sometimes used
as a robust measure of dispersion because it isn’t affected by extreme
values, unlike the range or the variance.</p>
<div
id="example-computing-quantiles-the-inter-quartile-range-and-z-scores"
class="section level4 tabset tabset-pills boxed">
<h4 class="tabset tabset-pills">Example: Computing Quantiles, the
Inter-Quartile Range, and <span
class="math inline">\(Z\)</span>-scores</h4>
<p>Given the observations:<br />
<span class="math inline">\(x =\)</span> 15.91, 16.63, 6.06, 13.62,
15.11, 16.86, 13.47, 12.47, 16.88, 21.81, 23.87, 11.12, 17.59, 27.26,
24.97, 15.90, 17.82, 17.29, 23.99, 17.91<br />
find the median, the 75th quantile, the 5th quantile the 95th quantile,
and the Inter-Quartile Range (IQR). For each observation compute the
<span class="math inline">\(Z\)</span>-score.</p>
<div id="solution-7" class="section level5">
<h5>Solution</h5>
<p>To find the median, we first sort the observations <span
class="math inline">\(x\)</span>:<br />
<br />
because there are <span class="math inline">\(n=\)</span> 20
observations the median is then: <span class="math display">\[
\begin{align}
\mbox{median}&amp;=\frac{x^{\left(\frac{n}{2}\right)}+x^{\left(\frac{n}{2}+1\right)}}{2}\\
&amp;=16.87
\end{align}
\]</span> There are multiple ways of computing quantiles when we have
(small) samples, as we do here. The function <code>quantile()</code> in
<code>R</code> has nine different options for computing quantiles (the
default is number 7). We are encouraged to read the documentation and
choose our method accordingly carefully.</p>
<p>Suppose we sort the data as we did for the median. In that case, we
note that if we had 21 observations, we could easily identify the 5th
quantile as the second observation. Still, since there are only 20
observations, we will use a linear interpolation method (the default in
<code>R</code> ) to compute the quantiles.</p>
<p>The 5th quantile is <code>r py$q5</code><br />
The 75th quantile is 18.88
<code>\ The 95th quantile is 25.08</code>.</p>
<p><strong><span class="math inline">\(Z\)</span>-scores</strong></p>
<p>The <span class="math inline">\(Z\)</span>-scores are calculated as
<span class="math display">\[
\hat{Z}=\frac{x-\bar{x}}{s}
\]</span></p>
<p>Note that <span class="math inline">\(\bar{x}=\)</span> 17.33
<code>and $s=$ 4.96, thus the $Z$-scores are:\ $\hat{Z}=$-2.27, -1.25, -0.98, -0.78, -0.75, -0.45, -0.29, -0.29, -0.14, -0.09, -0.09, -0.01, 0.05, 0.1, 0.12, 0.9, 1.32, 1.34, 1.54, 2</code>.</p>
</div>
<div id="code-6" class="section level5">
<h5>Code</h5>
<p>We can use the built-in functions <code>median</code> and
<code>quantile</code> in <code>R</code> to find the median and
quantiles. We can use the other built-in function <code>IQR</code> in
<code>R</code> to compute the interquartile range.</p>
<pre class="python"><code>x = numpy.array([15.91, 16.63,  6.06, 13.62, 15.11, 16.86, 13.47, 12.47, 16.88, 21.81, 23.87, 11.12, 17.59, 27.26,
24.97, 15.90, 17.82, 17.29, 23.99, 17.91])

numpy.median(x)
#&gt; 16.869999999999997
numpy.quantile(x,0.75)
#&gt; 18.884999999999998
numpy.quantile(x,0.05)
#&gt; 10.866999999999999
numpy.quantile(x,0.95)
#&gt; 25.084500000000002
scipy.stats.iqr(x)
#&gt; 4.147499999999999
Z = ((numpy.sort(x)-numpy.mean(x))/numpy.std(x)).round(2)
</code></pre>
</div>
</div>
</div>
<div id="boxplots-outliers-and-the-five-number-summary"
class="section level3">
<h3>Boxplots, Outliers, and the Five-Number Summary</h3>
<div class="sidenote">
<p><strong>Example: Five-Number Summries and Boxplots:</strong><br />
Compute the five number summaries and boxplots for the 1990 EPA City and
Highway Fuel Economy.</p>
</div>
<div id="outliers" class="section level4">
<h4>Outliers</h4>
<p>Outliers are extreme observations defined as falling outside some
interval defined by either the quantiles, e.g. falling above the 95%ile
or below the 5%ile, or in terms of Empirical Rule and standard
deviation, e.g. observations more than two standard deviations from the
mean. Outliers are not necessarily errors but should be investigated to
determine if they are errors or just naturally occurring extreme values.
This determination can be simple if the outliers fall outside the range
of possible values for a variable, but in some cases, they can present a
challenge to interpretation.</p>
<p><strong>Five-Number Summaries</strong><br />
The <strong>five-number summary</strong> is a standard set of numerical
measures that can give a relatively complete picture of the data,
highlight possible outliers, and indicate skew. It consists of the:
minimum or smallest value; the 25th quartile; the median; the 75th
quartile; and the maximum or largest value. These values are often
presented in a simple table with column headings for each value:</p>
<table>
<thead>
<tr class="header">
<th align="center">min.</th>
<th align="center">25th %in</th>
<th align="center">median</th>
<th align="center">75th %ile</th>
<th align="center">max</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">XX</td>
<td align="center">XX</td>
<td align="center">XX</td>
<td align="center">XX</td>
<td align="center">XX</td>
</tr>
</tbody>
</table>
<p><strong>Boxplots</strong> Boxplots are a graphical analogue to the
five-number summary. It is a plot with the values of the data mapped to
the <span class="math inline">\(y\)</span>-axis. There are several
variations in how the plot is constructed, but in <code>ggplot()</code>,
the function <code>geom_boxplot()</code> draws a box encompassing the
inter-quartile range (IQR) with a horizontal line indicating the median.
Vertical lines extend 1.5 times the IQR above and below the box. Any
point not within the box or the vertical lines (sometimes called
“whiskers”) are plotted, indicating potential outliers.</p>
</div>
<div id="example-five-number-summaries-and-boxplots"
class="section level4 tabset tabset-pills boxed">
<h4 class="tabset tabset-pills">Example: Five-Number Summaries and
Boxplots</h4>
<p>Compute the five number summaries and boxplots for the 1990 EPA City
and Highway Fuel Economy.</p>
<div id="solution-8" class="section level5">
<h5>Solution</h5>
<pre class="python"><code>
##  Five-Number Summary for City 

df = numpy.array(epa_data[epa_data.year.isin([1990])][&quot;city&quot;])
numpy.min(df)
#&gt; 6
numpy.quantile(df,0.25)
#&gt; 14.0
numpy.mean(df)
#&gt; 17.03339517625232
numpy.median(df)
#&gt; 16.0
numpy.quantile(df,0.75)
#&gt; 19.0
numpy.max(df)

##  Five-Number Summary for Highway 
#&gt; 43
df = numpy.array(epa_data[epa_data.year.isin([1990])][&quot;hwy&quot;])
numpy.min(df)
#&gt; 10
numpy.quantile(df,0.25)
#&gt; 18.0
numpy.mean(df)
#&gt; 22.337662337662337
numpy.median(df)
#&gt; 22.0
numpy.quantile(df,0.75)
#&gt; 26.0
numpy.max(df)

#&gt; 52</code></pre>
</div>
<div id="plots" class="section level5">
<h5>Plots</h5>
<p>A boxplot constructed of the EPA City data for 1990 illustrates how a
boxplot is constructed and interpreted.</p>
<pre class="python"><code>
df = epa_data[epa_data.year.isin([1990])]
df = df[[&quot;city&quot;,&quot;hwy&quot;]].reset_index()
df = pandas.melt(df,id_vars = [&quot;index&quot;],value_vars= [&quot;city&quot;,&quot;hwy&quot;], var_name = &quot;type&quot;, value_name = &quot;mpg&quot;)

mpg_box = (ggplot(df)+
  geom_boxplot(aes(y = &quot;mpg&quot;,x = &quot;type&quot;))
)  </code></pre>
</div>
</div>
</div>
</div>
</div>

<!--
&nbsp;
&nbsp;
<footer class="copyright">
<p>Copyright &copy;2021 Queensland University of Technology, All rights reserved.</p>
</footer>
-->

<!-- js for accordion button -->
<script>
var acc = document.getElementsByClassName("week");
var i   ;

for (i = 0; i < acc.length; i++) {
  acc[i].addEventListener("click", function() {
    /* Toggle between adding and removing the "active" class,
    to highlight the button that controls the panel */
    this.classList.toggle("active");

    /* Toggle between hiding and showing the active panel */
    var panel = this.nextElementSibling;
    if (panel.style.display === "block") {
      panel.style.display = "none";
    } else {
      panel.style.display = "block";
    }
  });
}
</script>



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
