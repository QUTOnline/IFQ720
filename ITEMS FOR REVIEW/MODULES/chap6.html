<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Module 6</title>

<script src="site_libs/header-attrs-2.14/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<head>
<link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

</head>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="QUTReadings.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">IFQ720</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="contact.html">Contacts</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Readings
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="chap1.html">Module 1</a>
    </li>
    <li>
      <a href="chap2.html">Module 2</a>
    </li>
    <li>
      <a href="chap3.html">Module 3</a>
    </li>
    <li>
      <a href="chap4.html">Module 4</a>
    </li>
    <li>
      <a href="chap5.html">Module 5</a>
    </li>
    <li>
      <a href="chap6.html">Module 6</a>
    </li>
    <li>
      <a href="chap7.html">Module 7</a>
    </li>
    <li>
      <a href="chap8.html">Module 8</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Workshops
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-01/">Workshop 1</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-02/">Workshop 2</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-03/">Workshop 3</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-04/">Workshop 4</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-05/">Workshop 5</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-06/">Workshop 6</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-07/">Workshop 7</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-08/">Workshop 8</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-09/">Workshop 9</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-10/">Workshop 10</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-11/">Workshop 11</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-12/">Workshop 12</a>
    </li>
  </ul>
</li>
<li>
  <a href="assessment.html">Assessments</a>
</li>
<li>
  <a href="videos.html">Videos</a>
</li>
<li>
  <a href="https://blackboard.qut.edu.au/webapps/blackboard/execute/announcement?method=search&amp;context=course&amp;course_id=_164898_1&amp;handle=cp_announcements&amp;mode=reset">IFQ720 Canvas</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="mailto:gentry.white@qut.edu.au">
    <span class="fa fa-envelope fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://mxb1072022.slack.com">
    <span class="fab fa-slack fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://https://qutvirtual4.qut.edu.au/web/qut/hiq">
    <span class="fa fa-users"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<div><a href="https://www.qut.edu.au">
     <img alt="QUT" src="logo.png" width=50px" align="left" style="vertical-align:middle;margin:10px 10px 10px 0px"></a></div>

<div id="header">



<h1 class="title toc-ignore">Module 6</h1>

</div>

<div id="TOC">
<ul>
<li><a href="#linear-regression-and-ancova"
id="toc-linear-regression-and-ancova">Linear Regression and ANCOVA</a>
<ul>
<li><a href="#simple-linear-regression"
id="toc-simple-linear-regression">Simple Linear Regression</a></li>
<li><a href="#testing-the-linear-model-utility"
id="toc-testing-the-linear-model-utility">Testing the Linear Model
Utility</a></li>
<li><a href="#estimation-and-prediction"
id="toc-estimation-and-prediction">Estimation and Prediction</a></li>
<li><a href="#linear-regression-and-analysis-of-covariance-ancova"
id="toc-linear-regression-and-analysis-of-covariance-ancova">Linear
Regression and Analysis of Covariance (ANCOVA)</a></li>
<li><a href="#videos" id="toc-videos">Videos</a></li>
</ul></li>
</ul>
</div>

<div id="linear-regression-and-ancova" class="section level1">
<h1>Linear Regression and ANCOVA</h1>
<p>We touched on the concept of linear regression when we looked at the
graphical methods for summarising bivariate data with a scatter plot and
a least-squares trend line. In this section, we are going to generalise
this idea and demonstrate the technique of linear regression, both as an
extension of least-squares and as a complement to ANOVA.</p>
<div id="simple-linear-regression" class="section level2">
<h2>Simple Linear Regression</h2>
<div class="sidenote">
<strong>Example:</strong><br />
Consider the relationship between city and highway fuel economy (in
miles per gallon or MPG) from the EPA data.
<p style="font-size:5px">
 
</p>
<p>Fit a linear model to demonstrate the relationship between city and
highway fuel economy.</p>
</div>
<p>Simple linear regression describes the relationship between the
dependent random variable <span class="math inline">\(y\)</span> and
independent variable <span class="math inline">\(x\)</span> is linear if
<span class="math display">\[y_i =
\beta_0+\beta_1x_i+\epsilon_i\]</span> where we assume that the “error”
terms <span class="math inline">\(\epsilon_i\)</span> are independently
and identically distributed with a Gaussian distribution with <span
class="math inline">\(E(\epsilon_i)=0\)</span> and <span
class="math inline">\(\mbox{Var}(\epsilon_i)=\sigma^2\)</span>,
i.e. <span class="math display">\[\epsilon_i\stackrel{iid}{\sim}
N(0,\sigma^2), \forall i.\]</span> We can state this in slightly
different terms as <span class="math display">\[y_i\sim
N(\beta_0+\beta_1x_i,\sigma^2)\]</span> where <span
class="math inline">\(y_i\)</span> is a Gaussian random variable for
fixed values of <span class="math inline">\(x_i\)</span> and the
parameters <span class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>. The variable <span
class="math inline">\(y\)</span> is sometimes referred to as the
response, and the variable <span class="math inline">\(x\)</span> is
sometimes referred to as the predictor. The nomenclature can vary
depending on circumstances, but it is always assumed that <span
class="math inline">\(x&#39;s\)</span> are fixed known quantities, not
random variables.</p>
<div id="estimation-and-inference" class="section level3 unnumbered">
<h3 class="unnumbered">Estimation and Inference</h3>
<p>Given a suspected linear relationship between two variables <span
class="math inline">\(y\)</span> and <span
class="math inline">\(x\)</span> we would like to answer several
questions requiring statistical inference, as such, it is necessary to
derive both estimators and a sampling distribution for those
estimators.</p>
<div class="sidenote">
<p>Note that the maximum likelihood estimates for <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> are the <strong>Least Squares
Solution</strong>, because maximising the log-likelihood <span
class="math inline">\(l(\beta_0,\beta_1,\sigma^2)\)</span> is equivalent
to finding the minimum <span class="math display">\[
\min_{\beta_0,\beta_1}\sum_{i=1}^n(y_i-\beta_0-\beta_1x_i)^2.
\]</span> We can obtain this result by working through both approaches
to attain identical results. As a result, linear regression is also
referred to as <strong>Least Squares Regression</strong>.</p>
</div>
<p>We can employ the method of maximum likelihood to derive a likelihood
and log-likelihood for the two parameters <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> and <span
class="math inline">\(\sigma^2\)</span>. <span class="math display">\[
\begin{aligned}
L(\beta_0,\beta_1,\sigma^2) &amp;=
\prod_{i=1}^n\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y_i-\beta_0-\beta_1x_i)^2}{2\sigma^2}\right)\\
&amp; =
\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^{n}\exp\left(-\frac{\sum_{i=1}^n(y_i-\beta_0-\beta_1x_i)^2}{2\sigma^2}\right)
\end{aligned}
\]</span> and <span class="math display">\[
\begin{aligned}
l(\beta_0,\beta_1,\sigma^2)&amp;=-\frac{n}{2}\log\left(2\pi\sigma^2\right)-\frac{\sum_{i=1}^n(y_i-\beta_0-\beta_1x_i)^2}{2\sigma^2}
\end{aligned}
\]</span> we can find the maximum likelihood estimates for these
parameters by taking the derivative of the log-likelihood with respect
to each parameter and setting them equal to <span
class="math inline">\(0\)</span>.</p>
<p>We can solve the resulting system of equations to give: <span
class="math display">\[
\begin{aligned}
\hat{\beta}_0 &amp; = \bar{y}-\hat{\beta}_1\bar{x}\\
\hat{\beta}_1 &amp; =
\frac{\sum_{i=1}^n(x_1-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}\\
s^2 &amp; =
\frac{\sum_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_i)^2}{n-2}
\end{aligned}
\]</span> It is important to note that these estimators are, in fact,
functions of <span class="math inline">\(x\)</span>, a known quantity
and <span class="math inline">\(y\)</span> a random variable, and are,
in fact, linear combinations of these. Given that we know the
probability distribution of <span class="math inline">\(y\)</span> is
Gaussian, then it follows that because the estimators are linear
functions of <span class="math inline">\(y\)</span>, then the estimators
must also follow a Gaussian distribution.</p>
</div>
<div id="example-city-and-highway-fuel-economy"
class="section level3 tabset tabset-pills boxed">
<h3 class="tabset tabset-pills">Example: City and Highway Fuel
Economy</h3>
<p>Consider the relationship between city and highway fuel economy (in
miles per gallon or MPG) from the EPA data.</p>
<div id="solution" class="section level4">
<h4>Solution</h4>
<p>The relationship between highway fuel economy (<span
class="math inline">\(x\)</span>) and city fuel economy (<span
class="math inline">\(y\)</span>) is <span class="math display">\[
y_i = \beta_0 + \beta_1x_i.
\]</span> Using the <code>epa_data</code>, we note the following
statistics <span class="math display">\[\bar{y} = 17.27, \quad\bar{x} =
23.55\\\sum(x_i-\bar{x})(y_i-\bar{y}) = 2.4657746\times
10^{4}\\\sum(x_i-\bar{x})^2 = 3.1474623\times 10^{4}\]</span> we can
compute <span class="math display">\[
\begin{aligned}
\hat{\beta}_1 = &amp; \frac{2.4657746\times 10^{4}}{3.1474623\times
10^{4}}=0.78\\
\hat{\beta}_0  = &amp; 17.27-(0.78)(23.55) =
-1.18\\
s^2 =&amp;\frac{\sum(y_i-\hat{\beta}_0-\hat{\beta}_1x_i)^2}{n-2} =
\frac{3213.54}{850} = 3.78
\end{aligned}
\]</span></p>
</div>
<div id="code" class="section level4">
<h4>Code</h4>
<pre class="python"><code>df = pandas.read_csv(&quot;epa_data.csv&quot;)

df = df[df[&quot;year&quot;]==1999]

city = df[&quot;city&quot;]
hwy = df[&quot;hwy&quot;]



Xbar = hwy.mean()
Ybar = city.mean()

SXX = numpy.sum((hwy-Xbar)**2)
SXY = numpy.sum((hwy-Xbar)*(city-Ybar))

beta_1 = SXY/SXX
beta_0 = Ybar-Xbar*beta_1

SE = ((city-beta_0-beta_1*hwy)**2)
S2 = numpy.sum(SE)/(hwy.size-2)

beta_0.round(3)
#&gt; -1.179
beta_1.round(3)
#&gt; 0.783
S2.round(3)
#&gt; 3.781
numpy.sqrt(S2).round(3)
#&gt; 1.944</code></pre>
<p>We can also solve this using the <code>statsmodels</code> module in
Python</p>
<pre class="python"><code>
import statsmodels

df = pandas.read_csv(&quot;epa_data.csv&quot;)

df = df[df[&quot;year&quot;]==1999]

model = statsmodels.formula.api.ols(&#39;city~hwy&#39;,df)

result = model.fit()

result.summary()</code></pre>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>city</td>       <th>  R-squared:         </th> <td>   0.857</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.857</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   5110.</td>
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 21 Nov 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> 
</tr>
<tr>
  <th>Time:</th>                 <td>15:12:10</td>     <th>  Log-Likelihood:    </th> <td> -1774.5</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   852</td>      <th>  AIC:               </th> <td>   3553.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   850</td>      <th>  BIC:               </th> <td>   3562.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   -1.1794</td> <td>    0.267</td> <td>   -4.424</td> <td> 0.000</td> <td>   -1.703</td> <td>   -0.656</td>
</tr>
<tr>
  <th>hwy</th>       <td>    0.7834</td> <td>    0.011</td> <td>   71.481</td> <td> 0.000</td> <td>    0.762</td> <td>    0.805</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>685.493</td> <th>  Durbin-Watson:     </th> <td>   0.460</td> 
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>28181.015</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 3.291</td>  <th>  Prob(JB):          </th> <td>    0.00</td> 
</tr>
<tr>
  <th>Kurtosis:</th>      <td>30.395</td>  <th>  Cond. No.          </th> <td>    97.5</td> 
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</div>
<div id="plot" class="section level4">
<h4>Plot</h4>
<pre><code>#&gt; &lt;ggplot: (305799784)&gt;</code></pre>
<p><img src="chap6_files/figure-html/unnamed-chunk-6-1.png" width="614" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="assumptions" class="section level3 unnumbered">
<h3 class="unnumbered">Assumptions</h3>
<div class="sidenote">
<p>We can summarise these assumptions stating that <span
class="math display">\[
e_i\stackrel{iid}{\sim}N(0,\sigma^2)
\]</span> or that the residuals are <strong>independently</strong> and
<strong>identically</strong> distributed following a Gaussian
distribution with mean <span class="math inline">\(0\)</span> and
variance <span class="math inline">\(\sigma^2\)</span>.</p>
</div>
<p>As stated above there are explicit assumptions concerning the
probability distribution of the residuals or errors that support the
derivation of the sampling distributions for the estimates of the model
parameters:</p>
<ol style="list-style-type: decimal">
<li><p>The parameter estimates <span
class="math inline">\(\hat{\beta}_0\)</span> and <span
class="math inline">\(\hat{\beta}_1\)</span> are unbiased, i.e. the
residuals <span class="math inline">\(e_i =
y_i-\hat{\beta}_0-\hat{\beta}_1x_i\)</span> have an expected value of
<span class="math inline">\(0\)</span>, hence <span
class="math inline">\(\sum_{i=1}^ne_i = 0\)</span>.</p></li>
<li><p>The residuals <span class="math inline">\(e_i\)</span> are
independent, i.e. <span class="math inline">\(\mbox{Cor}(e_i,e_j) = 0,
\forall i\neq j\)</span></p></li>
<li><p>The residuals <span class="math inline">\(e_i\sim N(0,\sigma^2),
\forall i\)</span>, that is the residuals follow a Gaussian distribution
with <span class="math inline">\(E(e_i)=0\)</span> and <span
class="math inline">\(\mbox{Var}(e_i)=\sigma^2\)</span>.</p></li>
</ol>
</div>
</div>
<div id="testing-the-linear-model-utility" class="section level2">
<h2>Testing the Linear Model Utility</h2>
<div class="sidenote">
<p><strong>Example:</strong><br />
For the linear model <span class="math display">\[
city=\beta_0+\beta_1hwy
\]</span> test the hypotheses <span class="math display">\[
H_0:\beta_0=0\quad\mbox{vs}\quad \beta_0\neq 0
\]</span> and <span class="math display">\[
H_0:\beta_1=0\quad\mbox{vs}\quad \beta_1\neq 0.
\]</span></p>
</div>
<p>The parameters estimates <span
class="math inline">\(\hat{\beta}_0\)</span> and <span
class="math inline">\(\hat{\beta}_1\)</span> are functions of data,
which are realisations of random variables. As a result, the estimators
are also random variables and have probability distributions associated
with them. We can use these sampling distributions to perform the two
most common tasks of inference: computing confidence intervals and
performing hypothesis testing.</p>
<div class="boxed">
<p><strong>Sampling distributions for <span
class="math inline">\(\hat{\beta}_0\)</span> and <span
class="math inline">\(\hat{\beta}_1\)</span></strong> <br> Given a known
value for <span class="math inline">\(\sigma^2\)</span>, we can show
that the sampling distributions for <span
class="math inline">\(\hat{\beta}_0\)</span> and <span
class="math inline">\(\hat{\beta}_1\)</span> are <span
class="math display">\[
\begin{aligned}
\hat{\beta}_0&amp;\sim
N\left(\beta_0,\frac{\sigma^2\sum_{i=1}^nx_i^2}{\sum_{i=1}^n(x_i-\bar{x})^2}\right)\\
\hat{\beta}_1&amp;\sim
N\left(\beta_1,\frac{\sigma^2}{\sum_{i=1}^2(x_i-\bar{x})^2}\right)
\end{aligned}
\]</span></p>
<p>if we substitute the estimator <span
class="math inline">\(s^2\)</span>, then we can see from prior examples
that <span class="math display">\[
\begin{aligned}
\frac{\hat{\beta}_0-\beta_0}{s_{\beta_0}}&amp;\sim t_{n-2}\\
\frac{\hat{\beta}_1-\beta_1}{s_{\beta_1}}&amp;\sim t_{n-2}
\end{aligned}
\]</span> noting that <span class="math display">\[
\begin{aligned}
s^2_{\beta_0} &amp; =
\frac{s^2\sum_{i=1}^nx_i^2}{\sum_{i=1}^n(x_i-\bar{x})^2}\\
s^2_{\beta_1} &amp; = \frac{s^2}{\sum_{i=1}^2(x_i-\bar{x})^2}.
\end{aligned}
\]</span> based on these, we can construct confidence intervals and
conduct hypothesis tests from our estimates <span
class="math inline">\(\hat{\beta}_0\)</span> and <span
class="math inline">\(\hat{\beta}_1\)</span>. <span
class="math inline">\(1-\alpha\)</span> Confidence intervals have the
form: <span class="math display">\[
\begin{aligned}
\hat{\beta}_0&amp;\pm t_{n-2,\alpha/2}s_{\beta_0}\\
\hat{\beta}_1&amp; \pm t_{n-2,\alpha/2}s_{\beta_1}
\end{aligned}
\]</span> where <span class="math display">\[Pr(t&lt;t_{n-2,\alpha/2}) =
1-\alpha/2\]</span> as previously defined.</p>
</div>
<div class="boxed">
<p><strong>Inference for the Slope and Intercept</strong> <br></p>
<p>For the Hypotheses <span class="math display">\[
H_0: \beta_0 = 0\qquad\mbox{and}\qquad H_A:\beta_0\neq 0
\]</span> the test statistic is <span class="math display">\[
t = \frac{\hat{\beta}_0}{s_{\beta_0}}
\]</span> and likewise for the hypotheses <span class="math display">\[
H_0: \beta_1 = 0\qquad\mbox{and}\qquad H_A:\beta_1\neq 0\]</span> the
test statistic is <span class="math display">\[t =
\frac{\hat{\beta}_1}{s_{\beta_1}}.
\]</span> in both cases the rejection region is based on the <span
class="math inline">\(t_{n-2}\)</span> where we reject <span
class="math inline">\(H_0\)</span> if <span
class="math inline">\(|t|&gt;t_{n-2,\alpha/2}\)</span>. Most statistical
software reports both a <span class="math inline">\(t\)</span> test
statistic and a <span class="math inline">\(p\)</span>-value; both can
be used to evaluate hypotheses.</p>
</div>
<div id="example-inference-on-the-slope-and-intercept"
class="section level3 tabset tabset-pills boxed">
<h3 class="tabset tabset-pills">Example: Inference on the Slope and
Intercept</h3>
<p>For the linear model <span class="math display">\[
city=\beta_0+\beta_1hwy
\]</span> test the hypotheses <span class="math display">\[
H_0:\beta_0=0\quad\mbox{vs}\quad \beta_0\neq 0
\]</span> and <span class="math display">\[
H_0:\beta_1=0\quad\mbox{vs}\quad \beta_1\neq 0.
\]</span></p>
<div id="solution-1" class="section level4">
<h4>Solution</h4>
<p>Note that for <span class="math inline">\(\hat{\beta}_0\)</span>
<span class="math display">\[t =  \frac{\hat{\beta}_0}{s_{\beta_0}}=
\frac{0.312}{0.197} = 1.58\]</span> and for <span
class="math inline">\(\hat{\beta}_1\)</span> <span
class="math display">\[
t =  \frac{\hat{\beta}_1}{s_{\beta_1}}= \frac{0.749}{0.009} \approx 87.7
\]</span> Note there is a rounding error in computing the test statistic
for <span class="math inline">\(\beta_1\)</span>.</p>
<p>The <span class="math inline">\(p\)</span>-values are given in the
table and can be used to perform the hypothesis tests, or we can note
that for <span class="math inline">\(n=10\)</span> and <span
class="math inline">\(\alpha = 0.05\)</span>, <span
class="math inline">\(t_{n-2,\alpha/2} = 2.31\)</span> so we would
reject the null hypotheses that <span
class="math inline">\(\beta_1=0\)</span>. But we would <em>fail to
reject</em> the hypothesis that <span
class="math inline">\(\beta_0=0\)</span>.</p>
<p>We can also compute confidence intervals for the test statistics</p>
<p><span class="math display">\[
\beta_0\pm t_{\nu,1-\alpha/2}SE_{\beta_0}\\
\beta_1\pm t_{\nu,1-\alpha/2}SE_{\beta_1}
\]</span> results in the <span class="math inline">\(95\%\)</span>
confidence intervals <span class="math display">\[
\begin{align}
\beta_0\pm t_{\nu,1-\alpha/2}SE_{\beta_0}&amp;=\\
&amp;=0.312\pm 2.31\times 0.197\\
&amp;=0.312\pm 0.455\\
&amp;=(-0.143,0.767)
\end{align}
\]</span> and <span class="math display">\[
\begin{align}
\beta_1\pm t_{\nu,1-\alpha/2}SE_{\beta_0}&amp;=\\
&amp;=0.749\pm 2.31\times 0.009\\
&amp;=0.749\pm 0.021\\
&amp;=(0.728,0.770).
\end{align}
\]</span> Notice that the confidence interval for <span
class="math inline">\(\beta_0\)</span> contained <span
class="math inline">\(0\)</span>, hence we would fail to reject the null
hypothesis <span class="math inline">\(\beta_0=0\)</span> for a test
with a Type I error rate of <span
class="math inline">\(\alpha=0.05\)</span>. The confidence interval for
<span class="math inline">\(\beta_1\)</span> <em>did not</em> contain
<span class="math inline">\(0\)</span>, hence we would <em>reject</em>
the null hypothesis <span class="math inline">\(\beta_1=0\)</span> for a
test with a Type I error rate of <span class="math inline">\(\alpha =
0.05\)</span>.</p>
</div>
<div id="code-1" class="section level4">
<h4>Code</h4>
<p>In terms of testing the hypotheses, the easiest option is to use the
<code>lm()</code> function</p>
<pre class="python"><code>
result.summary()</code></pre>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>city</td>       <th>  R-squared:         </th> <td>   0.857</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.857</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   5110.</td>
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 21 Nov 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> 
</tr>
<tr>
  <th>Time:</th>                 <td>15:12:15</td>     <th>  Log-Likelihood:    </th> <td> -1774.5</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   852</td>      <th>  AIC:               </th> <td>   3553.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   850</td>      <th>  BIC:               </th> <td>   3562.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   -1.1794</td> <td>    0.267</td> <td>   -4.424</td> <td> 0.000</td> <td>   -1.703</td> <td>   -0.656</td>
</tr>
<tr>
  <th>hwy</th>       <td>    0.7834</td> <td>    0.011</td> <td>   71.481</td> <td> 0.000</td> <td>    0.762</td> <td>    0.805</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>685.493</td> <th>  Durbin-Watson:     </th> <td>   0.460</td> 
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>28181.015</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 3.291</td>  <th>  Prob(JB):          </th> <td>    0.00</td> 
</tr>
<tr>
  <th>Kurtosis:</th>      <td>30.395</td>  <th>  Cond. No.          </th> <td>    97.5</td> 
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
<p>The model results display both the test statistic
<code>t value</code> and the <span
class="math inline">\(p\)</span>-value <code>Pr(&gt;|t|)</code>.</p>
<p>We can also extract confidence intervals for the parameters
:::{.table-narrow}</p>
<pre class="python"><code>
df = pandas.read_csv(&quot;epa_data.csv&quot;)

df = df[df[&quot;year&quot;]==1990]

model = statsmodels.formula.api.ols(&#39;city~hwy&#39;,df)

result = model.fit()

result.conf_int()
</code></pre>
</div>
</div>
<div id="video" class="section level4">
<h4>Video</h4>
<p>:::</p>
<p>We will see that in most cases, the testing related <span
class="math inline">\(\beta_0\)</span> are often not of interest, and we
will see that test about the <span
class="math inline">\(\beta_1\)</span> are of most interest and provide
a connection between linear regression and ANOVA.</p>
</div>
<div id="analysis-of-variance-for-linear-regression"
class="section level3 unnumbered">
<h3 class="unnumbered">Analysis of Variance for Linear Regression</h3>
<div class="sidenote">
<p><strong>Example:</strong><br />
Examine the results from the previous example in an ANOVA table and
comment on the results. Notice that the <span
class="math inline">\(F\)</span> statistic for <span
class="math inline">\(x\)</span> (<span
class="math inline">\(\beta_1\)</span>) is equal to <span
class="math inline">\(t^2\)</span> from from the previous example and
that their <span class="math inline">\(p\)</span>-values are identical.
In the case of simple linear regression (one independent variable) the
<span class="math inline">\(F\)</span>-test for model utility is
equivalent to the <span class="math inline">\(t\)</span>-test for <span
class="math inline">\(H_0:\beta_1 = 0\)</span>.</p>
</div>
<p>Aside from inferential questions concerning the values of the <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>, the next most useful question to
ask about our linear model is how well it fits the data. Suppose we
extend the idea from ANOVA of explaining variation. In that case, it is
natural to frame our goodness of fit question regarding what proportion
of the variation in <span class="math inline">\(y\)</span> is explained
by the model <span class="math inline">\(y = \beta_0+\beta_1x\)</span>.
This term is defined in terms of ratios of sums of squares as in ANOVA.
Hence we extend some of the ideas from ANOVA to linear regression and
use the definitions</p>
<p><span class="math display">\[
\mbox{SST} = \sum_{i=1}^n(y_i-\bar{y})^2
\]</span> as in ANOVA is the total sum of squares, which can be
partitioned (as in ANOVA) into SSR the sum of the squares of the
regression, and SSE the sum of the squares of the error.</p>
<p><span class="math display">\[
\begin{aligned}
\mbox{SSR} &amp; = \sum_{i=1}^n(\hat{y}_i-\bar{y})^2\\
&amp; = \sum_{i=1}^n(\hat{\beta}_0+\hat{\beta}_1x_i-\bar{y})^2\\
&amp; =
\frac{\left(\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})\right)^2}{\sum_{i=1}^n(x_i-\bar{x})^2}
\end{aligned}
\]</span> Because SST = SSE+SSR we can define SSE = SST-SSR, which can
be shown to be <span class="math display">\[
\begin{aligned}
\mbox{SSE} =&amp;
\sum_{i=1}^n(y_i-\bar{y})^2-\frac{\left(\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})\right)^2}{\sum_{i=1}^n(x_i-\bar{x})^2}.
\end{aligned}
\]</span></p>
<p>The mean squares terms are the sum of squares divided by the degrees
of freedom as shown:</p>
<div class="table">
<table>
<thead>
<tr class="header">
<th align="left">Source</th>
<th align="center">Degrees of Freedom</th>
<th align="center">Sum of Squares</th>
<th align="center">Mean Squares</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Regression</td>
<td align="center"><span class="math inline">\(1\)</span></td>
<td align="center"><span
class="math inline">\(\frac{\left(\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})\right)^2}{\sum_{i=1}^n(x_i-\bar{x})^2}\)</span></td>
<td align="center"><span
class="math inline">\(\frac{\mbox{SSR}}{1}\)</span></td>
</tr>
<tr class="even">
<td align="left">Error</td>
<td align="center"><span class="math inline">\(n-2\)</span></td>
<td align="center"><span
class="math inline">\(\sum_{i=1}^n(y_i-\bar{y})^2-
\frac{\left(\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})\right)^2}{\sum_{i=1}^n(x_i-\bar{x})^2}\)</span></td>
<td align="center"><span
class="math inline">\(\frac{\mbox{SSE}}{n-2}\)</span></td>
</tr>
<tr class="odd">
<td align="left">Total</td>
<td align="center"><span class="math inline">\(n-1\)</span></td>
<td align="center"><span
class="math inline">\(\sum_{i=1}^n(y_i-\bar{y})^2\)</span></td>
<td align="center"></td>
</tr>
</tbody>
</table>
</div>
<p>Note that <span class="math inline">\(s^2\)</span> is the
MSE=SSE/n-2, and is our best estimate of <span
class="math inline">\(\sigma^2\)</span>.</p>
</div>
<div id="the-coefficient-of-determination-r2"
class="section level3 unnumbered">
<h3 class="unnumbered">The Coefficient of Determination <span
class="math inline">\(R^2\)</span></h3>
<p>We can assess the regression model by computing the coefficient of
determination, <span class="math inline">\(R^2\)</span>, which is <span
class="math display">\[
R^2 = 1 - \frac{\mbox{SSE}}{\mbox{SST}} = \frac{\mbox{SSR}}{\mbox{SST}}
\]</span> which is the proportion of the total variation explained by
the regression model. We don’t typically assess any measures of
uncertainty about the coefficient of determination but instead use it as
a subjective measure. An <span class="math inline">\(R^2\)</span> of
<span class="math inline">\(1\)</span> means that all the variation in
<span class="math inline">\(y\)</span> is explained by the regression
model, which is good because we can feel confident that our knowledge of
<span class="math inline">\(x\)</span> is a perfect explanation of <span
class="math inline">\(y\)</span>. In practice, values of <span
class="math inline">\(R^2\)</span> close to <span
class="math inline">\(1\)</span> (over <span
class="math inline">\(0.95\)</span>) typically indicate that
“over-fitting” in the model which can lead to erroneous results using
the model. While we might think that small values of <span
class="math inline">\(R^2\)</span> are “bad”, in some fields, an <span
class="math inline">\(R^2\)</span> of <span
class="math inline">\(0.2\)</span> could be quite good. Interpreting the
coefficient of determination is not as straightforward as other
statistical measures, and should be used as a guide, rather than as
conclusive evidence in making decisions about the validity or utility of
a model.</p>
</div>
<div id="example" class="section level3 tabset tabset-pills boxed">
<h3 class="tabset tabset-pills">Example</h3>
<p>Find the <span class="math inline">\(R^2\)</span> value for the
regression model <span class="math display">\[
city=\beta_0+\beta_1hwy
\]</span></p>
<div id="solution-2" class="section level4">
<h4>Solution</h4>
<p>We compute the coefficient of determination as</p>
<p><span class="math display">\[
\begin{aligned}
R^2 = &amp; \frac{\mbox{SSR}}{\mbox{SST}}\\
= &amp; \frac{\mbox{SSR}}{\mbox{SSR+SSE}}\\
= &amp; \frac{1.9317291\times 10^{4}}{2.2530826\times 10^{4}}\\
= &amp; 0.8573716
\end{aligned}
\]</span> The interpretation of this is to say that 85.74<span
class="math inline">\(\%\)</span> of the observed variation in <span
class="math inline">\(y\)</span>, city fuel economy is explained by
<span class="math inline">\(x\)</span> highway fuel economy.</p>
</div>
<div id="code-2" class="section level4">
<h4>Code</h4>
<p>The values for <span class="math inline">\(R^2\)</span> and the
adjusted <span class="math inline">\(R^2\)</span> are automatically
produced by the <code>statsmodels</code> module for Python.</p>
<pre class="python"><code>
result.summary()</code></pre>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>city</td>       <th>  R-squared:         </th> <td>   0.857</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.857</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   5110.</td>
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 21 Nov 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> 
</tr>
<tr>
  <th>Time:</th>                 <td>15:12:18</td>     <th>  Log-Likelihood:    </th> <td> -1774.5</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   852</td>      <th>  AIC:               </th> <td>   3553.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   850</td>      <th>  BIC:               </th> <td>   3562.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   -1.1794</td> <td>    0.267</td> <td>   -4.424</td> <td> 0.000</td> <td>   -1.703</td> <td>   -0.656</td>
</tr>
<tr>
  <th>hwy</th>       <td>    0.7834</td> <td>    0.011</td> <td>   71.481</td> <td> 0.000</td> <td>    0.762</td> <td>    0.805</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>685.493</td> <th>  Durbin-Watson:     </th> <td>   0.460</td> 
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>28181.015</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 3.291</td>  <th>  Prob(JB):          </th> <td>    0.00</td> 
</tr>
<tr>
  <th>Kurtosis:</th>      <td>30.395</td>  <th>  Cond. No.          </th> <td>    97.5</td> 
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
<p><br></p>
<p>The adjusted <span class="math inline">\(R^2\)</span> takes into
account the number of parameters in the model (if there is more than one
independent variable in the model, there will be a difference between
the <span class="math inline">\(R^2\)</span> and the adjusted <span
class="math inline">\(R^2\)</span>) and thus can be used to compare
models with different parameters included or omitted.</p>
</div>
</div>
<div id="f-tests-for-model-utility" class="section level3 unnumbered">
<h3 class="unnumbered"><span class="math inline">\(F\)</span>-tests for
Model Utility</h3>
<p>In a more formal sense, we can measure model utility by noting that
the <span class="math inline">\(F\)</span> statistic we use in ANOVA is
equivalent in linear regression to <span class="math display">\[F =
\frac{\mbox{SSR}}{\mbox{SSE}}\]</span> which follows an <span
class="math inline">\(F\)</span> distribution with <span
class="math inline">\(1\)</span> degree of freedom in the numerator and
<span class="math inline">\(n-2\)</span> degrees of freedom in the
denominator. We use this result to perform a hypothesis test of the
general model utility where the specific null hypothesis for the <span
class="math inline">\(F\)</span> testis <span
class="math display">\[H_0:\mbox{The regression model explains more
variation in $y$ than the sample mean $\bar{y}$}\]</span> versus the
alternative hypothesis <span class="math display">\[
H_A:\mbox{The regression model does not explain more variation in $y$
than the sample mean $\bar{y}$. }
\]</span> where we reject the null hypothesis if <span
class="math inline">\(F&gt;F_{1,n-2,\alpha}\)</span>, where the Type I
Error Rate is <span class="math inline">\(\alpha\)</span>.</p>
<p>In the case where there is one independent variable <span
class="math inline">\(x\)</span>, the <span
class="math inline">\(F\)</span>-test for model utility is equivalent to
testing the hypotheses: <span class="math display">\[H_0: \beta_1 =
0\qquad\mbox{and}\qquad H_A:\beta_1\neq 0\]</span> in fact the <span
class="math inline">\(F\)</span> statistic is numerically equivalent to
the square of the same <span class="math inline">\(t\)</span> statistic,
i.e. <span class="math inline">\(F \equiv t^2\)</span>.</p>
<p>While the relationship between the <span
class="math inline">\(F\)</span> statistic and the <span
class="math inline">\(t\)</span> statistics may seem trivial, in further
studies, you will see models with more than one independent variable,
and in that case, the interpretation of the <span
class="math inline">\(F\)</span> statistic differs from the <span
class="math inline">\(t\)</span> statistic, and that the definition of
the coefficient of determination is slightly modified to account for the
additional variables.</p>
</div>
<div id="example-1" class="section level3 tabset tabset-pills boxed">
<h3 class="tabset tabset-pills">Example</h3>
<p>Examine the results from the previous example in an ANOVA table and
comment on the results.</p>
<div id="solution-3" class="section level4">
<h4>Solution</h4>
<pre><code>#&gt;               df        sum_sq       mean_sq            F  PR(&gt;F)
#&gt; hwy          1.0  19342.095676  19342.095676  7689.096988     0.0
#&gt; Residual  1076.0   2706.702098      2.515522          NaN     NaN</code></pre>
<p>Notice that the <span class="math inline">\(F\)</span> statistic for
<span class="math inline">\(x\)</span> (<span
class="math inline">\(\beta_1\)</span>) is equal to <span
class="math inline">\(t^2\)</span> from from the previous results. Their
<span class="math inline">\(p\)</span>-values are also identical. In the
case of simple linear regression (one independent variable) the <span
class="math inline">\(F\)</span>-test for model utility is equivalent to
the <span class="math inline">\(t\)</span>-test for <span
class="math inline">\(H_0:\beta_1 = 0\)</span>.</p>
</div>
</div>
</div>
<div id="estimation-and-prediction" class="section level2">
<h2>Estimation and Prediction</h2>
<p>The most useful aspect of linear regression, aside from the ability
to infer linear relationships between variables, is to make inference
about the expected value of observations via a sampling distribution and
the make predictions of values for the dependent variable <span
class="math inline">\(y\)</span> given unobserved values of the
independent variable <span class="math inline">\(x\)</span>. Both these
estimated expected values and predictions have known sampling
distributions, which can be used to make inferences about their true
values. These aspects make linear regression (and indeed all statistical
models) particularly useful in real-world decision making.</p>
<div id="sample-statistics-and-inference-for-estimated-values-of-y"
class="section level3">
<h3>Sample Statistics and Inference for Estimated Values of <span
class="math inline">\(y\)</span></h3>
<div class="sidenote">
<p><strong>Example:</strong> Compute and plot a 95% confidence band for
the model <span class="math display">\[
city = \beta_0+\beta_1hwy
\]</span></p>
</div>
<p>The estimated expected value of <span
class="math inline">\(y_i\)</span> is <span class="math display">\[
\hat{y}_i = \hat{\beta}_0+\hat{\beta}_1x_i
\]</span> which is a linear function of two random variable <span
class="math inline">\(\hat{\beta}_0\)</span> and <span
class="math inline">\(\hat{\beta}_1\)</span>, both of which have known
Gaussian sampling distributions. It is interesting to note that both
<span class="math inline">\(\hat{\beta}_0\)</span> and <span
class="math inline">\(\hat{\beta}_1\)</span> are linear functions of the
random variable <span class="math inline">\(y\)</span>, so in fact, the
estimated expected value <span class="math inline">\(\hat{y}\)</span> is
a linear function of the random variable <span
class="math inline">\(y\)</span>.</p>
<p>The resulting standard error of <span
class="math inline">\(\hat{y_i}\)</span> is <span
class="math display">\[
s_{\hat{y_i}} =
\sqrt{s^2\left(\frac{1}{n}+\frac{(x_i-\bar{x})^2}{\sum_{i=1}^n(x_i-\bar{x})^2}\right)}
\]</span> and confidence intervals and hypothesis testing will be based
on the sampling distribution <span class="math display">\[
\frac{\hat{y}_i-E(y_i)}{s_{\hat{y}_i}}\sim t_{n-2}
\]</span> although to be fair, hypothesis testing is moot to a certain
extent, not directly our concern. We are more interested in the
confidence intervals or confidence band for the fitted values.</p>
<p>The <span class="math inline">\((1-\alpha)\%\)</span> confidence
interval for a fitted value is <span class="math display">\[
\hat{y}_i\pm t_{n-2,\alpha/2}s_{\hat{y}_i}.
\]</span> Note that the value for <span
class="math inline">\(s_{\hat{y}_i}\)</span> depends on the squared
distance between <span class="math inline">\(x_i\)</span> and <span
class="math inline">\(\bar{x}\)</span>, thus the confidence interval is
narrowest where <span class="math inline">\(x_i=\bar{x}\)</span>, or in
other words, we are most confident of our estimates for values of <span
class="math inline">\(x\)</span> closest to <span
class="math inline">\(\bar{x}\)</span>. (This seems like a good question
for the exam).</p>
<p>Again following from our example, we can plot the data in
<code>ggplot()</code> and note that when we invoke the
<code>geom_smooth(method = “lm”, se = TRUE)</code> , the <span
class="math inline">\(95\%\)</span> confidence band is automatically
added.</p>
</div>
<div id="example-95-confidence-band-for-estimated-values-of-y"
class="section level3 tabset tabset-pills boxed">
<h3 class="tabset tabset-pills">Example: 95% Confidence Band for
Estimated Values of <span class="math inline">\(y\)</span></h3>
<p>Compute and plot a 95% confidence band for the model <span
class="math display">\[
city = \beta_0+\beta_1hwy
\]</span></p>
<div id="solution-4" class="section level4">
<h4>Solution</h4>
<p>The estimated values <span class="math display">\[
\hat{y}=\beta_0+\beta_1x
\]</span> are estimated values, like sample means, i.e. because the
actual model we fit is <span class="math display">\[
y_i = \beta_0+\beta_1x_i+\epsilon_i
\]</span> and <span class="math inline">\(E(\epsilon_i)=0\)</span> then
<span class="math display">\[
\begin{align}
\hat{y}_i&amp;=E(y_i)\\
&amp;=E(\beta_0+\beta_1x_i)\\
&amp;=\hat{\beta}_0+\hat{\beta}_1x_i.
\end{align}
\]</span> Since the <span class="math inline">\(x_i\)</span>s are over
continuous space, the confidence interval for the expected values <span
class="math inline">\(\hat{y}\)</span> is a continuous function. We can
plot it as a shaded region with the best fit line and the values of
<span class="math inline">\(y_i\)</span>.</p>
<p><span class="math display">\[
\hat{y}_i\pm t_{n-2,\alpha/2}s_{\hat{y}_i}
\]</span> where <span class="math display">\[
s_{\hat{y_i}} =
\sqrt{s^2\left(\frac{1}{n}+\frac{(x_i-\bar{x})^2}{\sum_{i=1}^n(x_i-\bar{x})^2}\right)}.
\]</span> We can see the confidence band more easily for a linear model
fit to a subset of the actual data. If we use the
<code>geom_smooth()</code> function with <code>method="lm"</code> it
calls the <code>lm()</code> function and can automatically add the
confidence band with the option <code>se = TRUE</code>.</p>
</div>
<div id="code-3" class="section level4">
<h4>Code</h4>
<pre class="python"><code>
(ggplot(df.sample(n=25),aes(x = &#39;hwy&#39;,y = &#39;city&#39;))+
  geom_point()+
    geom_smooth(method = &quot;lm&quot;, se = TRUE, level = 0.95))</code></pre>
</div>
<div id="plot-1" class="section level4">
<h4>Plot</h4>
<pre><code>#&gt; &lt;ggplot: (307261227)&gt;</code></pre>
<p><img src="chap6_files/figure-html/unnamed-chunk-13-1.png" width="614" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="sample-statistics-and-inference-for-predicted-values-of-y"
class="section level3">
<h3>Sample Statistics and Inference for Predicted Values of <span
class="math inline">\(y\)</span></h3>
<div class="sidenote">
<p><strong>Example:</strong><br />
Fit the linear model <span class="math display">\[
city = \beta_0+\beta_1hwy
\]</span> to a randomly selected subset of the data and then predict
values for the remaining data. Plot the results with a 95% prediction
interval.</p>
</div>
<p>If we want to predict a value of <span
class="math inline">\(y\)</span> for <span
class="math inline">\(x^*\)</span> a specific (unobserved ) value of
<span class="math inline">\(x\)</span>, the prediction is <span
class="math display">\[
y^* = \hat{\beta}_0+\hat{\beta}_1x^*
\]</span> there is additional uncertainty, as we are making a prediction
rather than estimating an expected value. In this case, the standard
error of the prediction is <span class="math display">\[
s_{y^*} =
\sqrt{s^2\left(1+\frac{1}{n}+\frac{(x^*-\bar{x})^2}{\sum_{i=1}^n(x_i-\bar{x})^2}\right)}.
\]</span></p>
<p>The <span class="math inline">\((1-\alpha)\%\)</span> prediction
interval is then <span class="math display">\[
\hat{y}_i\pm t_{n-2,\alpha/2}s_{y^*}.
\]</span></p>
<p>If we add the equivalent prediction interval or band for Example
11.4.1, it becomes obvious that the prediction intervals are much wider
than the confidence intervals. It is difficult to see in this plot, but
the prediction interval also widens as our predictions get farther and
farther from <span class="math inline">\(\bar{x}\)</span>.</p>
<p>Prediction intervals are wider than confidence intervals and exhibit
the same property of widening as <span
class="math inline">\(x^*\)</span> increases in the distance from <span
class="math inline">\(\bar{x}\)</span>. We should exercise caution when
relying on predictions to make decisions, especially if these
predictions are extrapolations for points outside the data domain.
History is rife with examples of catastrophe that follows from this <a
href="https://www.wired.com/story/netflixs-challenger-is-a-gripping-look-at-nasa-in-crisis/">see
here.</a></p>
</div>
<div id="example-prediction-intervals"
class="section level3 tabset tabset-pills boxed">
<h3 class="tabset tabset-pills">Example: Prediction Intervals</h3>
<p>Fit the linear model <span class="math display">\[
city = \beta_0+\beta_1hwy
\]</span> to a randomly selected subset of the data and then predict
values for the remaining data. Plot the results with a 95% prediction
interval.</p>
<div id="solution-5" class="section level4">
<h4>Solution</h4>
<p>The prediction interval is similar to the confidence intervals for
expected value, noting that the form of the interval is <span
class="math display">\[
\hat{y}_i\pm t_{n-2,\alpha/2}s_{y^*}.
\]</span> where <span class="math display">\[
s_{y^*} =
\sqrt{s^2\left(1+\frac{1}{n}+\frac{(x^*-\bar{x})^2}{\sum_{i=1}^n(x_i-\bar{x})^2}\right)}.
\]</span> We can compute this interval as a continuous function using
statistical software.</p>
</div>
<div id="code-4" class="section level4">
<h4>Code</h4>
<pre class="python"><code>
df_tmp = df.sample(n=50)

df_fit = df_tmp.iloc[0:25,]
df_pred = df_tmp.iloc[25:50,]

model = statsmodels.formula.api.ols(&#39;city~hwy&#39;,df_fit)
result = model.fit()

pred_values = result.predict(df_pred)

df_pred[&#39;pred_values&#39;] = numpy.array(pred_values)


(ggplot(df_pred, aes(x=&#39;hwy&#39;, y=&#39;city&#39;))+
    geom_point()+
    geom_smooth(method=&quot;lm&quot;)
    )
</code></pre>
</div>
<div id="plot-2" class="section level4">
<h4>Plot</h4>
<pre><code>#&gt; &lt;ggplot: (307215601)&gt;</code></pre>
<p><img src="chap6_files/figure-html/unnamed-chunk-15-3.png" width="614" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="linear-regression-and-analysis-of-covariance-ancova"
class="section level2">
<h2>Linear Regression and Analysis of Covariance (ANCOVA)</h2>
<div class="sidenote">
<p><strong>Example:</strong><br />
Consider the <code>epa_data</code> for 2011-2021, we can fit a
one-factor ANOVA for city fuel economy to determine if there is a
difference between fuel economy for automatic versus manual
transmissions.</p>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>city</td>       <th>  R-squared:         </th> <td>   0.000</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>  -0.000</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>  0.7218</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 21 Nov 2022</td> <th>  Prob (F-statistic):</th>  <td> 0.396</td>  
</tr>
<tr>
  <th>Time:</th>                 <td>15:12:28</td>     <th>  Log-Likelihood:    </th> <td> -50621.</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td> 12681</td>      <th>  AIC:               </th> <td>1.012e+05</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td> 12679</td>      <th>  BIC:               </th> <td>1.013e+05</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>       <td>   21.7341</td> <td>    0.127</td> <td>  171.349</td> <td> 0.000</td> <td>   21.485</td> <td>   21.983</td>
</tr>
<tr>
  <th>trans[T.Manual]</th> <td>   -0.2709</td> <td>    0.319</td> <td>   -0.850</td> <td> 0.396</td> <td>   -0.896</td> <td>    0.354</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>13972.501</td> <th>  Durbin-Watson:     </th>  <td>   1.064</td> 
</tr>
<tr>
  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>937684.545</td>
</tr>
<tr>
  <th>Skew:</th>           <td> 5.818</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> 
</tr>
<tr>
  <th>Kurtosis:</th>       <td>43.488</td>   <th>  Cond. No.          </th>  <td>    2.82</td> 
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
<p><br> Results show that there is no significant difference between
fuel economy for the two transmission types.</p>
<p><br />
Now let’s consider adding the displacement or size of the engine in
litres as a covariate to help explain the difference in fuel
economy.</p>
</div>
<p>ANOVA is the process of analysing the effects of discrete factors on
dependent variables, but in many cases, there are other (continuous)
covariates that we should account for in our analysis. For instance, in
a test of several different treatments for a medical condition, the
treatments may be randomly assigned, but researchers still might want to
take individual participant’s ages into account when assessing the
effects. In this case, we can use a technique called the analysis of
covariance or ANCOVA to account or control for these extraneous
covariates.</p>
<p>In this case the traditional single factor ANOVA model is <span
class="math display">\[y_{ij} = \alpha_i+\epsilon_{ij}\]</span> where
<span class="math inline">\(\alpha_i\)</span> represents the <span
class="math inline">\(i\)</span>th treatment mean. Note that is some
literature the same model will be written as <span
class="math display">\[y_{ij} = \mu+\tau_i+\epsilon_{ij}\]</span> where
<span class="math inline">\(\mu\)</span> is the overall mean of the
dependent variable and the treatment effects <span
class="math inline">\(\tau_i\)</span> are constrained such that <span
class="math inline">\(\sum\tau_i = 0\)</span>, these models are
equivalent, the only difference is in the interpretation of the results
as <span class="math inline">\(\alpha_i \equiv \mu+\tau_i\)</span>.</p>
<p>If we wanted to account some covariate effects we would then add a
term to the model <span class="math display">\[y_{ij} =
\alpha_i+\beta(x_{ij}-\bar{x})+\epsilon_{ij}\]</span> to adjust or
control for the effects of the covariate values <span
class="math inline">\(x_{ij}\)</span>. We can partition the resulting
total sum of squares into the sum of the squares of the treatments
(SSTR), the sum of the squares of the covariates (SSR) and the sum of
the squares of the error (SSE). The results are presented in a table
similar to an ANOVA table, with the same calculation and interpretation
of the <span class="math inline">\(F\)</span> statistics.</p>
<p>The addition of the covariates can increase the power of the test in
detecting treatment effects and is thus useful in many situations. But
it should be noted that all the assumptions and constraints of linear
regression (regarding the independence and homogeneity of the residuals
and their normality) apply to the covariate effects in the model.</p>
<div id="assumptions-1" class="section level3 unnumbered">
<h3 class="unnumbered">Addtional Assumptions</h3>
<p>The standard assumptions from linear regression apply to ANCOVA.
There is an additional assumption; the <strong>homogeneity of the
slopes</strong>. That is that we assume that the slope, <span
class="math inline">\(\beta\)</span>, is the same for all levels of
treatment. We can test this by plotting the response (or dependent)
variable against the covariate and fitting a least-squares line for each
treatment. While we can’t expect the slopes to be exactly equal, we can
rely to some extent on the plots to give us some guidance. We can test
this assumption rigorously by fitting an interaction term between the
treatment and the covariate to our ANCOVA; if this interaction is
<em>not</em> statistically significant, we can assume that the slopes
for the treatments are equal.</p>
</div>
<div id="example-ancova-model-for-fuel-economy"
class="section level3 tabset tabset-pills boxed">
<h3 class="tabset tabset-pills">Example: ANCOVA Model for Fuel
Economy</h3>
<p>Consider the <code>epa_data</code> for 2011-2021, we can fit a
one-factor ANOVA for city fuel economy to determine if there is a
difference between fuel economy for automatic versus manual
transmissions.</p>
<p><br> Results show that there is no significant difference between
fuel economy for the two transmission types.<br />
Now let’s consider adding the displacement or size of the engine in
litres as a covariate to help explain the difference in fuel
economy.</p>
<div id="solution-6" class="section level4">
<h4>Solution</h4>
<p>The single factor ANOVA model is <span class="math display">\[
city_{ij} = trans_i +\epsilon_{ij}
\]</span> the ANOVA results show that there is no significant in city
fuel economy for different transmission types.</p>
<p>Now let’s consider adding the displacement or size of the engine in
litres as a covariate to help explain the difference in fuel economy. We
can create a scatter plot of city fuel economy for both transmission
types.</p>
<p><span class="math display">\[
city_{ij}=trans_i+\beta(disp_{ij}-\overline{disp})+\epsilon_{ij}
\]</span></p>
<pre><code>#&gt; &lt;ggplot: (307419986)&gt;</code></pre>
<p><img src="chap6_files/figure-html/unnamed-chunk-18-5.png" width="614" style="display: block; margin: auto;" />
These results show that the slopes are approximately equal; therefore,
the ANCOVA model is reasonable to fit using statistical software.</p>
</div>
<div id="code-5" class="section level4">
<h4>Code</h4>
<p>We now add displacement as our covariate and see the results:</p>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>city</td>       <th>  R-squared:         </th> <td>   0.527</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.527</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   6946.</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 21 Nov 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  
</tr>
<tr>
  <th>Time:</th>                 <td>15:12:34</td>     <th>  Log-Likelihood:    </th> <td> -35287.</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td> 12465</td>      <th>  AIC:               </th> <td>7.058e+04</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td> 12462</td>      <th>  BIC:               </th> <td>7.060e+04</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>       <td>   30.6447</td> <td>    0.100</td> <td>  306.715</td> <td> 0.000</td> <td>   30.449</td> <td>   30.841</td>
</tr>
<tr>
  <th>trans[T.Manual]</th> <td>   -0.6627</td> <td>    0.102</td> <td>   -6.512</td> <td> 0.000</td> <td>   -0.862</td> <td>   -0.463</td>
</tr>
<tr>
  <th>disp</th>            <td>   -3.2095</td> <td>    0.027</td> <td> -116.890</td> <td> 0.000</td> <td>   -3.263</td> <td>   -3.156</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>11122.788</td> <th>  Durbin-Watson:     </th>  <td>   1.306</td> 
</tr>
<tr>
  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>795602.481</td>
</tr>
<tr>
  <th>Skew:</th>           <td> 3.996</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> 
</tr>
<tr>
  <th>Kurtosis:</th>       <td>41.314</td>   <th>  Cond. No.          </th>  <td>    11.6</td> 
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
<p>Note that now the difference between the transmission types and the
displacement are statistically significant. The interaction term between
transmission type and displacement is not statistically significant,
verifying the homogeneity of the slopes.</p>
</div>
</div>
</div>
<div id="videos" class="section level2 tabset tabset-pills boxed">
<h2 class="tabset tabset-pills">Videos</h2>
<div id="part-1" class="section level3">
<h3>Part 1</h3>
<div style="max-width: 590px">
<div
style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe width="590" height="332" src="https://web.microsoftstream.com/embed/video/7a5368c0-df20-44b6-99f9-383bbbd70bdc?autoplay=false&amp;showinfo=true" allowfullscreen style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
</iframe>
</div>
</div>
</div>
<div id="part-2" class="section level3">
<h3>Part 2</h3>
<div style="max-width: 590px">
<div
style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe width="590" height="332" src="https://web.microsoftstream.com/embed/video/604108ab-0262-423b-9588-3d2d524ec3bc?autoplay=false&amp;showinfo=true" allowfullscreen style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
</iframe>
</div>
</div>
</div>
<div id="part-3" class="section level3">
<h3>Part 3</h3>
<div style="max-width: 590px">
<div
style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe width="590" height="332" src="https://web.microsoftstream.com/embed/video/a0cf9b4d-cb36-406a-94e2-70fdd1ce3420?autoplay=false&amp;showinfo=true" allowfullscreen style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
</iframe>
</div>
</div>
</div>
<div id="part-4" class="section level3">
<h3>Part 4</h3>
<div style="max-width: 590px">
<div
style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe width="590" height="332" src="https://web.microsoftstream.com/embed/video/40c88a5e-c7af-4757-8147-7b35377acd52?autoplay=false&amp;showinfo=true" allowfullscreen style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
</iframe>
</div>
</div>
</div>
<div id="part-5" class="section level3">
<h3>Part 5</h3>
<div style="max-width: 590px">
<div
style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe width="590" height="332" src="https://web.microsoftstream.com/embed/video/e7cfe5a2-e1db-49d4-bfbc-625527ceea25?autoplay=false&amp;showinfo=true" allowfullscreen style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
</iframe>
</div>
</div>
</div>
<div id="part-6" class="section level3">
<h3>Part 6</h3>
<div style="max-width: 590px">
<div
style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe width="590" height="332" src="https://web.microsoftstream.com/embed/video/3407c8f6-a3e7-4803-bb95-231547c322d5?autoplay=false&amp;showinfo=true" allowfullscreen style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
</iframe>
</div>
</div>
</div>
<div id="part-7" class="section level3">
<h3>Part 7</h3>
<div style="max-width: 590px">
<div
style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe width="590" height="332" src="https://web.microsoftstream.com/embed/video/537c9684-822b-4443-a7e9-8b0418f91926?autoplay=false&amp;showinfo=true" allowfullscreen style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
</iframe>
</div>
</div>
</div>
<div id="part-8" class="section level3">
<h3>Part 8</h3>
<div style="max-width: 590px">
<div
style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe width="590" height="332" src="https://web.microsoftstream.com/embed/video/5c504275-6314-43d1-b59a-dde0fcade879?autoplay=false&amp;showinfo=true" allowfullscreen style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
</iframe>
</div>
</div>
</div>
</div>
<!--


Next, we can perform the ANCOVA analysis

                Df   Sum Sq   Mean Sq   F value   Pr($>$F)
  ----------- ---- -------- --------- --------- ----------
  group          2    72.13     36.07    213.05     0.0000
  pretest        1   101.29    101.29    598.32     0.0000
  Residuals     41     6.94      0.17           

Note that if we just perform a single factor ANOVA using just the group,
we still see that it is significant:

                Df   Sum Sq   Mean Sq   F value   Pr($>$F)
  ----------- ---- -------- --------- --------- ----------
  group          2    72.13     36.07     14.00     0.0000
  Residuals     42   108.23      2.58           

but note that in the ANOVA, the residual sum of squares is considerably
larger than for the ANCOVA, indicating that there is a substantial
amount of variance in the outcomes explained by the pre-program
test results.
-->
</div>

<!--
&nbsp;
&nbsp;
<footer class="copyright">
<p>Copyright &copy;2021 Queensland University of Technology, All rights reserved.</p>
</footer>
-->

<!-- js for accordion button -->
<script>
var acc = document.getElementsByClassName("week");
var i   ;

for (i = 0; i < acc.length; i++) {
  acc[i].addEventListener("click", function() {
    /* Toggle between adding and removing the "active" class,
    to highlight the button that controls the panel */
    this.classList.toggle("active");

    /* Toggle between hiding and showing the active panel */
    var panel = this.nextElementSibling;
    if (panel.style.display === "block") {
      panel.style.display = "none";
    } else {
      panel.style.display = "block";
    }
  });
}
</script>



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
