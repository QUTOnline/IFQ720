<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Module 3</title>

<script src="site_libs/header-attrs-2.14/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<head>
<link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

</head>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="QUTReadings.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">IFQ720</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="contact.html">Contacts</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Readings
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="chap1.html">Module 1</a>
    </li>
    <li>
      <a href="chap2.html">Module 2</a>
    </li>
    <li>
      <a href="chap3.html">Module 3</a>
    </li>
    <li>
      <a href="chap4.html">Module 4</a>
    </li>
    <li>
      <a href="chap5.html">Module 5</a>
    </li>
    <li>
      <a href="chap6.html">Module 6</a>
    </li>
    <li>
      <a href="chap7.html">Module 7</a>
    </li>
    <li>
      <a href="chap8.html">Module 8</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Workshops
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-01/">Workshop 1</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-02/">Workshop 2</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-03/">Workshop 3</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-04/">Workshop 4</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-05/">Workshop 5</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-06/">Workshop 6</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-07/">Workshop 7</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-08/">Workshop 8</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-09/">Workshop 9</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-10/">Workshop 10</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-11/">Workshop 11</a>
    </li>
    <li>
      <a href="https://shiny.qutmaths.net/MXB107/Workshops/ws-12/">Workshop 12</a>
    </li>
  </ul>
</li>
<li>
  <a href="assessment.html">Assessments</a>
</li>
<li>
  <a href="videos.html">Videos</a>
</li>
<li>
  <a href="https://blackboard.qut.edu.au/webapps/blackboard/execute/announcement?method=search&amp;context=course&amp;course_id=_164898_1&amp;handle=cp_announcements&amp;mode=reset">IFQ720 Canvas</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="mailto:gentry.white@qut.edu.au">
    <span class="fa fa-envelope fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://mxb1072022.slack.com">
    <span class="fab fa-slack fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://https://qutvirtual4.qut.edu.au/web/qut/hiq">
    <span class="fa fa-users"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<div><a href="https://www.qut.edu.au">
     <img alt="QUT" src="logo.png" width=50px" align="left" style="vertical-align:middle;margin:10px 10px 10px 0px"></a></div>

<div id="header">



<h1 class="title toc-ignore">Module 3</h1>

</div>

<div id="TOC">
<ul>
<li><a href="#sampling-distributions"
id="toc-sampling-distributions">Sampling Distributions</a>
<ul>
<li><a href="#observational-versus-experimental-data"
id="toc-observational-versus-experimental-data">Observational versus
Experimental Data</a></li>
<li><a
href="#sampling-distribution-of-statistics-and-the-central-limit-theorem"
id="toc-sampling-distribution-of-statistics-and-the-central-limit-theorem">Sampling
Distribution of Statistics and the Central Limit Theorem</a></li>
<li><a href="#assessing-normality"
id="toc-assessing-normality">Assessing Normality</a></li>
<li><a href="#sampling-distribution-of-a-sample-proportion"
id="toc-sampling-distribution-of-a-sample-proportion">Sampling
Distribution of a Sample Proportion</a></li>
<li><a href="#large-sample-estimation"
id="toc-large-sample-estimation">Large Sample Estimation</a></li>
<li><a href="#confidence-intervals"
id="toc-confidence-intervals">Confidence Intervals</a></li>
<li><a href="#large-sample-confidence-interval-for-the-population-mean"
id="toc-large-sample-confidence-interval-for-the-population-mean">Large
Sample Confidence Interval for the Population Mean</a></li>
<li><a
href="#large-sample-confidence-intervals-for-the-population-proportion"
id="toc-large-sample-confidence-intervals-for-the-population-proportion">Large
Sample Confidence Intervals for the Population Proportion</a></li>
<li><a href="#estimating-the-difference-between-two-population-means"
id="toc-estimating-the-difference-between-two-population-means">Estimating
the Difference Between Two Population Means</a></li>
<li><a
href="#estimating-the-difference-between-two-population-proportions"
id="toc-estimating-the-difference-between-two-population-proportions">Estimating
the Difference Between Two Population Proportions</a></li>
</ul></li>
</ul>
</div>

<div id="sampling-distributions" class="section level1">
<h1>Sampling Distributions</h1>
<p>In this module, we will explore two fundamental ideas about sampling
and inference. How do we efficiently and effectively collect data to
answer our questions of interest, and if we compute sample statistics
based on a random sample, how can we make inferences about population
parameters?</p>
<div id="observational-versus-experimental-data"
class="section level2 unnumbered">
<h2 class="unnumbered">Observational versus Experimental Data</h2>
<div class="sidenote">
<p><strong>Example:</strong><br />
A large law firm has thousands of case files stored on a computer
database; they would like to estimate associates’ workload based on the
number of cases they take on by estimating the average number of hours
needed to resolve a case. To do this, they randomly select <span
class="math inline">\(n=100\)</span> case files to compute the average
number of hours needed per case.</p>
</div>
<div class="sidenote">
<p><strong>Example:</strong><br />
A pharmaceutical company wants to determine if its new drug is effective
at helping people fall asleep. <span
class="math inline">\(n=3000\)</span> participants are selected to
participate in the study and randomly assigned to either take the new
drug or a placebo. They record their experiences in falling asleep each
night for a month.</p>
</div>
<p>Two basic scenarios arise in applied data analysis and modelling:
observational studies and experimental studies.</p>
<p>If we are collecting or sampling data that already exists, i.e. we
have no control over how we created the data, we are in a situation
described as an <strong>observational study</strong>.</p>
<p>If the data are generated in a controlled experimental environment,
then the data arise from an <strong>experimental study</strong>.</p>
<p>There are differences in how we both analyse and interpret the
results of observational and experimental studies, which we will discuss
as they arise. It is important as data analysts to correctly identify
these two situations before we collect data and analyse and interpret
our results.</p>
<div id="sampling-plans-and-experimental-designs"
class="section level3">
<h3>Sampling Plans and Experimental Designs</h3>
<p>We collect and use data samples because we want to understand the
characteristics of the populations characterised as
<strong>parameters</strong> of some underlying probabilistic model.
Because it is typically impossible to collect measurements or
observations from an entire population, we rely on samples and sample
statistics to make inferences about the population of interest. To make
an efficient and effective inference, we need to choose an appropriate
sampling method to ensure that our sample represents the population.
Sampling plans are schemes for efficiently collecting data in an
observational study to ensure that the sample represents the population.
There are several methods for this, depending on the situation.</p>
<p>There is no need for sophisticated sampling plans in an experimental
study because the experimenter predetermines every aspect of how the
data are produced. In this case, the important part of the process is
the experimental design. In an experimental design (typically), there is
some treatment or treatments that we want to test for their effects (and
possible interactions) on some measurable quantity. In this case, the
experimental design is concerned with allocating treatments to subjects
to ensure that any observed effects are due only to the treatments and
not extraneous factors.</p>
<p>In both observational and experimental studies, some degree of
randomness is inherent in how subjects are selected and allocated
treatments. The most straightforward method involves simple random
sampling in observational studies.</p>
<div class="sidenote">
<p><strong>Randomised Controlled Designs</strong> are analogous to
simple random sampling for experimental designs. In these cases,
treatments are assigned to subjects completely at random and are
considered the “gold standard” in experimental designs. Just as we will
see that there are modifications to simple random sampling for
observational studies, we will later see that there are also options for
modifying experimental designs. In both cases, these modifications are
intended to improve the results of studies and our inferences.</p>
</div>
<div class="boxed">
<p><strong>Simple random sampling</strong>, as the name implies, is the
most basic approach to collecting data, it consists of simply just
choosing completely at random a subset of size <span
class="math inline">\(n\)</span> from a population of size <span
class="math inline">\(N\)</span>.</p>
</div>
<p>Simple random sampling is used in <strong>observational
study</strong> because the data already exists, and we are merely
<em>observing</em> it and collecting measurements. Most data analysis
problems fall into this category of studies. There are caveats in
collecting data for observational studies that can lead to errors in the
conclusions reached. These are:</p>
<div class="boxed">
<ul>
<li><p><strong>Non-response</strong> In the case of survey data, a fixed
number of surveys are issued, and the responses recorded. Ideally, 100%
of the survey subjects would respond, but in practice, this rarely
happens. In this situation, you have to consider the possibility of bias
in the responses. Did people not respond completely at random, or is
there some underlying pattern to their non-response that creates
<strong>bias</strong> in the data, thus skewing your
conclusions.</p></li>
<li><p><strong>Under-coverage</strong> Under coverage is a similar
problem to non-response bias but instead results from an improper
selection of a sampling frame. For example, consider the use of
land-line phone number banks to perform phone surveys. Because many
people don’t own (or answer) land-line phone calls, the selection of
land-line numbers as a sampling frame can result in a biased sample. One
of the most famous examples of this was the 1948 US Presidential
Election. On the eve of election day, most polls and conventional wisdom
indicated that sitting President, Democrat Harry S Truman, would be
defeated by his Republican challenger Thomas Dewey. The newspaper
<em>The Chicago Tribune</em> published its morning-after edition with
the headline “Dewey Defeats Truman”, even though Truman won re-election
with a comfortable margin, and the Democrats also took control of both
houses of Congress. What went wrong? Early polls showed Truman and the
Democrats heading for such extreme losses that they stopped collecting
data; additionally, some of the most prominent polls were based on
magazine readership or by phone (when phone ownership was far from
universal). In effect, the polling data drastically failed to cover the
target population and thus was biased, leading to a drastic error in
predictions. (This is also an excellent example of confirmation
bias).</p></li>
<li><p><strong>Wording bias</strong> Writing survey questions can be
more art than science. Still, we must take care in writing questions to
capture the full range of possible responses without influencing their
responses. So-called “push polls”, which only ask for people’s responses
in terms of their positive or negative impressions of candidates and
issues are excellent examples of how not to write a survey
questions.</p></li>
</ul>
</div>
<p>When using a survey to collect information, it is especially
important to know how language can affect responses and eventual
outcomes. There are several scientific approaches to validation survey
instruments and items, but in general, it is good to be aware of what
makes a “good” or “bad” survey question.</p>
<div class="boxed">
<p><strong>Good and Bad Survey Questions</strong><br />
Here are some examples of good and bad survey questions (see <a
href="https://www.surveymonkey.com/mp/5-common-survey-mistakes-ruin-your-data/">Survey
Monkey</a> for more examples and information on writing good
surveys.)</p>
<ul>
<li><p>How short was Napoleon? — this question biases the readers
response by establishing that Napoleon is short.</p></li>
<li><p>How would you describe Napoleon’s height? — this is better
because is offers no frame of reference or predisposition to the readers
response.</p></li>
<li><p>Where do you enjoy drinking beer? — this is a loaded question
because it assumes that the respondent drinks beer.</p></li>
<li><p>Do you enjoy drinking beer, if so where? — this is better,
instead it ought to be presented as two separate questions with the
option of skipping the second question if the answer to the first is
“no”.</p></li>
<li><p>How useful will this textbook be to students and professionals in
the field? — this is a “double-barrelled” question; it asked the
respondent to make the same judgement for two different situations. The
textbook might be excellent for students but a poor reference for
professionals</p></li>
<li><p>How useful will this textbook be for students? How useful will
this textbook be for young professionals in the field? — splitting the
question into two questions will get more accurate responses.</p></li>
</ul>
</div>
<p>There are often other causes of misleading results from an
observational study, including confounding (where it is impossible to
distinguish between two or more possible causes for an effect) and
deception where people may deliberately misrepresent their opinions or
behaviours.</p>
<div class="boxed">
<p><strong>Example:</strong><br />
A pharmaceutical company wants to determine if its new drug is effective
at helping people fall asleep. <span
class="math inline">\(n=3000\)</span> participants are selected to
participate in the study and randomly assigned to either take the new
drug or a placebo. They record their experiences in falling asleep each
night for a month.</p>
<p>This situation is an example of an experimental design and seems
relatively straightforward, but there are several issues to
consider.</p>
<ul>
<li>How were participants selected? Were they a random sample of people
from the general population, or were they recruited from people
diagnosed with sleep disorders (good) or from people who self-reported
themselves as having sleep issues (not so good)?</li>
<li>How did people “record” their experiences falling asleep? Would it
be better to have them try to sleep in a lab where they can be monitored
(possibly disturbing in itself), or is there a way to record their sleep
experiences in their own home without relying on their impressions?</li>
<li>Is there any interaction between the drug and other factors like
genes or gender? Hopefully, this is mitigated by selecting a large
enough sample and assigning treatments at random.</li>
</ul>
</div>
<p>Experimental studies are considered a “gold standard” as the best
possible source of information concerning any phenomenon. While this may
be the case, it is often impossible or very difficult to collect
experimental data, and we have to make do with observational
studies.</p>
</div>
<div id="stratified-random-sampling" class="section level3 unnumbered">
<h3 class="unnumbered">Stratified Random Sampling</h3>
<p>Stratified random sampling is an extension of simple random sampling
that divides the population into non-overlapping
<strong><em>strata</em></strong> and draws random samples from each
stratum.</p>
<div class="boxed">
<ul>
<li><p><strong>Pre-stratification</strong> is when we identify strata
<em>before</em> we draw our samples and draw samples from these
strata.</p></li>
<li><p><strong>Post-stratification</strong> is when the results of a
simple random samples are divided into strata after they are collected.
In this case, results can be re-weighted to adjust for demographic
realities.</p></li>
</ul>
</div>
<div class="boxed">
<p><strong>Example:</strong><br />
A large law firm has thousands of case files stored on a computer
database; they would like to estimate associates’ workload based on the
number of cases they take on by estimating the average number of hours
needed to resolve a case. To do this, they randomly select <span
class="math inline">\(n=100\)</span> case files to compute the average
number of hours needed per case.</p>
<p>While this random sampling plan seems pretty good, the law firm knows
that the type of case greatly influences how long they take to resolve.
The law firm also knows that approximately 20% of their cases are
criminal, and 80% are civil matters. This proportion should be taken
into account when analysing the data, and post-stratification can be
used to correct any possible bias in a random sample. The sample of 100
case files averaged 51 hours to resolve. The sample consisted of 43
criminal cases and 57 civil cases. The criminal cases took, on average,
65 hours to resolve, and the civil cases took on average 37 hours to
resolve. The re-weighted average is <span
class="math display">\[(0.2)(65)+(0.8)(37) = 42.6\mbox{ hours
}.\]</span> A similar procedure is used in political polling to correct
gender imbalance and other demographic factors.</p>
<p>Another option would have been to <em>pre-stratify</em> the sample by
choosing 20 criminal cases at random and 80 civil cases at random and
then computing the average for the resulting sample of 100 cases.</p>
</div>
</div>
<div id="cluster-sampling" class="section level3 unnumbered">
<h3 class="unnumbered">Cluster Sampling</h3>
<p>Cluster sampling can be a more cost-effective means of collecting
data when there are limited resources or a lack of information about
individuals in the target population. It can also be useful if members
of the cluster are very similar in terms of the characteristics of
interest.</p>
<div class="boxed">
<p><strong>Example: Academic Performance</strong><br />
</p>
<p>A <strong>cluster sample</strong> is a random sample where the
sampling units are clusters.</p>
<p>If you wanted to look at academic performance across school districts
but could not obtain access to individual students’ academic results. In
that case, you might consider treating individuals schools as clusters
and collect your data from them.</p>
</div>
</div>
<div id="non-random-sampling-plans" class="section level3 unnumbered">
<h3 class="unnumbered">Non-random Sampling Plans</h3>
<p>Not all sampling involves collecting data at random, and in some
cases, a completely random sampling scheme may not work well for a
situation.</p>
<div class="boxed">
<p><strong>Non-random sampling schemes:</strong><br />
- <strong>Sequential sampling </strong> schemes deal with a list of
potential subjects sequentially, and samples are selected periodically,
sometimes these are referred to as <strong><span
class="math inline">\(1\)</span> in <span
class="math inline">\(k\)</span> random sampling</strong> schemes.</p>
<ul>
<li><p><strong>Convenience samples</strong> where subjects self-select
to participate in the survey (the IMDB user rankings are an example of
this). But it is important to recall that their use for inference is
highly suspect due to <strong>self-selection bias</strong>.</p></li>
<li><p><strong>Snowball sampling</strong> is like convenience sampling
except that participants are asked to refer other people to participate
in the study, and they can also suffer from self-selection
bias.</p></li>
<li><p><strong>Quota samples</strong>, which seek to ensure a
demographic balance to the sample, often involve some non-random
element. They are designed to ensure that the sample is “balanced”
demographically. Still, they run the risk of incorrect balancing and the
possibility that a very small subset of the sample could have an
excessive influence on the results.</p></li>
</ul>
</div>
</div>
</div>
<div
id="sampling-distribution-of-statistics-and-the-central-limit-theorem"
class="section level2">
<h2>Sampling Distribution of Statistics and the Central Limit
Theorem</h2>
<p>When we collect data as a sample of size <span
class="math inline">\(n\)</span> from a population of size <span
class="math inline">\(N\)</span>, we can then calculate some sample
statistics as an estimate of population-level characteristics or
parameters. But because these statistics are functions of random
variables, the resulting statistics are also random variables. Thus are
subject to interpretation via probabilistic inference via their sampling
distribution.</p>
<div class="boxed">
<p>The <strong>sampling distribution</strong> is the probability
distribution for a sample statistic.</p>
</div>
<div id="deriving-distributions-of-statistics"
class="section level3 unnumbered">
<h3 class="unnumbered">Deriving distributions of Statistics</h3>
<p>The sampling distribution theoretically arises when taking repeated
random samples of size <span class="math inline">\(n\)</span> from a
population. Typically is derived in one of three ways:</p>
<div class="boxed">
<ol style="list-style-type: decimal">
<li><p>Mathematical derivation directly from the probability
distribution of the data and the laws of probability.</p></li>
<li><p>Simulation-based methods where the sampling distribution can be
approximated by taking repeated samples and making an empirical estimate
of the probability distribution.</p></li>
<li><p>Asymptotic approximation, where under certain theorems and
properties of the sample statistics, as <span
class="math inline">\(n\)</span> increases, it is reasonable to assume
properties of the sampling distribution.</p></li>
</ol>
</div>
<p>The sampling distribution is important because it depends on the
population parameter. It allows us to make inferences about the
population parameter via probabilistic statements about the sample
statistic.</p>
</div>
<div id="the-central-limit-theorem" class="section level3 unnumbered">
<h3 class="unnumbered">The Central Limit Theorem</h3>
<div class="sidenote">
<p><strong>Example:</strong><br />
A beverage plant calibrates a bottling machine to fill <span
class="math inline">\(33\)</span> cL bottles, but there is some
variability in the process, and from time to time, the machine needs
adjustment. Every hour, an employee takes a sample of ten bottles and
measures the amount of beverage in each one to monitor the machine.
Prior testing shows that the volume of each bottle follows a Gaussian
distribution with a mean of <span class="math inline">\(33\)</span> cL
and a standard deviation of <span class="math inline">\(0.6\)</span> cL.
What is the probability of observing a sample mean of <span
class="math inline">\(\bar{x}\leq 32.7\)</span> cL based on a sample of
<span class="math inline">\(n=10\)</span> bottles?</p>
</div>
<p>The central limit theorem is one of the most important theorems in
statistics which gives a powerful tool for making an inference based on
sample statistics about the mean or expected value of the
population.</p>
<div class="boxed">
<p><strong>The Central Limit Theorem</strong> For a sample of size <span
class="math inline">\(n\)</span> from any random probability
distribution with expected value <span
class="math inline">\(\mu\)</span> and variance <span
class="math inline">\(\sigma^2\)</span> then: <span
class="math display">\[\frac{\sqrt{n}(\bar{x}-\mu}{\sigma}\stackrel{p}{\rightarrow}
N(0,1)\]</span> or in simpler terms, as <span
class="math inline">\(n\)</span> increases the sampling distribution of
<span class="math inline">\(\bar{x}\)</span> converges in probability a
Gaussian distribution with expected value <span
class="math inline">\(\mu\)</span> and variance <span
class="math inline">\(\sigma^2/n\)</span>.</p>
</div>
<p>The Central Limit Theorem is a powerful tool because it gives a means
of making inferences about the population mean regardless of the
underlying probability distribution. The question arises, however, how
large does <span class="math inline">\(n\)</span> have to be true for
the central limit theorem to apply? And of course, the answer is: it
depends. In general, the more the data looks like a Gaussian
distribution, the smaller <span class="math inline">\(n\)</span> can be;
but conservatively <span class="math inline">\(30\)</span> is a
reasonable lower limit.</p>
<div class="boxed">
<p><strong>Standard Error of the Sample Mean</strong> The
<strong>standard error of the estimate (SE)</strong> is the standard
deviation of the sample statistic used to estimate a population
parameter. Therefore the standard error of the mean (SEM, or just SE) is
the standard deviation of <span class="math inline">\(\bar{x}\)</span>,
<span class="math inline">\(\sigma^2/n\)</span>.</p>
</div>
<div class="boxed">
<p><strong>Example:</strong><br />
A beverage plant calibrates a bottling machine to fill <span
class="math inline">\(33\)</span> cL bottles, but there is some
variability in the process, and from time to time, the machine needs
adjustment. Every hour, an employee takes a sample of ten bottles and
measures the amount of beverage in each one to monitor the machine.
Prior testing shows that the volume of each bottle follows a Gaussian
distribution with a mean of <span class="math inline">\(33\)</span> cL
and a standard deviation of <span class="math inline">\(0.6\)</span> cL.
What is the probability of observing a sample mean of <span
class="math inline">\(\bar{x}\leq 32.7\)</span> cL based on a sample of
<span class="math inline">\(n=10\)</span> bottles?</p>
<p><strong>Solution:</strong><br />
The sample mean is <span class="math inline">\(\bar{x}\)</span> and the
sampling distribution is <span class="math display">\[
\begin{align}
\bar{x}&amp;\sim N\left(\mu,\sigma^2/n\right)\\
&amp;\sim N(33,0.6/10)
\end{align}
\]</span> So <span class="math inline">\(Pr(\bar{x}\leq32.7)\)</span>
can be found using the standard normal distribution. <span
class="math display">\[
\begin{align}
Pr(\bar{x}\leq
32.7)&amp;=Pr\left(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}}\leq\frac{32.7-\mu}{\sigma/\sqrt{n}}\right)\\
&amp;=Pr\left(Z\leq\frac{32.7-33}{\sqrt{0.6/10}}\right)\\
&amp;=Pr(Z\leq -1.22)\\
&amp;=0.11
\end{align}
\]</span> We can look this probability up in our tables or use the
<code>pnorm</code> function in <code>R</code>.</p>
</div>
</div>
</div>
<div id="assessing-normality" class="section level2">
<h2>Assessing Normality</h2>
<p>Assessing normality, or at least some degree of normality in your
data is an important task to determine the validity of your results when
applying statistical tools of inference based on asymptotic properties
of estimators.</p>
<div id="graphically-assessing-normality"
class="section level3 unnumbered">
<h3 class="unnumbered">Graphically Assessing Normality</h3>
<p>There are several tools available for graphically assessing the
normality of the data:</p>
<div class="boxed">
<ul>
<li><p><em>Histograms</em> can be useful for an initial graphical
summary of the data that reveals the basic shape of the distribution. If
the shape of the histogram deviates too much from asymmetric uni-modal
shape, we can assume that the normality assumptions are not
valid.</p></li>
<li><p><em>Boxplots</em> can be useful for showing both outliers and
often give a better picture of skew in the data than histograms. Extreme
clusters of an excessive number of outliers can be evidence of
non-normality.</p></li>
<li><p><em>Normal Probability Plots</em> or <span
class="math inline">\(q-q\)</span> plots are constructed by plotting the
sorted data values against their expected <span
class="math inline">\(Z\)</span>-scores. Data from a normal distribution
should fall on a straight diagonal line. If the data do not form an
approximately straight diagonal line, this is evidence of
non-normality.</p></li>
</ul>
</div>
<p>###Graphical indications of normality</p>
<p><strong>Gaussian Data</strong><br />
The following data of size <span class="math inline">\(n=500\)</span>
were generated from a Gaussian distribution with <span
class="math inline">\(\mu = 2.7\)</span> and <span
class="math inline">\(\sigma=1.3)\)</span></p>
<pre><code>#&gt; &lt;ggplot: (309658270)&gt;</code></pre>
<p><img src="chap3_files/figure-html/unnamed-chunk-2-1.png" width="100%" style="display: block; margin: auto;" /></p>
<pre><code>#&gt; &lt;ggplot: (309443966)&gt;</code></pre>
<p><img src="chap3_files/figure-html/unnamed-chunk-2-2.png" width="100%" style="display: block; margin: auto;" /></p>
<pre><code>#&gt; &lt;ggplot: (309592347)&gt;</code></pre>
<p><img src="chap3_files/figure-html/unnamed-chunk-3-5.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Notice how the the histogram appears unimodal and symmetric, the
points fall more or less on the diagonal line of the <span
class="math inline">\(q-q\)</span> plot, and the boxplot also appears
symmetric.</p>
<p><strong>Uniform Data</strong></p>
<p>The following data of size <span class="math inline">\(n=500\)</span>
were generated from a uniform distribution over the interval <span
class="math inline">\((0,1)\)</span>.</p>
<pre><code>#&gt; &lt;ggplot: (309725491)&gt;</code></pre>
<p><img src="chap3_files/figure-html/unnamed-chunk-4-7.png" width="614" style="display: block; margin: auto;" /></p>
<pre><code>#&gt; &lt;ggplot: (309699284)&gt;</code></pre>
<p><img src="chap3_files/figure-html/unnamed-chunk-4-8.png" width="614" style="display: block; margin: auto;" /></p>
<pre><code>#&gt; &lt;ggplot: (309725353)&gt;</code></pre>
<p><img src="chap3_files/figure-html/unnamed-chunk-5-11.png" width="50%" style="display: block; margin: auto;" /></p>
<p>While the boxplot appears symmetric, the historgram here doesn’t
appear to be unimodal and the points on the <span
class="math inline">\(q-q\)</span> plot clearly diverge at the upper and
lower limits.</p>
<p><strong>Exponential Data</strong><br />
The following data of size <span class="math inline">\(n=500\)</span>
were generated from an exponential distribution <span
class="math inline">\(Exp(1)\)</span>.</p>
<pre><code>#&gt; &lt;ggplot: (309602716)&gt;</code></pre>
<p><img src="chap3_files/figure-html/unnamed-chunk-6-13.png" width="614" style="display: block; margin: auto;" /></p>
<pre><code>#&gt; &lt;ggplot: (309772108)&gt;</code></pre>
<p><img src="chap3_files/figure-html/unnamed-chunk-6-14.png" width="614" style="display: block; margin: auto;" /></p>
<pre><code>#&gt; &lt;ggplot: (309555417)&gt;</code></pre>
<p><img src="chap3_files/figure-html/unnamed-chunk-7-17.png" width="50%" style="display: block; margin: auto;" /></p>
<p>The historgram in this case appears skewed as does the boxplot. The
points on the <span class="math inline">\(q-q\)</span> plot diverge at
the lower end.</p>
</div>
</div>
<div id="sampling-distribution-of-a-sample-proportion"
class="section level2">
<h2>Sampling Distribution of a Sample Proportion</h2>
<div class="sidenote">
<p><strong>The Sampling Distribution of the Sample
Proportion</strong><br />
Assume that a pollster asked <span class="math inline">\(653\)</span>
people about their two-party preference for the upcoming federal
elections. If <span class="math inline">\(333\)</span> people expressed
their preference for the LNP, what is the sampling distribution of <span
class="math inline">\(\hat{p}\)</span>?</p>
<p>Now assume that the true proportion of people who express a
preference for the LNP is <span class="math inline">\(0.5\)</span>. What
is the probability of observing a sample proportion larger than the one
calculated previously?</p>
</div>
<p>In most cases, we are interested in estimating the population mean
from a sample mean. But in many cases, we are interested more in
estimating the population proportion from a sample proportion. In these
instances, we can use the central limit theorem to determine the
sampling distribution.</p>
<div id="properties-of-the-sample-proportion"
class="section level3 unnumbered">
<h3 class="unnumbered">Properties of the Sample Proportion</h3>
<p>For a sample of size, <span class="math inline">\(n\)</span> let
<span class="math inline">\(x\)</span> be the number of members in the
sample who have a trait of interest. The sample estimate of the
population proportion <span class="math inline">\(p\)</span> of the
population who possess this trait is <span
class="math display">\[\hat{p}=\frac{x}{n}.\]</span> If we assume that
the sample statistic <span class="math inline">\(x\)</span> follows a
binomial distribution with probability <span
class="math inline">\(p\)</span> and size <span
class="math inline">\(n\)</span> then <span
class="math inline">\(x\)</span> has expected value <span
class="math inline">\(np\)</span> and standard deviation <span
class="math inline">\(\sqrt{np(1-p)}\)</span>. Then from the properties
of the expectation and variance, the estimator <span
class="math inline">\(\hat{p}\)</span> has the expectation <span
class="math display">\[E(\hat{p}) = p\]</span> and standard error <span
class="math display">\[\mbox{SE}(\hat{p})=
\sqrt{\frac{p(1-p)}{n}}.\]</span></p>
<div class="boxed">
<p><strong>Rule of Thumb for Assuming Normality of the Sample
Proportion</strong><br />
When <span class="math inline">\(n\)</span> and <span
class="math inline">\(p\)</span> are sufficiently large, then the
assumptions of normality for the central limit theorem can be applied.
The standard rule of thumb for this is to assume that if <span
class="math inline">\(np&gt;5\)</span> and <span
class="math inline">\(n(1-p)&gt;5\)</span>, then we can assume that the
sampling distribution of <span class="math inline">\(\hat{p}\)</span> is
approximately Gaussian.</p>
</div>
<div class="boxed">
<p><strong>The Sampling Distribution of the Sample
Proportion</strong><br />
Assume that a pollster asked <span class="math inline">\(653\)</span>
people about their two-party preference for the upcoming federal
elections. If <span class="math inline">\(333\)</span> people expressed
their preference for the LNP, what is the sampling distribution of <span
class="math inline">\(\hat{p}\)</span>?</p>
<p><strong>Solution:</strong><br />
We are given that: <span class="math display">\[
\begin{align}
x&amp;=333\\
n&amp;=653
\end{align}
\]</span> So we can compute <span class="math display">\[
\begin{align}
\hat{p}&amp;=\frac{x}{n}\\
&amp;=\frac{333}{653}\\
&amp;=0.51
\end{align}
\]</span> Note that <span class="math inline">\(n\hat{p} =
333&gt;5\)</span> and <span
class="math inline">\(n(1-\hat{p})=320&gt;5\)</span> so the assumption
of a Gaussian sampling distribution is reasonable.</p>
<p>The sampling distribution is <span class="math display">\[
\begin{align}
\hat{p}\sim N\left(p,\frac{p(1-p)}{n}\right)
\end{align}
\]</span></p>
<p>Now assume that the true proportion of people who express a
preference for the LNP is <span class="math inline">\(0.5\)</span>. What
is the probability of observing a sample proportion larger than the one
calculated previously?</p>
<p>The question is <span
class="math inline">\(Pr(\hat{p}&gt;0.51)\)</span> given that <span
class="math inline">\(p=0.5\)</span>. This is equivalent to <span
class="math display">\[
\begin{align}
Pr(\hat{p}&gt;0.51)&amp;=Pr\left(\frac{\hat{p}-p}{\sqrt{\frac{p(1-p)}{n}}}&gt;\frac{
0.51 - 0.5}{\sqrt{\frac{0.5(1-0.5)}{653}}}\right)\\
&amp;=Pr(Z&lt;0.509\\
&amp;=0.695
\end{align}
\]</span></p>
</div>
</div>
</div>
<div id="large-sample-estimation" class="section level2">
<h2>Large Sample Estimation</h2>
<p>Given the data, we can calculate sample statistics that summarise the
data. Our ultimate goal in statistical modelling is not to summarise
data but to use the data to gain insight into the population. We gain
understanding by using the data to estimate and make inferences about
the population parameters.</p>
<div id="point-estimation" class="section level3 unnumbered">
<h3 class="unnumbered">Point Estimation</h3>
<p>In classical statistics, model parameters are unknown but are assumed
to be fixed quantities. Parameter estimation is properly known as point
estimation, and the resulting estimators are called point estimators.
There are two basic approaches to point estimation, the method of
moments and the method of maximum likelihood.</p>
</div>
<div id="the-method-of-moments" class="section level3 unnumbered">
<h3 class="unnumbered">The Method of Moments</h3>
<div class="sidenote">
<p><strong>Example:</strong><br />
Find the method of moments estimator of <span
class="math inline">\(\lambda\)</span> for the exponential distribution
<span class="math display">\[
X\sim \operatorname{Exp}(\lambda)
\]</span> i.e. <span class="math display">\[
f(x)=\lambda e^{-\lambda x}.
\]</span></p>
</div>
<p>The statistician Karl Pearson developed the method of moments in the
early twentieth century as the first coherent methodology for estimating
population parameters, or the parameters from the probability
distribution of the population. Pearson’s method is fairly
straightforward, first he defined moments of a probability distribution
(borrowing heavily from physics) as <span class="math display">\[
\begin{aligned}
\mu_1 &amp; =   \int_{-\infty}^{\infty}xf(x)dx\\
\mu_2 &amp; =  \int_{-\infty}^{\infty}x^2f(x)dx\\
\vdots &amp;  \\
\mu_k &amp; =  \int_{-\infty}^{\infty}x^kf(x)dx.\end{aligned}
\]</span> We can see that we are already familiar with moments to a
certain extent, as <span class="math inline">\(\mu_1=E(X)\)</span> and
<span class="math inline">\(\mbox{Var}(X)=\mu_2-\mu_1^2\)</span>.
Pearson further defined the sample moments as <span
class="math display">\[
\begin{aligned}
m_1 &amp; =  \frac{1}{n}\sum_{i = 1}^nx_i\\
m_2 &amp; =  \frac{1}{n}\sum_{i = 1}^nx_i^2\\
\vdots  &amp; \\
m_k &amp; =  \frac{1}{n}\sum_{i = 1}^nx_i^k\end{aligned}
\]</span> If we can find the parameters of the population probability
distribution analytically and expressed as functions of the probability
distribution moments, the method of moments estimates the probability
distribution parameters by substituting the sample moments into these
expressions.</p>
<div class="boxed">
<p><strong>The Method of Moments Estimator for the Rate of the
Exponential Distribution</strong><br />
There is only one parameter for the exponential distribution, so we will
only need to match the first moment. <span
class="math inline">\(\mu_1=E(X)\)</span>. For the exponential
distribution <span class="math display">\[
f(x) = \lambda e^{-\lambda x}
\]</span> and <span class="math display">\[
\begin{aligned}
E(X) &amp;= \frac{1}{\lambda}\\
\frac{1}{E(X)}&amp;=\lambda
\end{aligned}
\]</span> Substituting the sample moment <span class="math display">\[
m_1=\frac{\sum_{i=1}^nx_i}{n}
\]</span> for <span class="math inline">\(\mu_1\)</span> yields the
moment estimator of the rate of the exponential distribution (note that
<span class="math inline">\(\bar{x}=m_1\)</span>). <span
class="math display">\[
\tilde{\lambda}=\frac{1}{\bar{x}}
\]</span></p>
</div>
</div>
<div id="the-method-of-maximum-likelihood"
class="section level3 unnumbered">
<h3 class="unnumbered">The Method of Maximum Likelihood</h3>
<p>In the early twentieth century, Karl Pearson dominated the field of
statistics in a way that is rarely witnessed or understood in the modern
era and his method of moments was de rigueur for practising
statisticians. But beginning in the early 1920s and continuing over the
next decade Pearson became embroiled in one of the great and most bitter
battles in scientific history. Begining in 1919, Ronald Ayers Fisher
(who we’ve seen before), was a young mathematician working in relative
isolation at a remote agricultural station analysing 80 years of
accumulated data. Over the next 14 years, Fisher would become one of the
most well-known and respected mathematicians and scientists in recent
history, as well as become embroiled in a fiercely personal feud with
Karl Pearson, eventually unseating him as the leading force in
developing statistical methods. The core of Fisher and Pearson’s feud
lay in Fisher’s method of point estimation and his subsequent criticism
of Pearson’s method of moments.</p>
<div id="likelihood" class="section level4 unnumbered">
<h4 class="unnumbered">Likelihood</h4>
<p>Instead of focusing on moments, Fisher developed what he called the
likelihood function and showed that his approach of estimating
parameters by choosing values that maximised the likelihood function was
in some cases superior to Pearson’s method of moments. For a pmf <span
class="math inline">\(p(x;\theta)\)</span> or pdf <span
class="math inline">\(f(x;\theta)\)</span> the likelihood function is
defined as: <span class="math display">\[L(\theta|\mathbf{x}) =
\prod_{i=1}^np(x_i)\]</span> in the discrete case and <span
class="math display">\[L(\theta|\mathbf{x}) =
\prod_{i=1}^nf(x_i)\]</span> in the continuous case. Note that the
notation for the pmf and pdf shows that they are functions of both the
data <span class="math inline">\(x\)</span> and the parameter <span
class="math inline">\(\theta\)</span> but be careful to remember that
while <span class="math inline">\(\theta\)</span> may be unknown, it is
not a random variable.</p>
</div>
<div id="the-maximum-likelihood-estimator"
class="section level4 unnumbered">
<h4 class="unnumbered">The Maximum Likelihood Estimator</h4>
<p>The maximum likelihood estimator (MLE) is the value <span
class="math display">\[
\hat{\theta}=\max_{\Theta}L(\theta|\mathbf{x}).
\]</span></p>
<p>Note that directly maximising the likelihood is often not very
feasible, and it can be simpler to work with the log-likelihood, e.g.
<span class="math display">\[
\begin{aligned}
l(\theta|\mathbf{x}) &amp;= \log(L(\theta|\mathbf{x}))\\
&amp;=\sum_{i=1}^n\log(f(x_i;\theta))
\end{aligned}
\]</span> Because the <span class="math inline">\(\log\)</span> function
is monotonic, then the maximum of the log-likelihood is the maximum of
the likelihood, and we can find the MLE via the log-likelihood, i.e. 
<span class="math display">\[
\hat{\theta}=\max_{\Theta}l(\theta|\mathbf{x})
\]</span></p>
<p><strong>The Maximum Likelihood Estimator of the Poisson
Distribution</strong><br />
<span class="math display">\[
\begin{aligned}
L(\lambda|\mathbf{x})&amp;=\prod_{i=1}^n\frac{\lambda^{x_i}}{x_i!}e^{-\lambda}\\
l(\lambda|\mathbf{x})&amp;=\sum_{i=1}^n
x_i\log(\lambda)-\log(x_i)-n\lambda
\end{aligned}
\]</span> Find the maximum by taking the derivative of the
log-likelihood w.r.t. <span class="math inline">\(\lambda\)</span> and
solving for <span class="math inline">\(0\)</span> <span
class="math display">\[
\frac{d}{d\lambda}l(\lambda|\mathbf{x})=\frac{\sum_{i=1}^nx_i}{\lambda}-n
\]</span> <span class="math display">\[
\begin{aligned}
\frac{\sum_{i=1}^nx_i}{\lambda}-n&amp; = 0\\
\frac{\sum_{i=1}^nx_i}{\lambda} &amp; =  n\\
\sum_{i=1}^nx_i &amp; =  n\lambda\\
\frac{\sum_{i=1}^nx_i}{n} &amp; = \hat{ \lambda}\end{aligned}
\]</span></p>
<p>Now we will look at an example comparing a moment-based estimator to
the MALE.</p>
</div>
</div>
<div id="properties-of-estimators" class="section level3 unnumbered">
<h3 class="unnumbered">Properties of Estimators</h3>
<p>Both methods of moments and maximum likelihood produce estimates of
population parameters. But the choice of which one to use depends on the
situation, income cases; it is computationally difficult to compute the
MLE and the method of moments can be easier to find. In general, we
evaluate estimators using two properties.</p>
<div id="bias" class="section level4 unnumbered">
<h4 class="unnumbered">Bias</h4>
<p>Bias in a parameter estimate is the difference between the expected
value of the estimator and the true value of the parameter, i.e. where
<span class="math inline">\(\theta_0\)</span> is the true value of the
parameter, the bias of an estimator <span
class="math inline">\(\hat{\theta}\)</span> is <span
class="math display">\[
\mbox{Bias} = E(\hat{\theta})-\theta_0
\]</span> Ideally, we want unbiased estimators; from the central limit
theorem, we can see that the sample mean is impartial, but there are
other estimators that are not unbiased but can still be useful.</p>
</div>
<div id="variance" class="section level4 unnumbered">
<h4 class="unnumbered">Variance</h4>
<p>Variance refers to the variance of the estimator, given two unbiased
estimators of <span class="math inline">\(\theta\)</span>, we would
prefer to use the one with the smaller variance, i.e. choose <span
class="math inline">\(\hat{\theta}\)</span> over <span
class="math inline">\(\tilde{\theta}\)</span> when <span
class="math display">\[
\operatorname{Var}(\hat{\theta})&lt;\operatorname{Var}(\tilde{\theta}).
\]</span> Sometimes the choice of estimator is governed by what is known
as the bias-variance trade-off. Given data <span
class="math inline">\(x_i\)</span> with variance <span
class="math inline">\(\sigma^2\)</span>, estimators of <span
class="math inline">\(\theta=E(X)\)</span> are selected that minimise
the mean-squared error <span class="math display">\[
\sum_{i=1}^n(x_i-\hat{\theta})^2=\mbox{Bias}^2+\operatorname{Var}(\hat{\theta})+\sigma^2
\]</span> It is apparent that for two unbiased estimators (i.e. Bias=0
for both), then the choice becomes the estimator with the smallest
variance, however, in some cases, there may be biased estimators that
have small enough variances to make them preferable to an unbiased
estimator.</p>
<p>Now we will look at an example comparing a moment-based estimator to
the MALE.</p>
<div class="boxed">
<p><strong>The MLE versus the Moment Estimator for the Uniform
Distribution</strong><br />
Consider <span class="math inline">\(x_i\sim U(0,a)\)</span> the method
of moments based estimator is <span
class="math display">\[\tilde{a}=2\bar{x}\]</span> and the MLE is <span
class="math display">\[\hat{a}=\max(x_i),\mbox{ or }x^{(n)}\]</span>
Both of these estimators are unbiased, but note that the problem with
the method of moments based estimators is that (particularly for small
<span class="math inline">\(n\)</span>) is quite possible to find <span
class="math inline">\(\bar{x}&gt;a/2\)</span>, that is a value of <span
class="math inline">\(\tilde{a}\)</span> that is greater than the true
value, and is outside the sample space, or it is also possible to obtain
a moment estimator such that <span
class="math inline">\(2\bar{x}&lt;a\)</span>, in which case there may be
observations in the data that are larger than <span
class="math inline">\(\tilde{a}\)</span>, which would make them outside
the sample space. The MLE is guaranteed to not exhibit this behaviour
and is the preferred estimator.</p>
</div>
</div>
</div>
</div>
<div id="confidence-intervals" class="section level2">
<h2>Confidence Intervals</h2>
<p>We see <a href="./chap7.html#estimation">previously</a> how to find
<em>point-estimates</em> of parameters; these are single values deemed
optimal estimates of the parameter’s true value. We can also create what
are called interval estimates. An interval estimator is a rule for
creating an interval that hopefully captures the parameter’s true value.
We specify the interval rule based on some probability or confidence
coefficient <span class="math inline">\(1-\alpha\)</span> that describes
the probability that a confidence interval constructed according to the
rule will capture the parameter’s true value. It is important to
distinguish between estimators as a rule for constructing an interval
and a specific estimator. The confidence coefficient refers to the
probability associated with the rule for constructing the interval, not
the probability that a specific interval will capture the true value of
the parameter.</p>
<p>Consider the classic ring toss game; contestants throw rings seeking
to throw a ring such that it captures a peg. The confidence coefficient
is the probability or proportion of tosses that will capture the target
peg. The interval estimator is the rule for creating a ring so that you
have a <span class="math inline">\(1-\alpha\%\)</span> chance of
capturing the peg.</p>
<p>For a given <span class="math inline">\(1-\alpha\)</span> confidence
coefficient, the <span class="math inline">\((1-\alpha)\%\)</span>
confidence interval is defined as: <span
class="math display">\[\hat{\theta}\pm
Z_{\alpha/2}\mbox{SE}_{\theta}\]</span> where <span
class="math inline">\(Z_{\alpha/2}\)</span> is the tail value for <span
class="math inline">\(Z\)</span> with area <span
class="math inline">\(\alpha/2\)</span>, and SE<span
class="math inline">\(_{\theta}\)</span> is the standard error of <span
class="math inline">\(\hat{\theta}\)</span>. The resulting formula gives
an <strong>upper confidence limit (UCL)</strong> and a <strong>lower
confidence limit (LCL)</strong>.</p>
<p>Values for commonly used confidence coefficients<br />
</p>
<table>
<thead>
<tr class="header">
<th align="center">Confidence Coefficient</th>
<th align="left"><span class="math inline">\(Z_{\alpha}\)</span></th>
<th align="left"><span class="math inline">\(Z_{\alpha/2}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0.90</td>
<td align="left">1.28</td>
<td align="left">1.645</td>
</tr>
<tr class="even">
<td align="center">0.95</td>
<td align="left">1.645</td>
<td align="left">1.96</td>
</tr>
<tr class="odd">
<td align="center">0.98</td>
<td align="left">2.054</td>
<td align="left">2.33</td>
</tr>
<tr class="even">
<td align="center">0.99</td>
<td align="left">2.33</td>
<td align="left">2.58</td>
</tr>
</tbody>
</table>
</div>
<div id="large-sample-confidence-interval-for-the-population-mean"
class="section level2">
<h2>Large Sample Confidence Interval for the Population Mean</h2>
<div class="sidenote">
<p><strong>NOTE:</strong> If <span class="math inline">\(\sigma\)</span>
is unknown then for large samples <span
class="math inline">\(n&gt;30\)</span> it is reasonable to use the
approximation <span class="math inline">\(s\approx\sigma\)</span> to
compute the standard error and the test statistic <span
class="math inline">\(Z\)</span>.</p>
</div>
<div class="sidenote">
<p><strong>Example:</strong><br />
A sample of <span class="math inline">\(n=50\)</span> adult men reveals
that their average daily intake of protein is <span
class="math inline">\(\bar{x}=75.6\)</span> grams per day with a
standard deviation of <span class="math inline">\(s=3.5\)</span> grams.
Construct a 95% confidence interval for the average daily intake of
protein for men.</p>
</div>
<p>Many practical problems deal with making an inference or estimating
population means. For large samples <span
class="math inline">\((n&gt;30)\)</span> we know that <span
class="math inline">\(\bar{x}\)</span> is the best estimator for the
population mean <span class="math inline">\(\mu\)</span> and that <span
class="math display">\[
\bar{x}\sim N\left(\mu,\frac{\sigma^2}{n}\right).
\]</span> Given the sampling distribution then the confidence interval
is <span class="math display">\[
\bar{x}\pm Z_{\alpha/2}\frac{\sigma}{\sqrt{n}}
\]</span></p>
<div class="boxed">
<p><strong>Deriving the Large-Sample Confidence Interval for the
Population Mean</strong><br />
Let <span class="math inline">\(Z_{\alpha/2}\)</span> be the value
corresponding to the upper tail area of <span
class="math inline">\(\alpha/2\)</span>. <span class="math display">\[Z
= \frac{\bar{x}-\mu}{\sigma/\sqrt{n}}\]</span> then <span
class="math display">\[Pr\left(-Z_{\alpha/2}\leq\frac{\bar{x}-\mu}{\sigma/\sqrt{n}}\leq
Z_{\alpha/2}\right)=1-\alpha\]</span> Rearranging the inequality <span
class="math display">\[
Pr\left(\bar{x}-Z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\leq\mu\leq
\bar{x}+Z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right)=1-\alpha
\]</span> The UCL and LCL are random quantities depending on the value
of <span class="math inline">\(\bar{x}\)</span>, therefore the
probability for the inequality arises from the fact that for repeated
samples, the interval will only capture the true value of <span
class="math inline">\(\mu\)</span> with probability <span
class="math inline">\(1-\alpha\)</span>.</p>
</div>
<div id="example" class="section level3 tabset tabset-pills boxed">
<h3 class="tabset tabset-pills">Example:</h3>
<p>A sample of <span class="math inline">\(n=50\)</span> adult men
reveals that their average daily intake of protein is <span
class="math inline">\(\bar{x}=75.6\)</span> grams per day with a
standard deviation of <span class="math inline">\(s=3.5\)</span> grams.
Construct a 95% confidence interval for the average daily intake of
protein for men.</p>
<div id="solution" class="section level4">
<h4>Solution</h4>
<p>We are given: <span class="math display">\[
\begin{align}
n&amp;=50\\
\bar{x}&amp;=75.6\\
s&amp;=3.5.
\end{align}
\]</span> Noting that this is a 95% confidence interval substituting
these values into the expression, <span
class="math inline">\(Z_{\alpha/2}=1.96\)</span>. Substituting these
values into the expression <span class="math display">\[
\bar{x}\pm Z_{\alpha/2}\frac{s}{\sqrt{n}}
\]</span> yields the confidence interval <span class="math display">\[
\left(\bar{x}-Z_{\alpha/2}\frac{s}{\sqrt{n}},\:
\bar{x}+Z_{\alpha/2}\frac{s}{\sqrt{n}}\right)\\
\left(75.6-1.96\frac{3.5}{\sqrt{50}},\:75.6+1.96\frac{3.5}{\sqrt{50}}
\right)\\
\left( 74.63 ,\: 76.57 \right).
\]</span></p>
</div>
<div id="video" class="section level4">
<h4>Video</h4>
<div style="max-width: 590px">
<div
style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe width="590" height="332" src="https://web.microsoftstream.com/embed/video/1b51bf17-6e8d-4151-a1bc-a6ca9a213949?autoplay=false&amp;showinfo=true" allowfullscreen style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
</iframe>
</div>
</div>
</div>
</div>
</div>
<div
id="large-sample-confidence-intervals-for-the-population-proportion"
class="section level2">
<h2>Large Sample Confidence Intervals for the Population Proportion</h2>
<div class="sidenote">
<p><strong>Example: Confidence Intervals for a Population
Proportion</strong><br />
A random sample of <span class="math inline">\(n=985\)</span> Queensland
residents seeking their opinions on how the Queensland State Government
was handling the COVID-19 crisis. Of those surveyed, <span
class="math inline">\(x=592\)</span> indicated that they approved of the
current government’s handling of the COVID-19 crisis. Construct a <span
class="math inline">\(90\%\)</span> confidence interval for the
proportion of the population that approves of the current government’s
handling of the COVID-19 crisis.</p>
</div>
<p>In many cases, population proportions are the primary quantity of
interest. Previously, we have seen that there is a convenient sampling
distribution for the sample proportion <span
class="math inline">\(\hat{p}\)</span> <span class="math display">\[
\hat{p}\sim N\left(p,\frac{p(1-p)}{n}\right).
\]</span></p>
<div class="sidenote">
<p><strong>NOTE:</strong> that when <span
class="math inline">\(p\)</span> is unknown the value of <span
class="math inline">\(\hat{p}\)</span> can be substituted into the
conditions, yielding the constraints that <span
class="math inline">\(n\hat{p}&gt;5\)</span> and <span
class="math inline">\(n(1-\hat{p})&gt;5\)</span>, and it is possible to
estimate the standard error as <span class="math display">\[
\mbox{SE}=\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}
\]</span></p>
</div>
<div id="example-confidence-intervals-for-a-population-proportion"
class="section level3 tabset tabset-pills boxed">
<h3 class="tabset tabset-pills">Example: Confidence Intervals for a
Population Proportion</h3>
<p>A random sample of <span class="math inline">\(n=985\)</span>
Queensland residents seeking their opinions on how the Queensland State
Government was handling the COVID-19 crisis. Of those surveyed, <span
class="math inline">\(x=592\)</span> indicated that they approved of the
current government’s handling of the COVID-19 crisis. Construct a <span
class="math inline">\(90\%\)</span> confidence interval for the
proportion of the population that approves of the current government’s
handling of the COVID-19 crisis.</p>
<div id="solution-1" class="section level4">
<h4>Solution</h4>
<p>We are given: <span class="math display">\[
\begin{align}
n&amp;=985\\
x&amp;=1592\\
\hat{p}&amp;= 0.6010152.
\end{align}
\]</span> First, we can check that <span class="math display">\[
n\hat{p}&gt;5\mbox{ and }n(1-\hat{p})&gt;5\\
n\hat{p} = 592 \mbox{ and } n(1-\hat{p})= 393
\]</span> so it is reasonable to assume that the confidence interval
will have the form <span class="math display">\[
\hat{p}\pm Z_{\alpha/2}\mbox{SE}
\]</span> where <span class="math display">\[
\mbox{SE}=\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}.
\]</span></p>
<p>For a 90% confidence interval <span
class="math inline">\(Z_{\alpha/2}=1.645\)</span>. Substituting these
numbers into the expression <span class="math display">\[
\hat{p}\pm Z_{\alpha/2}\mbox{SE}
\]</span></p>
<p>yields <span class="math display">\[
\left(\hat{p}-Z_{\alpha/2}\mbox{SE},\:\hat{p}+Z_{\alpha/2}\mbox{SE}\right)\\
\left(\hat{p}-Z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}},\:\hat{p}+Z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}
\right)\\
\left(\frac{592}{985}-1.645\sqrt{\frac{\frac{592}{985}\frac{393}{985}}{985}},\:\frac{592}{985}+1.645\sqrt{\frac{\frac{592}{985}\frac{393}{985}}{985}}\right)\\
\left(\frac{592}{985}-1.645\sqrt{\frac{(592)(393)}{985^3}},\:
\frac{592}{985}-1.645\sqrt{\frac{(592)(393)}{985^3}}
\right)\\
\left(0.58 ,\: 0.63 \right)
\]</span></p>
</div>
<div id="video-1" class="section level4">
<h4>Video</h4>
<div style="max-width: 590px">
<div
style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe width="590" height="332" src="https://web.microsoftstream.com/embed/video/cae4ef33-4a36-453c-a499-7427a3cad964?autoplay=false&amp;showinfo=true" allowfullscreen style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
</iframe>
</div>
</div>
</div>
</div>
</div>
<div id="estimating-the-difference-between-two-population-means"
class="section level2">
<h2>Estimating the Difference Between Two Population Means</h2>
<div class="sidenote">
<p><strong>Confidence interval for the difference between the mean of
two populations</strong><br />
A consumer interest group collected data on the life cycle of two
different makes of car tyres with sample sizes of <span
class="math inline">\(n_1=n_2=100\)</span> and computed the following
statistics.</p>
<table>
<thead>
<tr class="header">
<th align="left">Tyre A</th>
<th align="left">Tyre B</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(\bar{x}_1 = 26,400\)</span>
KM</td>
<td align="left"><span class="math inline">\(\bar{x}_2 = 25,100\)</span>
KM</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(s_1^2 = 1,444,000\)</span>
(KM)<span class="math inline">\(^2\)</span></td>
<td align="left"><span class="math inline">\(s_2^2 = 1,960,000\)</span>
(KM)<span class="math inline">\(^2\)</span></td>
</tr>
</tbody>
</table>
<p>Compute the <span class="math inline">\(99\%\)</span> confidence
interval for the difference between the expected life-cycle for these
two makes of tyres.</p>
</div>
<p>In addition to estimating populations means, often (and more
interestingly) we are interested in the difference between two
population means. This means that now there are two populations means
<span class="math inline">\(\mu_1\)</span> and <span
class="math inline">\(\mu_2\)</span>, two variances <span
class="math inline">\(\sigma_1^2\)</span> and <span
class="math inline">\(\sigma_2^2\)</span>, and two sample sizes <span
class="math inline">\(n_1\)</span> and <span
class="math inline">\(n_2\)</span>. And of course there are two sets of
estimates <span class="math inline">\(\bar{x}_1\)</span>, <span
class="math inline">\(s_1^2\)</span>, <span
class="math inline">\(\bar{x}_2\)</span>, and <span
class="math inline">\(s^2_2\)</span>.</p>
<p>It is reasonable (and correct) to assume that the best estimate of
the difference between population means <span
class="math inline">\(\mu_1-\mu_2\)</span> is the difference between the
sample means <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span>.
The question is then, what is the sampling distribution of <span
class="math inline">\(\bar{x}_1-\bar{x}_2\)</span>?</p>
<div class="boxed">
<p><strong>The sampling distribution of <span
class="math inline">\(\bar{x}_1-\bar{x}_2\)</span></strong><br />
The expected value of <span
class="math inline">\(\bar{x}_1-\bar{x}_2\)</span> is <span
class="math display">\[E(\bar{x}_1-\bar{x}_2) = \mu_1-\mu_2\]</span> and
the standard error is <span class="math display">\[\mbox{SE} =
\sqrt{\frac{\sigma^2_1}{n_1}+\frac{\sigma_2^2}{n_2}}\]</span> which for
large <span class="math inline">\(n_1\)</span> and <span
class="math inline">\(n_2\)</span> can be approximated as <span
class="math display">\[\mbox{SE} =
\sqrt{\frac{s^2_1}{n_1}+\frac{s_2^2}{n_2}}\]</span></p>
</div>
<ol style="list-style-type: decimal">
<li><p>If the two populations follow a Gaussian distribution, then the
sampling distribution is exactly Gaussian.</p></li>
<li><p>If the two populations are not Gaussian, then the sampling
distribution is approximately Gaussian, for <span
class="math inline">\(n_1&gt;30\)</span> and <span
class="math inline">\(n_2&gt;30\)</span>.</p></li>
</ol>
<p>From this we can construct the <span
class="math inline">\((1-\alpha)\%\)</span> confidence interval for the
difference <span class="math inline">\(\mu_1-\mu_2\)</span> as: <span
class="math display">\[\bar{x}_1-\bar{x}_2\pm
Z_{\alpha/2}\sqrt{\frac{s^2_1}{n_1}+\frac{s_2^2}{n_2}}\]</span></p>
<div
id="example-confidence-interval-for-the-difference-between-the-mean-of-two-populations"
class="section level3 tabset tabset-pills boxed">
<h3 class="tabset tabset-pills">Example: Confidence interval for the
difference between the mean of two populations</h3>
<p>A consumer interest group collected data on the life cycle of two
different makes of car tyres with sample sizes of <span
class="math inline">\(n_1=n_2=100\)</span> and computed the following
statistics.</p>
<div class="table-narrow">
<table>
<thead>
<tr class="header">
<th align="left">Tyre A</th>
<th align="left">Tyre B</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(\bar{x}_1 = 26,400\)</span>
KM</td>
<td align="left"><span class="math inline">\(\bar{x}_2 = 25,100\)</span>
KM</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(s_1^2 = 1,444,000\)</span>
(KM)<span class="math inline">\(^2\)</span></td>
<td align="left"><span class="math inline">\(s_2^2 = 1,960,000\)</span>
(KM)<span class="math inline">\(^2\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>Compute the <span class="math inline">\(99\%\)</span> confidence
interval for the difference between the expected life-cycle for these
two makes of tyres.</p>
<div id="solution-2" class="section level4">
<h4>Solution</h4>
<p>The confidence interval for the difference <span
class="math inline">\(\mu_1-\mu_2\)</span> is <span
class="math display">\[
\bar{x}_1-\bar{x}_2\pm
Z_{\alpha/2}\sqrt{\frac{s^2_1}{n_1}+\frac{s_2^2}{n_2}}.
\]</span> Substituting the data we have <span class="math display">\[
\begin{align}
&amp;=\bar{x}_1-\bar{x}_2\pm
Z_{\alpha/2}\sqrt{\frac{s^2_1}{n_1}+\frac{s_2^2}{n_2}}\\
&amp;=(26,400-25,100)\pm
Z_{\alpha/2}\sqrt{\frac{1,444,000}{100}+\frac{1,960,000}{100}}\\
&amp;=(26,400-25,100)\pm Z_{\alpha/2}\sqrt{14,440+19,600}\\
&amp;=1,300\pm Z_{\alpha/2}\sqrt{3.404\times 10^{4} }\\
&amp;=1300\pm 2.58( 184.5 )\\
&amp;=\left(823.99 ,\: 1776.01
\right)
\end{align}
\]</span></p>
</div>
<div id="video-2" class="section level4">
<h4>Video</h4>
<div style="max-width: 590px">
<div
style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe width="590" height="332" src="https://web.microsoftstream.com/embed/video/8bae89bd-0eab-4909-9f08-3def64ec7cba?autoplay=false&amp;showinfo=true" allowfullscreen style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
</iframe>
</div>
</div>
</div>
</div>
</div>
<div id="estimating-the-difference-between-two-population-proportions"
class="section level2">
<h2>Estimating the Difference Between Two Population Proportions</h2>
<div class="sidenote">
<p><strong>The confidence interval for the difference between two
population proportions</strong><br />
A random sample of <span class="math inline">\(n=985\)</span> Queensland
residents sought their opinions on how the Queensland State Government
handled the COVID-19 crisis. Of those surveyed, <span
class="math inline">\(x=592\)</span> indicated that they approved the
current government’s handling of the COVID-19 crisis. Construct a <span
class="math inline">\(90\%\)</span> confidence interval for the
proportion of the population that approves of the current government’s
handling of the COVID-19 crisis. Now consider the polling results with
residents divided into urban and rural voters.</p>
<table>
<tbody>
<tr class="odd">
<td></td>
<td align="right">Rural-U</td>
<td align="left">rban</td>
</tr>
<tr class="even">
<td>Sample Size</td>
<td align="right">328</td>
<td align="left">657</td>
</tr>
<tr class="odd">
<td>Number Approve</td>
<td align="right">249</td>
<td align="left">427</td>
</tr>
<tr class="even">
<td>Proportion Approve</td>
<td align="right">0.76</td>
<td align="left">0.65</td>
</tr>
</tbody>
</table>
<p>Compute the <span class="math inline">\(95\%\)</span> confidence
interval for the difference in the population proportions for urban and
rural residents.</p>
</div>
<p>Just as we are often interested in estimating the difference between
the means or two populations, we may often be interested in the
difference between the proportions of two populations. As in the case of
the difference between two populations means, the best estimator for the
difference between two population proportions <span
class="math inline">\(p_1-p_2\)</span>, is <span
class="math inline">\(\hat{p}_1-\hat{p}_2\)</span>.</p>
<div class="boxed">
<p><strong>The sampling distribution of <span
class="math inline">\(\hat{p}_1-\hat{p}_2\)</span></strong><br />
The expected value of <span
class="math inline">\(\hat{p}_1-\hat{p}_2\)</span> is <span
class="math display">\[E(\hat{p}_1-\hat{p}_2)=p_1-p_2\]</span> and the
standard error is <span
class="math display">\[\mbox{SE}=\sqrt{\frac{p_1(1-p_1)}{n_1}+\frac{p_2(1-p_2)}{n_2}}\]</span>
which is estimated as <span
class="math display">\[\mbox{SE}=\sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1}+\frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}\]</span>
Note that by the Central Limit Theorem, the sampling distribution is
approximately Gaussian when <span class="math inline">\(n_1\)</span> and
<span class="math inline">\(n_2\)</span> are large.</p>
</div>
<p>From this the <span class="math inline">\((1-\alpha)\%\)</span>
confidence interval for the difference in two population proportions is:
<span class="math display">\[(\hat{p}_1-\hat{p}_2)\pm
Z_{\alpha/2}\sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1}+\frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}\]</span>
Note the constraints <span
class="math inline">\(n_1\hat{p}_1\)</span>,<span
class="math inline">\(n_1(1-\hat{p}_1)\)</span>, <span
class="math inline">\(n_2\hat{p}_2\)</span>, and <span
class="math inline">\(n_2(1-\hat{p}_2)\)</span> all be greater than 5
apply.</p>
<div
id="example-the-confidence-interval-for-the-difference-between-two-population-proportions"
class="section level3 tabset tabset-pills boxed">
<h3 class="tabset tabset-pills">Example: The confidence interval for the
difference between two population proportions</h3>
<p>A random sample of <span class="math inline">\(n=985\)</span>
Queensland residents sought their opinions on how the Queensland State
Government handled the COVID-19 crisis. Of those surveyed, <span
class="math inline">\(x=592\)</span> indicated that they approved the
current government’s handling of the COVID-19 crisis. Construct a <span
class="math inline">\(90\%\)</span> confidence interval for the
proportion of the population that approves of the current government’s
handling of the COVID-19 crisis. Now consider the polling results with
residents divided into urban and rural voters.</p>
<div class="table-narrow">
<table>
<tbody>
<tr class="odd">
<td></td>
<td align="right">Rural-U</td>
<td align="left">rban</td>
</tr>
<tr class="even">
<td>Sample Size</td>
<td align="right">328</td>
<td align="left">657</td>
</tr>
<tr class="odd">
<td>Number Approve</td>
<td align="right">249</td>
<td align="left">427</td>
</tr>
<tr class="even">
<td>Proportion Approve</td>
<td align="right">0.76</td>
<td align="left">0.65</td>
</tr>
</tbody>
</table>
</div>
<p>Compute the <span class="math inline">\(95\%\)</span> confidence
interval for the difference in the population proportions for urban and
rural residents.</p>
<div id="solution-3" class="section level4">
<h4>Solution</h4>
<p>The <span class="math inline">\((1-\alpha)\%\)</span> confidence
interval for the difference between two population proportions is: <span
class="math display">\[
(\hat{p}_1-\hat{p}_2)\pm
Z_{\alpha/2}\sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1}+\frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}
\]</span> Substituting in the data, we have the 95% confidence interval
<span class="math display">\[
\begin{align}
&amp;=(\hat{p}_1-\hat{p}_2)\pm
Z_{\alpha/2}\sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1}+\frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}\\
&amp;=(0.76-0.65)\pm
1.96\sqrt{\frac{(0.76)(0.24)}{328}+\frac{(0.65)(0.35)}{657}}\\
&amp;=0.11\pm 1.96\sqrt{\frac{0.18 }{328 }+\frac{0.23 }{657 }}\\
&amp;=0.11\pm 1.96\sqrt{0.03}\\
&amp;=\left(0.05 \: 0.17
\right)
\end{align}
\]</span></p>
</div>
<div id="video-3" class="section level4">
<h4>Video</h4>
<div style="max-width: 590px">
<div
style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe width="590" height="332" src="https://web.microsoftstream.com/embed/video/a02cd0f6-ba73-4d76-90eb-98fdb5474adc?autoplay=false&amp;showinfo=true" allowfullscreen style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
</iframe>
</div>
</div>
</div>
</div>
</div>
</div>

<!--
&nbsp;
&nbsp;
<footer class="copyright">
<p>Copyright &copy;2021 Queensland University of Technology, All rights reserved.</p>
</footer>
-->

<!-- js for accordion button -->
<script>
var acc = document.getElementsByClassName("week");
var i   ;

for (i = 0; i < acc.length; i++) {
  acc[i].addEventListener("click", function() {
    /* Toggle between adding and removing the "active" class,
    to highlight the button that controls the panel */
    this.classList.toggle("active");

    /* Toggle between hiding and showing the active panel */
    var panel = this.nextElementSibling;
    if (panel.style.display === "block") {
      panel.style.display = "none";
    } else {
      panel.style.display = "block";
    }
  });
}
</script>



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
